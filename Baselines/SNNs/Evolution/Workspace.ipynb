{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from EvolutionStrategy import ESModel\n",
    "from RandmanFunctions import get_randman_dataset\n",
    "from Utilities import spike_to_label\n",
    "\n",
    "import snntorch as snn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, learn_beta, beta=0.95):\n",
    "        super(SNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta, learn_beta=learn_beta)\n",
    "\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta, learn_beta=learn_beta)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        batch_size, time_steps, num_neurons = x.shape\n",
    "        x = x.permute(1, 0, 2)  # (time, batch, neurons)\n",
    "\n",
    "        mem1, mem2 = [torch.zeros(batch_size, layer.out_features, device=x.device)\n",
    "                      for layer in [self.fc1, self.fc2]]\n",
    "\n",
    "        mem2_rec = []\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            spk1, mem1 = self.lif1(self.fc1(x[t]), mem1)\n",
    "            _, mem2 = self.lif2(self.fc2(spk1), mem2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(mem2_rec, dim=0)  # (time_steps, batch_size, num_outputs)\n",
    "    \n",
    "def losscustom(pred, labels):\n",
    "  # batch, classes, time_steps\n",
    "  # import pdb; pdb.set_trace()\n",
    "  mem = pred.permute(1,2,0)\n",
    "  labels = labels.long()\n",
    "  non_labels = 1-labels\n",
    "\n",
    "  batch_idx = torch.arange(mem.shape[0])\n",
    "\n",
    "  correct = mem[batch_idx, labels]\n",
    "  non_correct = mem[batch_idx, non_labels]\n",
    "\n",
    "  diff = non_correct - correct\n",
    "  diff_activated = torch.where(diff > 0, diff, torch.zeros_like(diff))\n",
    "  return (diff_activated).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Training SNN for Randman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "def _run_snn_on_batch(model, x, y, loss_fn): \n",
    "    # shape: [time_steps, batch_size, classes]\n",
    "    spike_train = model(x)\n",
    "    pred_y = spike_to_label(spike_train, scheme = 'highest_voltage')\n",
    "    \n",
    "    loss = loss_fn(spike_train, y.long())\n",
    "    correct = (pred_y == y).sum().item()\n",
    "    \n",
    "    return loss, correct\n",
    "\n",
    "def log_model(es_model,run):\n",
    "    filename = 'best-model.pth'\n",
    "    model = es_model.get_best_model()\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    run.log_model(path=filename)\n",
    "    os.remove(filename)  \n",
    "\n",
    "def val_loop_snn(es_model, dataloader, loss_fn):\n",
    "    model = es_model.get_best_model()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        batch_loss, batch_correct = _run_snn_on_batch(model, x, y, loss_fn) \n",
    "        test_loss += batch_loss\n",
    "        correct += batch_correct\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_acc = correct / size\n",
    "    print(f\"Test Error: \\nAccuracy: {(100*test_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "        \n",
    "    return test_loss.item(), test_acc\n",
    "\n",
    "def train_loop_snn(es_model, train_dataloader, val_dataloader, loss_fn, nb_model_samples, run):\n",
    "    \"\"\" one epoch of training, going through all the batches once\n",
    "    \"\"\"    \n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # train model with samples\n",
    "        samples_loss = []\n",
    "        for model in es_model.samples(nb_model_samples):\n",
    "            loss, _ = _run_snn_on_batch(model, x, y, loss_fn)\n",
    "            samples_loss.append(loss)            \n",
    "            \n",
    "        samples_loss = torch.stack(samples_loss) \n",
    "        es_model.gradient_descent(samples_loss)\n",
    "    \n",
    "        # best model loss and accuracy\n",
    "        best_model = es_model.get_best_model()\n",
    "        best_loss, best_correct = _run_snn_on_batch(best_model, x, y, loss_fn)   \n",
    "        best_acc = best_correct / len(y)\n",
    "        print(f\"batch {batch}, loss: {best_loss:>7f}, accuracy: {100 * best_acc:>0.1f}%\")\n",
    "        \n",
    "        # validation loss and accuracy\n",
    "        val_loss, val_acc = val_loop_snn(es_model, val_dataloader, loss_fn)\n",
    "        \n",
    "        # record keeping\n",
    "        run.log({'train_loss': best_loss.item(), 'train_acc' : best_acc, 'val_loss': val_loss, 'val_acc': val_acc}) \n",
    "        log_model(es_model, run)\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myixing\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wyx/darwin_neuron/wandb/run-20250408_052107-061lckg3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DarwinNeuron/DarwinNeuron/runs/061lckg3' target=\"_blank\">dashing-dream-39</a></strong> to <a href='https://wandb.ai/DarwinNeuron/DarwinNeuron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DarwinNeuron/DarwinNeuron' target=\"_blank\">https://wandb.ai/DarwinNeuron/DarwinNeuron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DarwinNeuron/DarwinNeuron/runs/061lckg3' target=\"_blank\">https://wandb.ai/DarwinNeuron/DarwinNeuron/runs/061lckg3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "batch 0, loss: 0.019187, accuracy: 46.5%\n",
      "Test Error: \n",
      "Accuracy: 57.8%, Avg loss: 0.018650 \n",
      "\n",
      "batch 1, loss: 0.039958, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 52.0%, Avg loss: 0.042622 \n",
      "\n",
      "batch 2, loss: 0.083525, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 48.0%, Avg loss: 0.086030 \n",
      "\n",
      "batch 3, loss: 0.126849, accuracy: 44.1%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.125744 \n",
      "\n",
      "batch 4, loss: 0.140900, accuracy: 60.5%\n",
      "Test Error: \n",
      "Accuracy: 55.5%, Avg loss: 0.143034 \n",
      "\n",
      "batch 5, loss: 0.154596, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 47.0%, Avg loss: 0.150514 \n",
      "\n",
      "batch 6, loss: 0.150560, accuracy: 60.9%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.153189 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "batch 0, loss: 0.157537, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 52.8%, Avg loss: 0.155472 \n",
      "\n",
      "batch 1, loss: 0.160378, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.158860 \n",
      "\n",
      "batch 2, loss: 0.159865, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 50.0%, Avg loss: 0.160475 \n",
      "\n",
      "batch 3, loss: 0.159730, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 48.0%, Avg loss: 0.162226 \n",
      "\n",
      "batch 4, loss: 0.158981, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 50.7%, Avg loss: 0.162120 \n",
      "\n",
      "batch 5, loss: 0.159144, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.159314 \n",
      "\n",
      "batch 6, loss: 0.161758, accuracy: 42.2%\n",
      "Test Error: \n",
      "Accuracy: 52.2%, Avg loss: 0.160365 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "batch 0, loss: 0.161736, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 48.5%, Avg loss: 0.160025 \n",
      "\n",
      "batch 1, loss: 0.160582, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 53.8%, Avg loss: 0.161208 \n",
      "\n",
      "batch 2, loss: 0.159676, accuracy: 44.9%\n",
      "Test Error: \n",
      "Accuracy: 47.0%, Avg loss: 0.162468 \n",
      "\n",
      "batch 3, loss: 0.161539, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 52.5%, Avg loss: 0.162160 \n",
      "\n",
      "batch 4, loss: 0.160955, accuracy: 48.0%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.161894 \n",
      "\n",
      "batch 5, loss: 0.162440, accuracy: 48.0%\n",
      "Test Error: \n",
      "Accuracy: 52.2%, Avg loss: 0.158846 \n",
      "\n",
      "batch 6, loss: 0.166133, accuracy: 39.1%\n",
      "Test Error: \n",
      "Accuracy: 52.0%, Avg loss: 0.159471 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "batch 0, loss: 0.158560, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.157580 \n",
      "\n",
      "batch 1, loss: 0.155108, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 51.7%, Avg loss: 0.153865 \n",
      "\n",
      "batch 2, loss: 0.152436, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.151744 \n",
      "\n",
      "batch 3, loss: 0.146732, accuracy: 53.5%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.144560 \n",
      "\n",
      "batch 4, loss: 0.143637, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 51.5%, Avg loss: 0.143333 \n",
      "\n",
      "batch 5, loss: 0.138093, accuracy: 47.7%\n",
      "Test Error: \n",
      "Accuracy: 50.0%, Avg loss: 0.138759 \n",
      "\n",
      "batch 6, loss: 0.128862, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.132471 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "batch 0, loss: 0.126777, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.123913 \n",
      "\n",
      "batch 1, loss: 0.119520, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 46.5%, Avg loss: 0.118481 \n",
      "\n",
      "batch 2, loss: 0.105459, accuracy: 46.1%\n",
      "Test Error: \n",
      "Accuracy: 49.5%, Avg loss: 0.104654 \n",
      "\n",
      "batch 3, loss: 0.097890, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 49.0%, Avg loss: 0.096154 \n",
      "\n",
      "batch 4, loss: 0.087181, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 47.5%, Avg loss: 0.088773 \n",
      "\n",
      "batch 5, loss: 0.081049, accuracy: 43.0%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.079693 \n",
      "\n",
      "batch 6, loss: 0.073578, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 48.0%, Avg loss: 0.070823 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "batch 0, loss: 0.066314, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 50.0%, Avg loss: 0.064729 \n",
      "\n",
      "batch 1, loss: 0.055990, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 47.2%, Avg loss: 0.058771 \n",
      "\n",
      "batch 2, loss: 0.054672, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 52.5%, Avg loss: 0.055413 \n",
      "\n",
      "batch 3, loss: 0.049859, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 52.2%, Avg loss: 0.050435 \n",
      "\n",
      "batch 4, loss: 0.045194, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.046830 \n",
      "\n",
      "batch 5, loss: 0.043662, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.042787 \n",
      "\n",
      "batch 6, loss: 0.043497, accuracy: 31.2%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.039747 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "batch 0, loss: 0.034525, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 54.8%, Avg loss: 0.037152 \n",
      "\n",
      "batch 1, loss: 0.033707, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 53.0%, Avg loss: 0.033701 \n",
      "\n",
      "batch 2, loss: 0.029432, accuracy: 47.3%\n",
      "Test Error: \n",
      "Accuracy: 52.5%, Avg loss: 0.029347 \n",
      "\n",
      "batch 3, loss: 0.025128, accuracy: 46.1%\n",
      "Test Error: \n",
      "Accuracy: 53.8%, Avg loss: 0.026481 \n",
      "\n",
      "batch 4, loss: 0.020982, accuracy: 53.5%\n",
      "Test Error: \n",
      "Accuracy: 54.0%, Avg loss: 0.022644 \n",
      "\n",
      "batch 5, loss: 0.020240, accuracy: 55.9%\n",
      "Test Error: \n",
      "Accuracy: 58.0%, Avg loss: 0.019400 \n",
      "\n",
      "batch 6, loss: 0.015189, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 53.5%, Avg loss: 0.016688 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "batch 0, loss: 0.013739, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 53.8%, Avg loss: 0.013676 \n",
      "\n",
      "batch 1, loss: 0.012353, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 53.5%, Avg loss: 0.012571 \n",
      "\n",
      "batch 2, loss: 0.011023, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 53.5%, Avg loss: 0.012427 \n",
      "\n",
      "batch 3, loss: 0.012333, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 52.0%, Avg loss: 0.013071 \n",
      "\n",
      "batch 4, loss: 0.013065, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 54.2%, Avg loss: 0.012953 \n",
      "\n",
      "batch 5, loss: 0.012182, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 51.7%, Avg loss: 0.011653 \n",
      "\n",
      "batch 6, loss: 0.008628, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 53.2%, Avg loss: 0.009689 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "batch 0, loss: 0.007147, accuracy: 55.9%\n",
      "Test Error: \n",
      "Accuracy: 54.2%, Avg loss: 0.006977 \n",
      "\n",
      "batch 1, loss: 0.005043, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 54.8%, Avg loss: 0.005275 \n",
      "\n",
      "batch 2, loss: 0.004379, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 54.0%, Avg loss: 0.004708 \n",
      "\n",
      "batch 3, loss: 0.003801, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 54.2%, Avg loss: 0.004117 \n",
      "\n",
      "batch 4, loss: 0.003357, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 54.2%, Avg loss: 0.003628 \n",
      "\n",
      "batch 5, loss: 0.002765, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 53.8%, Avg loss: 0.003090 \n",
      "\n",
      "batch 6, loss: 0.000440, accuracy: 73.4%\n",
      "Test Error: \n",
      "Accuracy: 54.8%, Avg loss: 0.001275 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "batch 0, loss: 0.002711, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 49.2%, Avg loss: 0.002767 \n",
      "\n",
      "batch 1, loss: 0.003777, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 48.2%, Avg loss: 0.003975 \n",
      "\n",
      "batch 2, loss: 0.005508, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 48.0%, Avg loss: 0.005790 \n",
      "\n",
      "batch 3, loss: 0.007672, accuracy: 50.4%\n",
      "Test Error: \n",
      "Accuracy: 47.5%, Avg loss: 0.008082 \n",
      "\n",
      "batch 4, loss: 0.009772, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 47.8%, Avg loss: 0.009724 \n",
      "\n",
      "batch 5, loss: 0.010955, accuracy: 44.5%\n",
      "Test Error: \n",
      "Accuracy: 47.8%, Avg loss: 0.010431 \n",
      "\n",
      "batch 6, loss: 0.010197, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 46.2%, Avg loss: 0.008421 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "batch 0, loss: 0.005855, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 46.0%, Avg loss: 0.005942 \n",
      "\n",
      "batch 1, loss: 0.002969, accuracy: 47.3%\n",
      "Test Error: \n",
      "Accuracy: 44.8%, Avg loss: 0.003028 \n",
      "\n",
      "batch 2, loss: 0.001417, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 50.0%, Avg loss: 0.001604 \n",
      "\n",
      "batch 3, loss: 0.004987, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 49.0%, Avg loss: 0.005638 \n",
      "\n",
      "batch 4, loss: 0.010379, accuracy: 46.5%\n",
      "Test Error: \n",
      "Accuracy: 48.8%, Avg loss: 0.010479 \n",
      "\n",
      "batch 5, loss: 0.014254, accuracy: 44.1%\n",
      "Test Error: \n",
      "Accuracy: 45.0%, Avg loss: 0.013522 \n",
      "\n",
      "batch 6, loss: 0.016422, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 44.8%, Avg loss: 0.015378 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "batch 0, loss: 0.015819, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 44.2%, Avg loss: 0.015675 \n",
      "\n",
      "batch 1, loss: 0.015420, accuracy: 40.2%\n",
      "Test Error: \n",
      "Accuracy: 45.0%, Avg loss: 0.014982 \n",
      "\n",
      "batch 2, loss: 0.012631, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 44.8%, Avg loss: 0.013880 \n",
      "\n",
      "batch 3, loss: 0.011575, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 43.8%, Avg loss: 0.011663 \n",
      "\n",
      "batch 4, loss: 0.008923, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 48.2%, Avg loss: 0.009472 \n",
      "\n",
      "batch 5, loss: 0.008123, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 48.2%, Avg loss: 0.008435 \n",
      "\n",
      "batch 6, loss: 0.006451, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 49.5%, Avg loss: 0.006865 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "batch 0, loss: 0.004712, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 49.0%, Avg loss: 0.004722 \n",
      "\n",
      "batch 1, loss: 0.001588, accuracy: 47.7%\n",
      "Test Error: \n",
      "Accuracy: 49.2%, Avg loss: 0.001789 \n",
      "\n",
      "batch 2, loss: 0.003016, accuracy: 52.0%\n",
      "Test Error: \n",
      "Accuracy: 46.0%, Avg loss: 0.003097 \n",
      "\n",
      "batch 3, loss: 0.004330, accuracy: 44.9%\n",
      "Test Error: \n",
      "Accuracy: 45.2%, Avg loss: 0.004307 \n",
      "\n",
      "batch 4, loss: 0.005065, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 45.2%, Avg loss: 0.005527 \n",
      "\n",
      "batch 5, loss: 0.005929, accuracy: 46.5%\n",
      "Test Error: \n",
      "Accuracy: 46.0%, Avg loss: 0.005660 \n",
      "\n",
      "batch 6, loss: 0.004852, accuracy: 40.6%\n",
      "Test Error: \n",
      "Accuracy: 46.8%, Avg loss: 0.004052 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "batch 0, loss: 0.002732, accuracy: 50.4%\n",
      "Test Error: \n",
      "Accuracy: 48.2%, Avg loss: 0.002789 \n",
      "\n",
      "batch 1, loss: 0.001320, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 47.8%, Avg loss: 0.001262 \n",
      "\n",
      "batch 2, loss: 0.002498, accuracy: 46.1%\n",
      "Test Error: \n",
      "Accuracy: 46.8%, Avg loss: 0.002604 \n",
      "\n",
      "batch 3, loss: 0.003123, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 46.5%, Avg loss: 0.003004 \n",
      "\n",
      "batch 4, loss: 0.003527, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 46.5%, Avg loss: 0.003812 \n",
      "\n",
      "batch 5, loss: 0.004275, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 45.5%, Avg loss: 0.004453 \n",
      "\n",
      "batch 6, loss: 0.005133, accuracy: 32.8%\n",
      "Test Error: \n",
      "Accuracy: 43.8%, Avg loss: 0.005481 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "batch 0, loss: 0.005695, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 44.5%, Avg loss: 0.006016 \n",
      "\n",
      "batch 1, loss: 0.005443, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 43.5%, Avg loss: 0.005145 \n",
      "\n",
      "batch 2, loss: 0.004446, accuracy: 39.5%\n",
      "Test Error: \n",
      "Accuracy: 46.2%, Avg loss: 0.004882 \n",
      "\n",
      "batch 3, loss: 0.003519, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 46.5%, Avg loss: 0.003595 \n",
      "\n",
      "batch 4, loss: 0.002753, accuracy: 48.0%\n",
      "Test Error: \n",
      "Accuracy: 50.0%, Avg loss: 0.002824 \n",
      "\n",
      "batch 5, loss: 0.002668, accuracy: 55.9%\n",
      "Test Error: \n",
      "Accuracy: 49.8%, Avg loss: 0.002767 \n",
      "\n",
      "batch 6, loss: 0.003636, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.003361 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "batch 0, loss: 0.004091, accuracy: 47.3%\n",
      "Test Error: \n",
      "Accuracy: 51.5%, Avg loss: 0.003895 \n",
      "\n",
      "batch 1, loss: 0.004096, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.004120 \n",
      "\n",
      "batch 2, loss: 0.003793, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.003695 \n",
      "\n",
      "batch 3, loss: 0.003871, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.004227 \n",
      "\n",
      "batch 4, loss: 0.004220, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.004099 \n",
      "\n",
      "batch 5, loss: 0.004091, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.004016 \n",
      "\n",
      "batch 6, loss: 0.004153, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.004106 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "batch 0, loss: 0.003148, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.003234 \n",
      "\n",
      "batch 1, loss: 0.003249, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.003331 \n",
      "\n",
      "batch 2, loss: 0.003822, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 49.8%, Avg loss: 0.003702 \n",
      "\n",
      "batch 3, loss: 0.003865, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 50.0%, Avg loss: 0.003893 \n",
      "\n",
      "batch 4, loss: 0.004562, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.004486 \n",
      "\n",
      "batch 5, loss: 0.005876, accuracy: 44.1%\n",
      "Test Error: \n",
      "Accuracy: 49.8%, Avg loss: 0.005840 \n",
      "\n",
      "batch 6, loss: 0.006868, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 49.0%, Avg loss: 0.006718 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "batch 0, loss: 0.007402, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 49.5%, Avg loss: 0.007247 \n",
      "\n",
      "batch 1, loss: 0.008258, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 48.2%, Avg loss: 0.008432 \n",
      "\n",
      "batch 2, loss: 0.009564, accuracy: 44.5%\n",
      "Test Error: \n",
      "Accuracy: 48.0%, Avg loss: 0.009529 \n",
      "\n",
      "batch 3, loss: 0.010355, accuracy: 46.5%\n",
      "Test Error: \n",
      "Accuracy: 48.5%, Avg loss: 0.010049 \n",
      "\n",
      "batch 4, loss: 0.010245, accuracy: 48.0%\n",
      "Test Error: \n",
      "Accuracy: 49.0%, Avg loss: 0.010516 \n",
      "\n",
      "batch 5, loss: 0.010310, accuracy: 44.5%\n",
      "Test Error: \n",
      "Accuracy: 48.8%, Avg loss: 0.010635 \n",
      "\n",
      "batch 6, loss: 0.009563, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 48.8%, Avg loss: 0.010114 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "batch 0, loss: 0.008748, accuracy: 47.3%\n",
      "Test Error: \n",
      "Accuracy: 49.5%, Avg loss: 0.008895 \n",
      "\n",
      "batch 1, loss: 0.007855, accuracy: 43.4%\n",
      "Test Error: \n",
      "Accuracy: 49.5%, Avg loss: 0.008162 \n",
      "\n",
      "batch 2, loss: 0.007485, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 49.5%, Avg loss: 0.007127 \n",
      "\n",
      "batch 3, loss: 0.005630, accuracy: 45.7%\n",
      "Test Error: \n",
      "Accuracy: 50.0%, Avg loss: 0.005720 \n",
      "\n",
      "batch 4, loss: 0.005096, accuracy: 46.1%\n",
      "Test Error: \n",
      "Accuracy: 50.0%, Avg loss: 0.005300 \n",
      "\n",
      "batch 5, loss: 0.004303, accuracy: 44.9%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.004288 \n",
      "\n",
      "batch 6, loss: 0.002664, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.002690 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "batch 0, loss: 0.002002, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.001993 \n",
      "\n",
      "batch 1, loss: 0.001785, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 51.5%, Avg loss: 0.001705 \n",
      "\n",
      "batch 2, loss: 0.001939, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.001981 \n",
      "\n",
      "batch 3, loss: 0.002125, accuracy: 48.0%\n",
      "Test Error: \n",
      "Accuracy: 50.7%, Avg loss: 0.002231 \n",
      "\n",
      "batch 4, loss: 0.002451, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.002409 \n",
      "\n",
      "batch 5, loss: 0.002485, accuracy: 45.7%\n",
      "Test Error: \n",
      "Accuracy: 51.5%, Avg loss: 0.002449 \n",
      "\n",
      "batch 6, loss: 0.002828, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 50.7%, Avg loss: 0.003296 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "batch 0, loss: 0.003795, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.003941 \n",
      "\n",
      "batch 1, loss: 0.003784, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 50.0%, Avg loss: 0.003542 \n",
      "\n",
      "batch 2, loss: 0.003293, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 50.7%, Avg loss: 0.003213 \n",
      "\n",
      "batch 3, loss: 0.002827, accuracy: 44.1%\n",
      "Test Error: \n",
      "Accuracy: 50.7%, Avg loss: 0.002767 \n",
      "\n",
      "batch 4, loss: 0.002522, accuracy: 48.0%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.002684 \n",
      "\n",
      "batch 5, loss: 0.001982, accuracy: 43.4%\n",
      "Test Error: \n",
      "Accuracy: 50.7%, Avg loss: 0.001872 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_925953/1949407616.py\", line 29, in train_snn\n",
      "    train_loop_snn(es_model,train_dataloader, val_dataloader, losscustom, run.config.nb_model_samples, run)\n",
      "  File \"/tmp/ipykernel_925953/4221535779.py\", line 48, in train_loop_snn\n",
      "    loss, _ = _run_snn_on_batch(model, x, y, loss_fn)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_925953/4221535779.py\", line 7, in _run_snn_on_batch\n",
      "    spike_train = model(x)\n",
      "                  ^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_925953/1753872684.py\", line 27, in forward\n",
      "    _, mem2 = self.lif2(self.fc2(spk1), mem2)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/snntorch/_neurons/leaky.py\", line 210, in forward\n",
      "    self.mem = self.state_function(input_)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/snntorch/_neurons/leaky.py\", line 243, in _base_sub\n",
      "    return self._base_state_function(input_) - self.reset * self.threshold\n",
      "                                                            ^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1918, in __getattr__\n",
      "    def __getattr__(self, name: str) -> Any:\n",
      "    \n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mtrain_snn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m train_loop_snn(es_model,train_dataloader, val_dataloader, losscustom, run\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnb_model_samples, run)\n",
      "Cell \u001b[0;32mIn[3], line 48\u001b[0m, in \u001b[0;36mtrain_loop_snn\u001b[0;34m(es_model, train_dataloader, val_dataloader, loss_fn, nb_model_samples, run)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m es_model\u001b[38;5;241m.\u001b[39msamples(nb_model_samples):\n\u001b[0;32m---> 48\u001b[0m     loss, _ \u001b[38;5;241m=\u001b[39m _run_snn_on_batch(model, x, y, loss_fn)\n\u001b[1;32m     49\u001b[0m     samples_loss\u001b[38;5;241m.\u001b[39mappend(loss)            \n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36m_run_snn_on_batch\u001b[0;34m(model, x, y, loss_fn)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_snn_on_batch\u001b[39m(model, x, y, loss_fn): \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# shape: [time_steps, batch_size, classes]\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     spike_train \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m      8\u001b[0m     pred_y \u001b[38;5;241m=\u001b[39m spike_to_label(spike_train, scheme \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighest_voltage\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m, in \u001b[0;36mSNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m spk1, mem1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x[t]), mem1)\n\u001b[0;32m---> 27\u001b[0m _, mem2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(spk1), mem2)\n\u001b[1;32m     28\u001b[0m mem2_rec\u001b[38;5;241m.\u001b[39mappend(mem2)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/snntorch/_neurons/leaky.py:210\u001b[0m, in \u001b[0;36mLeaky.forward\u001b[0;34m(self, input_, mem)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem_reset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem)\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_function(input_)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant:\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/snntorch/_neurons/leaky.py:243\u001b[0m, in \u001b[0;36mLeaky._base_sub\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_base_sub\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_):\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_state_function(input_) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py:1918\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m             train_loop_snn(es_model,train_dataloader, val_dataloader, losscustom, run\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnb_model_samples, run)\n\u001b[0;32m---> 31\u001b[0m train_snn()\n",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m, in \u001b[0;36mtrain_snn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_snn\u001b[39m():   \n\u001b[1;32m      2\u001b[0m     config \u001b[38;5;241m=\u001b[39m { \u001b[38;5;66;03m# Dataset:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_input\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      4\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_output\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m2\u001b[39m,  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     20\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregularization\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), wandb\u001b[38;5;241m.\u001b[39minit(entity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDarwinNeuron\u001b[39m\u001b[38;5;124m'\u001b[39m, project \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDarwinNeuron\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig) \u001b[38;5;28;01mas\u001b[39;00m run:  \n\u001b[1;32m     22\u001b[0m         train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m train_test_split(get_randman_dataset(nb_classes\u001b[38;5;241m=\u001b[39mrun\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnb_output, nb_units\u001b[38;5;241m=\u001b[39mrun\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnb_input, nb_steps\u001b[38;5;241m=\u001b[39mrun\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnb_steps, nb_samples\u001b[38;5;241m=\u001b[39mrun\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnb_data_samples), test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m     23\u001b[0m         train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mrun\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:3719\u001b[0m, in \u001b[0;36mRun.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m   3717\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exception(exc_type, exc_val, exc_tb)\n\u001b[1;32m   3718\u001b[0m exit_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 3719\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finish(exit_code\u001b[38;5;241m=\u001b[39mexit_code)\n\u001b[1;32m   3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exception_raised\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:387\u001b[0m, in \u001b[0;36m_log_to_run.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m     run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_id\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging\u001b[38;5;241m.\u001b[39mlog_to_run(run_id):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2190\u001b[0m, in \u001b[0;36mRun._finish\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2184\u001b[0m \u001b[38;5;129m@_log_to_run\u001b[39m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_finish\u001b[39m(\n\u001b[1;32m   2186\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2187\u001b[0m     exit_code: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2188\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2189\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinishing run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_path()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2190\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m telemetry\u001b[38;5;241m.\u001b[39mcontext(run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tel:\n\u001b[1;32m   2191\u001b[0m         tel\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mfinish \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2193\u001b[0m     \u001b[38;5;66;03m# Run hooks that need to happen before the last messages to the\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;66;03m# internal service, like Jupyter hooks.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/lib/telemetry.py:42\u001b[0m, in \u001b[0;36m_TelemetryObject.__exit__\u001b[0;34m(self, exctype, excinst, exctb)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run\u001b[38;5;241m.\u001b[39m_telemetry_callback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:783\u001b[0m, in \u001b[0;36mRun._telemetry_callback\u001b[0;34m(self, telem_obj)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj\u001b[38;5;241m.\u001b[39mMergeFrom(telem_obj)\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj_dirty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_flush()\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:796\u001b[0m, in \u001b[0;36mRun._telemetry_flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m serialized \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj_flushed:\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39m_publish_telemetry(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj)\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj_flushed \u001b[38;5;241m=\u001b[39m serialized\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj_dirty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/interface/interface_shared.py:60\u001b[0m, in \u001b[0;36mInterfaceShared._publish_telemetry\u001b[0;34m(self, telem)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_telemetry\u001b[39m(\u001b[38;5;28mself\u001b[39m, telem: tpb\u001b[38;5;241m.\u001b[39mTelemetryRecord) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_record(telemetry\u001b[38;5;241m=\u001b[39mtelem)\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_publish(rec)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock_client\u001b[38;5;241m.\u001b[39msend_record_publish(record)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_server_request(server_req)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_message(msg)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sendall_with_error_handle(header \u001b[38;5;241m+\u001b[39m data)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msend(data)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f1be392f140>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f1be3989fa0, execution_count=4 error_before_exec=None error_in_exec=[Errno 32] Broken pipe info=<ExecutionInfo object at 7f1be395bb30, raw_cell=\"def train_snn():   \n",
      "    config = { # Dataset:\n",
      "    ..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wyx/darwin_neuron/Workspace.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:547\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpausing backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mpublish_pause()\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/interface/interface.py:769\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    768\u001b[0m     pause \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_publish_pause(pause)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/interface/interface_shared.py:289\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb\u001b[38;5;241m.\u001b[39mPauseRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(pause\u001b[38;5;241m=\u001b[39mpause)\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_publish(rec)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock_client\u001b[38;5;241m.\u001b[39msend_record_publish(record)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_server_request(server_req)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_message(msg)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sendall_with_error_handle(header \u001b[38;5;241m+\u001b[39m data)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msend(data)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "def train_snn():   \n",
    "    config = { # Dataset:\n",
    "              'nb_input' : 100,\n",
    "              'nb_output' : 2,  \n",
    "              'nb_steps' : 200,\n",
    "              'nb_data_samples': 1000,\n",
    "              # SNN:\n",
    "              'nb_hidden' : 100,\n",
    "              'learn_beta' : False,             \n",
    "              # Evolution Strategy:\n",
    "              'nb_model_samples' : 1000, \n",
    "              # Training: \n",
    "              'std' : 0.05,\n",
    "              'epochs' : 50, \n",
    "              'batch_size' : 256,\n",
    "              # Optimization:\n",
    "              'loss': 'Parashbuh',\n",
    "              'optimizer' : 'AdamW',\n",
    "              'lr' : 0.01,\n",
    "              'regularization':'None'}\n",
    "    with torch.no_grad(), wandb.init(entity = 'DarwinNeuron', project = 'DarwinNeuron', config=config) as run:  \n",
    "        train_dataset, val_dataset = train_test_split(get_randman_dataset(nb_classes=run.config.nb_output, nb_units=run.config.nb_input, nb_steps=run.config.nb_steps, nb_samples=run.config.nb_data_samples), test_size=0.2)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=512, shuffle=False)      \n",
    "        es_model = ESModel(SNN, run.config.nb_input, run.config.nb_hidden, run.config.nb_output, 0.95, param_std = run.config.std, Optimizer=optim.AdamW, lr=run.config.lr, weight_decay=1e-3)\n",
    "        for epoch in range(run.config.epochs):\n",
    "            print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "            # train the model\n",
    "            train_loop_snn(es_model,train_dataloader, val_dataloader, losscustom, run.config.nb_model_samples, run)\n",
    "    \n",
    "train_snn() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_retrieve():   \n",
    "  with torch.no_grad(), wandb.init() as run:\n",
    "    model_path = run.use_model('DarwinNeuron/SingleNeuron/run-eouglkvm-test-model.pth:v1')  \n",
    "    model = torch.load(model_path, weights_only=False)\n",
    "\n",
    "test_retrieve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
