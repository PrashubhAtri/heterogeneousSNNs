{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from EvolutionStrategy import ESModel\n",
    "from RandmanFunctions import get_randman_dataset\n",
    "from Utilities import spike_to_label, voltage_to_logits\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "import snntorch as snn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, learn_beta, beta=0.95):\n",
    "        super(SNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden, bias=False)\n",
    "        self.lif1 = snn.Leaky(beta=beta, learn_beta=learn_beta)\n",
    "\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs, bias= False)\n",
    "        self.lif2 = snn.Leaky(beta=beta, learn_beta=learn_beta, reset_mechanism='none')\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                # nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        batch_size, time_steps, num_neurons = x.shape\n",
    "        x = x.permute(1, 0, 2)  # (time, batch, neurons)\n",
    "\n",
    "        mem1, mem2 = [torch.zeros(batch_size, layer.out_features, device=x.device)\n",
    "                      for layer in [self.fc1, self.fc2]]\n",
    "\n",
    "        mem2_rec = []\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            spk1, mem1 = self.lif1(self.fc1(x[t]), mem1)\n",
    "            _, mem2 = self.lif2(self.fc2(spk1), mem2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(mem2_rec, dim=0)  # (time_steps, batch_size, num_outputs)\n",
    "    \n",
    "def losscustom(pred, labels):\n",
    "  # batch, classes, time_steps\n",
    "  # import pdb; pdb.set_trace()\n",
    "  mem = pred.permute(1,2,0)\n",
    "  labels = labels.long()\n",
    "  non_labels = 1-labels\n",
    "\n",
    "  batch_idx = torch.arange(mem.shape[0])\n",
    "\n",
    "  correct = mem[batch_idx, labels]\n",
    "  non_correct = mem[batch_idx, non_labels]\n",
    "\n",
    "  diff = non_correct - correct\n",
    "  diff_activated = torch.where(diff > 0, diff, torch.zeros_like(diff))\n",
    "  return (diff_activated).mean()\n",
    "\n",
    "def regularized_cross_entropy(pred, y):\n",
    "    # pred shape: [batch, classes (logits)]\n",
    "    regularization_term = torch.sigmoid(-15 * torch.abs(pred[:,0] - pred[:, 1]))\n",
    "    \n",
    "    return cross_entropy(pred, y) + regularization_term.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Training SNN for Randman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "def _run_snn_on_batch(model, x, y, loss_fn): \n",
    "    # shape: [time_steps, batch_size, classes]\n",
    "    voltages = model(x)\n",
    "    pred_y = spike_to_label(voltages, scheme = 'highest_voltage')\n",
    "    logits = voltage_to_logits(voltages, scheme='highest-voltage')\n",
    "    \n",
    "    loss = loss_fn(logits, y.long())\n",
    "    correct = (pred_y == y).sum().item()\n",
    "    \n",
    "    return loss, correct\n",
    "\n",
    "def log_model(es_model,run):\n",
    "    filename = 'best-model.pth'\n",
    "    model = es_model.get_best_model()\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    run.log_model(path=filename)\n",
    "    os.remove(filename)  \n",
    "\n",
    "def val_loop_snn(es_model, dataloader, loss_fn):\n",
    "    model = es_model.get_best_model()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        batch_loss, batch_correct = _run_snn_on_batch(model, x, y, loss_fn) \n",
    "        test_loss += batch_loss\n",
    "        correct += batch_correct\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_acc = correct / size\n",
    "    print(f\"Test Error: \\nAccuracy: {(100*test_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "        \n",
    "    return test_loss.item(), test_acc\n",
    "\n",
    "def train_loop_snn(es_model, train_dataloader, val_dataloader, loss_fn, nb_model_samples, run):\n",
    "    \"\"\" one epoch of training, going through all the batches once\n",
    "    \"\"\"    \n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # train model with samples\n",
    "        samples_loss = []\n",
    "        for model in es_model.samples(nb_model_samples):\n",
    "            loss, _ = _run_snn_on_batch(model, x, y, loss_fn)\n",
    "            samples_loss.append(loss)            \n",
    "            \n",
    "        samples_loss = torch.stack(samples_loss) \n",
    "        es_model.gradient_descent(samples_loss)\n",
    "    \n",
    "        # best model loss and accuracy\n",
    "        best_model = es_model.get_best_model()\n",
    "        best_loss, best_correct = _run_snn_on_batch(best_model, x, y, loss_fn)   \n",
    "        best_acc = best_correct / len(y)\n",
    "        print(f\"batch {batch}, loss: {best_loss:>7f}, accuracy: {100 * best_acc:>0.1f}%\")\n",
    "        \n",
    "        # validation loss and accuracy\n",
    "        val_loss, val_acc = val_loop_snn(es_model, val_dataloader, loss_fn)\n",
    "        \n",
    "        # record keeping\n",
    "        run.log({'train_loss': best_loss.item(), 'train_acc' : best_acc, 'val_loss': val_loss, 'val_acc': val_acc}) \n",
    "        log_model(es_model, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myixing\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wyx/darwin_neuron/wandb/run-20250410_044440-wvoaejgq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DarwinNeuron/DarwinNeuron/runs/wvoaejgq' target=\"_blank\">cross-entropy-no-weight</a></strong> to <a href='https://wandb.ai/DarwinNeuron/DarwinNeuron' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DarwinNeuron/DarwinNeuron' target=\"_blank\">https://wandb.ai/DarwinNeuron/DarwinNeuron</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DarwinNeuron/DarwinNeuron/runs/wvoaejgq' target=\"_blank\">https://wandb.ai/DarwinNeuron/DarwinNeuron/runs/wvoaejgq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "batch 0, loss: 0.759352, accuracy: 52.0%\n",
      "Test Error: \n",
      "Accuracy: 43.2%, Avg loss: 0.786760 \n",
      "\n",
      "batch 1, loss: 0.766292, accuracy: 42.6%\n",
      "Test Error: \n",
      "Accuracy: 44.5%, Avg loss: 0.773640 \n",
      "\n",
      "batch 2, loss: 0.766191, accuracy: 43.0%\n",
      "Test Error: \n",
      "Accuracy: 44.1%, Avg loss: 0.761532 \n",
      "\n",
      "batch 3, loss: 0.744340, accuracy: 44.9%\n",
      "Test Error: \n",
      "Accuracy: 45.0%, Avg loss: 0.749920 \n",
      "\n",
      "batch 4, loss: 0.724434, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 45.5%, Avg loss: 0.747345 \n",
      "\n",
      "batch 5, loss: 0.702150, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 45.6%, Avg loss: 0.743667 \n",
      "\n",
      "batch 6, loss: 0.724966, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 47.0%, Avg loss: 0.735923 \n",
      "\n",
      "batch 7, loss: 0.741561, accuracy: 44.1%\n",
      "Test Error: \n",
      "Accuracy: 47.0%, Avg loss: 0.732970 \n",
      "\n",
      "batch 8, loss: 0.716242, accuracy: 46.5%\n",
      "Test Error: \n",
      "Accuracy: 46.6%, Avg loss: 0.731681 \n",
      "\n",
      "batch 9, loss: 0.708410, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 46.6%, Avg loss: 0.729497 \n",
      "\n",
      "batch 10, loss: 0.700385, accuracy: 53.5%\n",
      "Test Error: \n",
      "Accuracy: 46.9%, Avg loss: 0.729159 \n",
      "\n",
      "batch 11, loss: 0.711387, accuracy: 50.4%\n",
      "Test Error: \n",
      "Accuracy: 46.2%, Avg loss: 0.731537 \n",
      "\n",
      "batch 12, loss: 0.697636, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 46.0%, Avg loss: 0.729974 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "batch 0, loss: 0.736208, accuracy: 41.8%\n",
      "Test Error: \n",
      "Accuracy: 46.2%, Avg loss: 0.728885 \n",
      "\n",
      "batch 1, loss: 0.723891, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 46.4%, Avg loss: 0.729018 \n",
      "\n",
      "batch 2, loss: 0.703316, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 46.6%, Avg loss: 0.727469 \n",
      "\n",
      "batch 3, loss: 0.734014, accuracy: 43.0%\n",
      "Test Error: \n",
      "Accuracy: 46.9%, Avg loss: 0.727328 \n",
      "\n",
      "batch 4, loss: 0.710913, accuracy: 45.7%\n",
      "Test Error: \n",
      "Accuracy: 47.4%, Avg loss: 0.727030 \n",
      "\n",
      "batch 5, loss: 0.720312, accuracy: 47.7%\n",
      "Test Error: \n",
      "Accuracy: 46.6%, Avg loss: 0.725917 \n",
      "\n",
      "batch 6, loss: 0.705512, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 47.0%, Avg loss: 0.723350 \n",
      "\n",
      "batch 7, loss: 0.722308, accuracy: 47.3%\n",
      "Test Error: \n",
      "Accuracy: 47.9%, Avg loss: 0.721758 \n",
      "\n",
      "batch 8, loss: 0.717978, accuracy: 45.7%\n",
      "Test Error: \n",
      "Accuracy: 47.9%, Avg loss: 0.719949 \n",
      "\n",
      "batch 9, loss: 0.703788, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 47.6%, Avg loss: 0.719516 \n",
      "\n",
      "batch 10, loss: 0.697489, accuracy: 52.0%\n",
      "Test Error: \n",
      "Accuracy: 47.5%, Avg loss: 0.719649 \n",
      "\n",
      "batch 11, loss: 0.711268, accuracy: 47.7%\n",
      "Test Error: \n",
      "Accuracy: 48.4%, Avg loss: 0.717620 \n",
      "\n",
      "batch 12, loss: 0.699368, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 47.4%, Avg loss: 0.719342 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "batch 0, loss: 0.701513, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 47.8%, Avg loss: 0.718560 \n",
      "\n",
      "batch 1, loss: 0.722882, accuracy: 45.7%\n",
      "Test Error: \n",
      "Accuracy: 48.6%, Avg loss: 0.718211 \n",
      "\n",
      "batch 2, loss: 0.715170, accuracy: 46.5%\n",
      "Test Error: \n",
      "Accuracy: 48.8%, Avg loss: 0.718307 \n",
      "\n",
      "batch 3, loss: 0.720475, accuracy: 44.5%\n",
      "Test Error: \n",
      "Accuracy: 48.5%, Avg loss: 0.718344 \n",
      "\n",
      "batch 4, loss: 0.710235, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 48.4%, Avg loss: 0.718634 \n",
      "\n",
      "batch 5, loss: 0.718413, accuracy: 46.1%\n",
      "Test Error: \n",
      "Accuracy: 48.5%, Avg loss: 0.718822 \n",
      "\n",
      "batch 6, loss: 0.692527, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 49.1%, Avg loss: 0.719179 \n",
      "\n",
      "batch 7, loss: 0.701727, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 49.5%, Avg loss: 0.717690 \n",
      "\n",
      "batch 8, loss: 0.697876, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 48.9%, Avg loss: 0.720111 \n",
      "\n",
      "batch 9, loss: 0.714956, accuracy: 46.1%\n",
      "Test Error: \n",
      "Accuracy: 48.2%, Avg loss: 0.722847 \n",
      "\n",
      "batch 10, loss: 0.701083, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 49.4%, Avg loss: 0.722090 \n",
      "\n",
      "batch 11, loss: 0.705139, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 49.6%, Avg loss: 0.722637 \n",
      "\n",
      "batch 12, loss: 0.724488, accuracy: 44.5%\n",
      "Test Error: \n",
      "Accuracy: 49.8%, Avg loss: 0.723547 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "batch 0, loss: 0.701497, accuracy: 47.3%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.723176 \n",
      "\n",
      "batch 1, loss: 0.728356, accuracy: 47.3%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.723138 \n",
      "\n",
      "batch 2, loss: 0.698479, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 50.7%, Avg loss: 0.722178 \n",
      "\n",
      "batch 3, loss: 0.707421, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 50.4%, Avg loss: 0.722216 \n",
      "\n",
      "batch 4, loss: 0.732749, accuracy: 43.8%\n",
      "Test Error: \n",
      "Accuracy: 50.6%, Avg loss: 0.722904 \n",
      "\n",
      "batch 5, loss: 0.696329, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 50.1%, Avg loss: 0.724567 \n",
      "\n",
      "batch 6, loss: 0.706693, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 49.4%, Avg loss: 0.725584 \n",
      "\n",
      "batch 7, loss: 0.702348, accuracy: 48.0%\n",
      "Test Error: \n",
      "Accuracy: 49.2%, Avg loss: 0.725411 \n",
      "\n",
      "batch 8, loss: 0.707375, accuracy: 50.4%\n",
      "Test Error: \n",
      "Accuracy: 48.8%, Avg loss: 0.726682 \n",
      "\n",
      "batch 9, loss: 0.709326, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 48.4%, Avg loss: 0.726996 \n",
      "\n",
      "batch 10, loss: 0.713984, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 48.0%, Avg loss: 0.728000 \n",
      "\n",
      "batch 11, loss: 0.718734, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 48.2%, Avg loss: 0.727750 \n",
      "\n",
      "batch 12, loss: 0.706181, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 49.4%, Avg loss: 0.725785 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "batch 0, loss: 0.722270, accuracy: 50.4%\n",
      "Test Error: \n",
      "Accuracy: 49.5%, Avg loss: 0.725981 \n",
      "\n",
      "batch 1, loss: 0.707913, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 49.0%, Avg loss: 0.727108 \n",
      "\n",
      "batch 2, loss: 0.713964, accuracy: 47.3%\n",
      "Test Error: \n",
      "Accuracy: 48.9%, Avg loss: 0.728357 \n",
      "\n",
      "batch 3, loss: 0.696869, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 48.9%, Avg loss: 0.727843 \n",
      "\n",
      "batch 4, loss: 0.723701, accuracy: 45.7%\n",
      "Test Error: \n",
      "Accuracy: 48.2%, Avg loss: 0.727649 \n",
      "\n",
      "batch 5, loss: 0.694914, accuracy: 56.6%\n",
      "Test Error: \n",
      "Accuracy: 48.5%, Avg loss: 0.726944 \n",
      "\n",
      "batch 6, loss: 0.713806, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 48.6%, Avg loss: 0.726714 \n",
      "\n",
      "batch 7, loss: 0.719407, accuracy: 48.0%\n",
      "Test Error: \n",
      "Accuracy: 48.4%, Avg loss: 0.726722 \n",
      "\n",
      "batch 8, loss: 0.697207, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 49.0%, Avg loss: 0.725406 \n",
      "\n",
      "batch 9, loss: 0.704673, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 49.0%, Avg loss: 0.724920 \n",
      "\n",
      "batch 10, loss: 0.702065, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 48.9%, Avg loss: 0.724743 \n",
      "\n",
      "batch 11, loss: 0.695524, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 48.2%, Avg loss: 0.725128 \n",
      "\n",
      "batch 12, loss: 0.695179, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 48.4%, Avg loss: 0.725367 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "batch 0, loss: 0.694736, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 49.2%, Avg loss: 0.722939 \n",
      "\n",
      "batch 1, loss: 0.697437, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 49.6%, Avg loss: 0.721555 \n",
      "\n",
      "batch 2, loss: 0.687346, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 49.2%, Avg loss: 0.722494 \n",
      "\n",
      "batch 3, loss: 0.711728, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 49.9%, Avg loss: 0.719379 \n",
      "\n",
      "batch 4, loss: 0.696955, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.717504 \n",
      "\n",
      "batch 5, loss: 0.708812, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 50.9%, Avg loss: 0.715630 \n",
      "\n",
      "batch 6, loss: 0.697909, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.715632 \n",
      "\n",
      "batch 7, loss: 0.698677, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.713787 \n",
      "\n",
      "batch 8, loss: 0.698879, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 51.5%, Avg loss: 0.713668 \n",
      "\n",
      "batch 9, loss: 0.710758, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 52.2%, Avg loss: 0.712258 \n",
      "\n",
      "batch 10, loss: 0.717448, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 53.0%, Avg loss: 0.709935 \n",
      "\n",
      "batch 11, loss: 0.714366, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 53.2%, Avg loss: 0.709085 \n",
      "\n",
      "batch 12, loss: 0.695138, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 54.1%, Avg loss: 0.706858 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "batch 0, loss: 0.712601, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 54.4%, Avg loss: 0.706262 \n",
      "\n",
      "batch 1, loss: 0.678720, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 54.2%, Avg loss: 0.706770 \n",
      "\n",
      "batch 2, loss: 0.671805, accuracy: 59.4%\n",
      "Test Error: \n",
      "Accuracy: 53.6%, Avg loss: 0.706901 \n",
      "\n",
      "batch 3, loss: 0.707680, accuracy: 44.9%\n",
      "Test Error: \n",
      "Accuracy: 53.8%, Avg loss: 0.708007 \n",
      "\n",
      "batch 4, loss: 0.710151, accuracy: 46.1%\n",
      "Test Error: \n",
      "Accuracy: 53.4%, Avg loss: 0.708082 \n",
      "\n",
      "batch 5, loss: 0.716689, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 52.9%, Avg loss: 0.709262 \n",
      "\n",
      "batch 6, loss: 0.702549, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 52.5%, Avg loss: 0.710011 \n",
      "\n",
      "batch 7, loss: 0.708224, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 51.9%, Avg loss: 0.712062 \n",
      "\n",
      "batch 8, loss: 0.709425, accuracy: 48.0%\n",
      "Test Error: \n",
      "Accuracy: 51.6%, Avg loss: 0.712659 \n",
      "\n",
      "batch 9, loss: 0.689494, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 51.1%, Avg loss: 0.713808 \n",
      "\n",
      "batch 10, loss: 0.710541, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.715277 \n",
      "\n",
      "batch 11, loss: 0.716178, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 50.4%, Avg loss: 0.716006 \n",
      "\n",
      "batch 12, loss: 0.711826, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 50.4%, Avg loss: 0.717145 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "batch 0, loss: 0.708603, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 50.6%, Avg loss: 0.716481 \n",
      "\n",
      "batch 1, loss: 0.710019, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 50.0%, Avg loss: 0.717416 \n",
      "\n",
      "batch 2, loss: 0.694740, accuracy: 55.9%\n",
      "Test Error: \n",
      "Accuracy: 49.8%, Avg loss: 0.717913 \n",
      "\n",
      "batch 3, loss: 0.707092, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 50.6%, Avg loss: 0.716650 \n",
      "\n",
      "batch 4, loss: 0.697407, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 50.6%, Avg loss: 0.717049 \n",
      "\n",
      "batch 5, loss: 0.703498, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.716509 \n",
      "\n",
      "batch 6, loss: 0.697189, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.716328 \n",
      "\n",
      "batch 7, loss: 0.698761, accuracy: 48.0%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.716768 \n",
      "\n",
      "batch 8, loss: 0.707289, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.716567 \n",
      "\n",
      "batch 9, loss: 0.690353, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.715231 \n",
      "\n",
      "batch 10, loss: 0.699037, accuracy: 52.0%\n",
      "Test Error: \n",
      "Accuracy: 51.7%, Avg loss: 0.715164 \n",
      "\n",
      "batch 11, loss: 0.699751, accuracy: 53.5%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.714883 \n",
      "\n",
      "batch 12, loss: 0.699043, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.714485 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "batch 0, loss: 0.711401, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 50.6%, Avg loss: 0.714043 \n",
      "\n",
      "batch 1, loss: 0.700591, accuracy: 52.0%\n",
      "Test Error: \n",
      "Accuracy: 50.6%, Avg loss: 0.713155 \n",
      "\n",
      "batch 2, loss: 0.713539, accuracy: 44.9%\n",
      "Test Error: \n",
      "Accuracy: 50.6%, Avg loss: 0.711982 \n",
      "\n",
      "batch 3, loss: 0.685933, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.712766 \n",
      "\n",
      "batch 4, loss: 0.694061, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.711382 \n",
      "\n",
      "batch 5, loss: 0.690398, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 51.7%, Avg loss: 0.708482 \n",
      "\n",
      "batch 6, loss: 0.691605, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 51.9%, Avg loss: 0.708176 \n",
      "\n",
      "batch 7, loss: 0.688286, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 52.1%, Avg loss: 0.707534 \n",
      "\n",
      "batch 8, loss: 0.707538, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 53.1%, Avg loss: 0.704779 \n",
      "\n",
      "batch 9, loss: 0.692046, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 53.1%, Avg loss: 0.704518 \n",
      "\n",
      "batch 10, loss: 0.699270, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 52.9%, Avg loss: 0.704773 \n",
      "\n",
      "batch 11, loss: 0.689181, accuracy: 55.9%\n",
      "Test Error: \n",
      "Accuracy: 52.6%, Avg loss: 0.705251 \n",
      "\n",
      "batch 12, loss: 0.673984, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 53.1%, Avg loss: 0.704384 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "batch 0, loss: 0.708085, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 53.2%, Avg loss: 0.704500 \n",
      "\n",
      "batch 1, loss: 0.677263, accuracy: 58.6%\n",
      "Test Error: \n",
      "Accuracy: 52.4%, Avg loss: 0.706652 \n",
      "\n",
      "batch 2, loss: 0.686198, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 52.8%, Avg loss: 0.706409 \n",
      "\n",
      "batch 3, loss: 0.692121, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 52.8%, Avg loss: 0.706123 \n",
      "\n",
      "batch 4, loss: 0.699706, accuracy: 47.7%\n",
      "Test Error: \n",
      "Accuracy: 53.2%, Avg loss: 0.704856 \n",
      "\n",
      "batch 5, loss: 0.704173, accuracy: 53.5%\n",
      "Test Error: \n",
      "Accuracy: 53.4%, Avg loss: 0.704835 \n",
      "\n",
      "batch 6, loss: 0.690718, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 54.5%, Avg loss: 0.703542 \n",
      "\n",
      "batch 7, loss: 0.717822, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 54.9%, Avg loss: 0.702752 \n",
      "\n",
      "batch 8, loss: 0.674923, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 55.5%, Avg loss: 0.701150 \n",
      "\n",
      "batch 9, loss: 0.686874, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 55.8%, Avg loss: 0.700637 \n",
      "\n",
      "batch 10, loss: 0.697966, accuracy: 53.5%\n",
      "Test Error: \n",
      "Accuracy: 55.5%, Avg loss: 0.700586 \n",
      "\n",
      "batch 11, loss: 0.697585, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 55.1%, Avg loss: 0.701403 \n",
      "\n",
      "batch 12, loss: 0.692555, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 55.1%, Avg loss: 0.700103 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "batch 0, loss: 0.690450, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 55.5%, Avg loss: 0.698978 \n",
      "\n",
      "batch 1, loss: 0.675058, accuracy: 60.2%\n",
      "Test Error: \n",
      "Accuracy: 54.9%, Avg loss: 0.700166 \n",
      "\n",
      "batch 2, loss: 0.701998, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 55.1%, Avg loss: 0.699809 \n",
      "\n",
      "batch 3, loss: 0.704547, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 55.2%, Avg loss: 0.700461 \n",
      "\n",
      "batch 4, loss: 0.675564, accuracy: 53.5%\n",
      "Test Error: \n",
      "Accuracy: 54.8%, Avg loss: 0.700086 \n",
      "\n",
      "batch 5, loss: 0.678188, accuracy: 58.6%\n",
      "Test Error: \n",
      "Accuracy: 54.0%, Avg loss: 0.702065 \n",
      "\n",
      "batch 6, loss: 0.696614, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 53.8%, Avg loss: 0.702628 \n",
      "\n",
      "batch 7, loss: 0.707656, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 53.5%, Avg loss: 0.702788 \n",
      "\n",
      "batch 8, loss: 0.671868, accuracy: 61.3%\n",
      "Test Error: \n",
      "Accuracy: 52.6%, Avg loss: 0.704803 \n",
      "\n",
      "batch 9, loss: 0.703346, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 52.9%, Avg loss: 0.704269 \n",
      "\n",
      "batch 10, loss: 0.711485, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 53.0%, Avg loss: 0.704219 \n",
      "\n",
      "batch 11, loss: 0.679037, accuracy: 58.2%\n",
      "Test Error: \n",
      "Accuracy: 53.1%, Avg loss: 0.703946 \n",
      "\n",
      "batch 12, loss: 0.689908, accuracy: 60.2%\n",
      "Test Error: \n",
      "Accuracy: 53.2%, Avg loss: 0.704393 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "batch 0, loss: 0.695650, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 53.4%, Avg loss: 0.703700 \n",
      "\n",
      "batch 1, loss: 0.678972, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 53.6%, Avg loss: 0.701085 \n",
      "\n",
      "batch 2, loss: 0.705061, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 53.8%, Avg loss: 0.700730 \n",
      "\n",
      "batch 3, loss: 0.680308, accuracy: 56.6%\n",
      "Test Error: \n",
      "Accuracy: 54.0%, Avg loss: 0.701635 \n",
      "\n",
      "batch 4, loss: 0.686891, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 53.6%, Avg loss: 0.701633 \n",
      "\n",
      "batch 5, loss: 0.688004, accuracy: 57.4%\n",
      "Test Error: \n",
      "Accuracy: 53.6%, Avg loss: 0.699537 \n",
      "\n",
      "batch 6, loss: 0.697626, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 53.9%, Avg loss: 0.697863 \n",
      "\n",
      "batch 7, loss: 0.686114, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 54.4%, Avg loss: 0.696233 \n",
      "\n",
      "batch 8, loss: 0.677562, accuracy: 58.6%\n",
      "Test Error: \n",
      "Accuracy: 54.5%, Avg loss: 0.695198 \n",
      "\n",
      "batch 9, loss: 0.676316, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 54.6%, Avg loss: 0.693588 \n",
      "\n",
      "batch 10, loss: 0.686821, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 55.0%, Avg loss: 0.692705 \n",
      "\n",
      "batch 11, loss: 0.675995, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 55.5%, Avg loss: 0.691407 \n",
      "\n",
      "batch 12, loss: 0.686766, accuracy: 59.4%\n",
      "Test Error: \n",
      "Accuracy: 55.1%, Avg loss: 0.690851 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "batch 0, loss: 0.690477, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 55.1%, Avg loss: 0.691336 \n",
      "\n",
      "batch 1, loss: 0.677018, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 54.9%, Avg loss: 0.692485 \n",
      "\n",
      "batch 2, loss: 0.671940, accuracy: 58.6%\n",
      "Test Error: \n",
      "Accuracy: 54.9%, Avg loss: 0.691453 \n",
      "\n",
      "batch 3, loss: 0.692763, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 55.8%, Avg loss: 0.688376 \n",
      "\n",
      "batch 4, loss: 0.679404, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 56.2%, Avg loss: 0.686280 \n",
      "\n",
      "batch 5, loss: 0.688570, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 55.6%, Avg loss: 0.687581 \n",
      "\n",
      "batch 6, loss: 0.681850, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 55.0%, Avg loss: 0.685731 \n",
      "\n",
      "batch 7, loss: 0.658822, accuracy: 62.5%\n",
      "Test Error: \n",
      "Accuracy: 55.2%, Avg loss: 0.687190 \n",
      "\n",
      "batch 8, loss: 0.672494, accuracy: 57.8%\n",
      "Test Error: \n",
      "Accuracy: 56.0%, Avg loss: 0.685891 \n",
      "\n",
      "batch 9, loss: 0.677613, accuracy: 59.0%\n",
      "Test Error: \n",
      "Accuracy: 56.1%, Avg loss: 0.686145 \n",
      "\n",
      "batch 10, loss: 0.673418, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 55.6%, Avg loss: 0.686866 \n",
      "\n",
      "batch 11, loss: 0.648748, accuracy: 64.5%\n",
      "Test Error: \n",
      "Accuracy: 56.1%, Avg loss: 0.686686 \n",
      "\n",
      "batch 12, loss: 0.660503, accuracy: 64.8%\n",
      "Test Error: \n",
      "Accuracy: 56.6%, Avg loss: 0.685216 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "batch 0, loss: 0.656284, accuracy: 60.9%\n",
      "Test Error: \n",
      "Accuracy: 56.6%, Avg loss: 0.686265 \n",
      "\n",
      "batch 1, loss: 0.662417, accuracy: 59.4%\n",
      "Test Error: \n",
      "Accuracy: 56.6%, Avg loss: 0.684250 \n",
      "\n",
      "batch 2, loss: 0.673108, accuracy: 59.0%\n",
      "Test Error: \n",
      "Accuracy: 56.8%, Avg loss: 0.683236 \n",
      "\n",
      "batch 3, loss: 0.670837, accuracy: 62.5%\n",
      "Test Error: \n",
      "Accuracy: 56.1%, Avg loss: 0.686377 \n",
      "\n",
      "batch 4, loss: 0.663126, accuracy: 60.5%\n",
      "Test Error: \n",
      "Accuracy: 55.4%, Avg loss: 0.686500 \n",
      "\n",
      "batch 5, loss: 0.682292, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 55.8%, Avg loss: 0.685956 \n",
      "\n",
      "batch 6, loss: 0.654307, accuracy: 60.2%\n",
      "Test Error: \n",
      "Accuracy: 55.8%, Avg loss: 0.686359 \n",
      "\n",
      "batch 7, loss: 0.672677, accuracy: 58.6%\n",
      "Test Error: \n",
      "Accuracy: 56.1%, Avg loss: 0.684190 \n",
      "\n",
      "batch 8, loss: 0.635554, accuracy: 67.2%\n",
      "Test Error: \n",
      "Accuracy: 56.4%, Avg loss: 0.684070 \n",
      "\n",
      "batch 9, loss: 0.691116, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 56.4%, Avg loss: 0.683232 \n",
      "\n",
      "batch 10, loss: 0.665834, accuracy: 58.6%\n",
      "Test Error: \n",
      "Accuracy: 56.5%, Avg loss: 0.682204 \n",
      "\n",
      "batch 11, loss: 0.664769, accuracy: 60.2%\n",
      "Test Error: \n",
      "Accuracy: 55.9%, Avg loss: 0.682777 \n",
      "\n",
      "batch 12, loss: 0.680534, accuracy: 59.4%\n",
      "Test Error: \n",
      "Accuracy: 56.5%, Avg loss: 0.682854 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "batch 0, loss: 0.687597, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 56.4%, Avg loss: 0.682303 \n",
      "\n",
      "batch 1, loss: 0.677329, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 57.1%, Avg loss: 0.679421 \n",
      "\n",
      "batch 2, loss: 0.667268, accuracy: 60.9%\n",
      "Test Error: \n",
      "Accuracy: 57.0%, Avg loss: 0.679295 \n",
      "\n",
      "batch 3, loss: 0.666611, accuracy: 57.4%\n",
      "Test Error: \n",
      "Accuracy: 57.2%, Avg loss: 0.678591 \n",
      "\n",
      "batch 4, loss: 0.662290, accuracy: 60.2%\n",
      "Test Error: \n",
      "Accuracy: 57.2%, Avg loss: 0.679008 \n",
      "\n",
      "batch 5, loss: 0.657796, accuracy: 63.3%\n",
      "Test Error: \n",
      "Accuracy: 56.9%, Avg loss: 0.677910 \n",
      "\n",
      "batch 6, loss: 0.664575, accuracy: 63.7%\n",
      "Test Error: \n",
      "Accuracy: 57.2%, Avg loss: 0.677427 \n",
      "\n",
      "batch 7, loss: 0.656707, accuracy: 64.1%\n",
      "Test Error: \n",
      "Accuracy: 56.8%, Avg loss: 0.677488 \n",
      "\n",
      "batch 8, loss: 0.667313, accuracy: 59.4%\n",
      "Test Error: \n",
      "Accuracy: 56.5%, Avg loss: 0.678668 \n",
      "\n",
      "batch 9, loss: 0.668396, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 56.1%, Avg loss: 0.679998 \n",
      "\n",
      "batch 10, loss: 0.668225, accuracy: 61.7%\n",
      "Test Error: \n",
      "Accuracy: 56.2%, Avg loss: 0.679574 \n",
      "\n",
      "batch 11, loss: 0.663915, accuracy: 62.5%\n",
      "Test Error: \n",
      "Accuracy: 55.8%, Avg loss: 0.680675 \n",
      "\n",
      "batch 12, loss: 0.673823, accuracy: 63.3%\n",
      "Test Error: \n",
      "Accuracy: 55.6%, Avg loss: 0.678646 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "batch 0, loss: 0.673165, accuracy: 59.8%\n",
      "Test Error: \n",
      "Accuracy: 56.1%, Avg loss: 0.677903 \n",
      "\n",
      "batch 1, loss: 0.661844, accuracy: 59.4%\n",
      "Test Error: \n",
      "Accuracy: 56.1%, Avg loss: 0.678074 \n",
      "\n",
      "batch 2, loss: 0.671656, accuracy: 57.4%\n",
      "Test Error: \n",
      "Accuracy: 55.8%, Avg loss: 0.678382 \n",
      "\n",
      "batch 3, loss: 0.665039, accuracy: 60.5%\n",
      "Test Error: \n",
      "Accuracy: 56.5%, Avg loss: 0.676308 \n",
      "\n",
      "batch 4, loss: 0.667960, accuracy: 59.8%\n",
      "Test Error: \n",
      "Accuracy: 57.0%, Avg loss: 0.675990 \n",
      "\n",
      "batch 5, loss: 0.665249, accuracy: 60.2%\n",
      "Test Error: \n",
      "Accuracy: 57.4%, Avg loss: 0.676490 \n",
      "\n",
      "batch 6, loss: 0.664043, accuracy: 60.9%\n",
      "Test Error: \n",
      "Accuracy: 56.6%, Avg loss: 0.676686 \n",
      "\n",
      "batch 7, loss: 0.650966, accuracy: 63.7%\n",
      "Test Error: \n",
      "Accuracy: 56.8%, Avg loss: 0.675477 \n",
      "\n",
      "batch 8, loss: 0.657569, accuracy: 63.7%\n",
      "Test Error: \n",
      "Accuracy: 56.1%, Avg loss: 0.675739 \n",
      "\n",
      "batch 9, loss: 0.681981, accuracy: 58.6%\n",
      "Test Error: \n",
      "Accuracy: 56.5%, Avg loss: 0.675715 \n",
      "\n",
      "batch 10, loss: 0.661027, accuracy: 58.6%\n",
      "Test Error: \n",
      "Accuracy: 57.6%, Avg loss: 0.674541 \n",
      "\n",
      "batch 11, loss: 0.691147, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 57.4%, Avg loss: 0.674703 \n",
      "\n",
      "batch 12, loss: 0.660484, accuracy: 60.9%\n",
      "Test Error: \n",
      "Accuracy: 57.4%, Avg loss: 0.675748 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "batch 0, loss: 0.685915, accuracy: 56.6%\n",
      "Test Error: \n",
      "Accuracy: 57.8%, Avg loss: 0.674836 \n",
      "\n",
      "batch 1, loss: 0.675747, accuracy: 59.0%\n",
      "Test Error: \n",
      "Accuracy: 58.1%, Avg loss: 0.675806 \n",
      "\n",
      "batch 2, loss: 0.662477, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 57.6%, Avg loss: 0.677466 \n",
      "\n",
      "batch 3, loss: 0.655341, accuracy: 63.7%\n",
      "Test Error: \n",
      "Accuracy: 57.6%, Avg loss: 0.678158 \n",
      "\n",
      "batch 4, loss: 0.689072, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 57.6%, Avg loss: 0.676974 \n",
      "\n",
      "batch 5, loss: 0.656236, accuracy: 60.5%\n",
      "Test Error: \n",
      "Accuracy: 56.9%, Avg loss: 0.677400 \n",
      "\n",
      "batch 6, loss: 0.647920, accuracy: 63.7%\n",
      "Test Error: \n",
      "Accuracy: 55.9%, Avg loss: 0.677163 \n",
      "\n",
      "batch 7, loss: 0.673434, accuracy: 59.0%\n",
      "Test Error: \n",
      "Accuracy: 56.0%, Avg loss: 0.677713 \n",
      "\n",
      "batch 8, loss: 0.674995, accuracy: 60.9%\n",
      "Test Error: \n",
      "Accuracy: 56.9%, Avg loss: 0.677138 \n",
      "\n",
      "batch 9, loss: 0.663950, accuracy: 60.2%\n",
      "Test Error: \n",
      "Accuracy: 57.1%, Avg loss: 0.677453 \n",
      "\n",
      "batch 10, loss: 0.667887, accuracy: 58.2%\n",
      "Test Error: \n",
      "Accuracy: 56.9%, Avg loss: 0.676667 \n",
      "\n",
      "batch 11, loss: 0.671304, accuracy: 56.6%\n",
      "Test Error: \n",
      "Accuracy: 56.5%, Avg loss: 0.676865 \n",
      "\n",
      "batch 12, loss: 0.657145, accuracy: 63.3%\n",
      "Test Error: \n",
      "Accuracy: 56.9%, Avg loss: 0.675330 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "batch 0, loss: 0.653361, accuracy: 63.7%\n",
      "Test Error: \n",
      "Accuracy: 56.6%, Avg loss: 0.676687 \n",
      "\n",
      "batch 1, loss: 0.658135, accuracy: 59.8%\n",
      "Test Error: \n",
      "Accuracy: 57.1%, Avg loss: 0.675593 \n",
      "\n",
      "batch 2, loss: 0.656865, accuracy: 57.4%\n",
      "Test Error: \n",
      "Accuracy: 56.9%, Avg loss: 0.676673 \n",
      "\n",
      "batch 3, loss: 0.672491, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 56.8%, Avg loss: 0.677085 \n",
      "\n",
      "batch 4, loss: 0.650253, accuracy: 60.5%\n",
      "Test Error: \n",
      "Accuracy: 56.8%, Avg loss: 0.677925 \n",
      "\n",
      "batch 5, loss: 0.670880, accuracy: 59.0%\n",
      "Test Error: \n",
      "Accuracy: 56.4%, Avg loss: 0.677667 \n",
      "\n",
      "batch 6, loss: 0.689049, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 55.5%, Avg loss: 0.679101 \n",
      "\n",
      "batch 7, loss: 0.691842, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 54.4%, Avg loss: 0.680617 \n",
      "\n",
      "batch 8, loss: 0.678262, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 55.0%, Avg loss: 0.679329 \n",
      "\n",
      "batch 9, loss: 0.681074, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 54.9%, Avg loss: 0.678839 \n",
      "\n",
      "batch 10, loss: 0.655010, accuracy: 60.9%\n",
      "Test Error: \n",
      "Accuracy: 54.6%, Avg loss: 0.679300 \n",
      "\n",
      "batch 11, loss: 0.679083, accuracy: 57.4%\n",
      "Test Error: \n",
      "Accuracy: 55.1%, Avg loss: 0.679072 \n",
      "\n",
      "batch 12, loss: 0.683539, accuracy: 59.4%\n",
      "Test Error: \n",
      "Accuracy: 55.1%, Avg loss: 0.678762 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "batch 0, loss: 0.672299, accuracy: 57.8%\n",
      "Test Error: \n",
      "Accuracy: 55.2%, Avg loss: 0.678317 \n",
      "\n",
      "batch 1, loss: 0.657173, accuracy: 62.1%\n",
      "Test Error: \n",
      "Accuracy: 56.8%, Avg loss: 0.674839 \n",
      "\n",
      "batch 2, loss: 0.638846, accuracy: 64.1%\n",
      "Test Error: \n",
      "Accuracy: 57.4%, Avg loss: 0.674250 \n",
      "\n",
      "batch 3, loss: 0.665968, accuracy: 59.0%\n",
      "Test Error: \n",
      "Accuracy: 57.9%, Avg loss: 0.674573 \n",
      "\n",
      "batch 4, loss: 0.669319, accuracy: 58.2%\n",
      "Test Error: \n",
      "Accuracy: 58.2%, Avg loss: 0.675349 \n",
      "\n",
      "batch 5, loss: 0.680742, accuracy: 57.4%\n",
      "Test Error: \n",
      "Accuracy: 58.4%, Avg loss: 0.675598 \n",
      "\n",
      "batch 6, loss: 0.658443, accuracy: 63.3%\n",
      "Test Error: \n",
      "Accuracy: 58.1%, Avg loss: 0.674858 \n",
      "\n",
      "batch 7, loss: 0.656633, accuracy: 61.3%\n",
      "Test Error: \n",
      "Accuracy: 58.2%, Avg loss: 0.674300 \n",
      "\n",
      "batch 8, loss: 0.667320, accuracy: 57.8%\n",
      "Test Error: \n",
      "Accuracy: 59.4%, Avg loss: 0.673376 \n",
      "\n",
      "batch 9, loss: 0.657677, accuracy: 57.4%\n",
      "Test Error: \n",
      "Accuracy: 58.4%, Avg loss: 0.674698 \n",
      "\n",
      "batch 10, loss: 0.665496, accuracy: 59.8%\n",
      "Test Error: \n",
      "Accuracy: 58.6%, Avg loss: 0.673083 \n",
      "\n",
      "batch 11, loss: 0.677717, accuracy: 58.2%\n",
      "Test Error: \n",
      "Accuracy: 59.2%, Avg loss: 0.671538 \n",
      "\n",
      "batch 12, loss: 0.663277, accuracy: 60.2%\n",
      "Test Error: \n",
      "Accuracy: 59.0%, Avg loss: 0.671329 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "batch 0, loss: 0.664167, accuracy: 62.5%\n",
      "Test Error: \n",
      "Accuracy: 59.8%, Avg loss: 0.669590 \n",
      "\n",
      "batch 1, loss: 0.661922, accuracy: 61.3%\n",
      "Test Error: \n",
      "Accuracy: 60.5%, Avg loss: 0.667202 \n",
      "\n",
      "batch 2, loss: 0.635598, accuracy: 62.9%\n",
      "Test Error: \n",
      "Accuracy: 60.0%, Avg loss: 0.668482 \n",
      "\n",
      "batch 3, loss: 0.651819, accuracy: 62.9%\n",
      "Test Error: \n",
      "Accuracy: 60.0%, Avg loss: 0.668217 \n",
      "\n",
      "batch 4, loss: 0.633773, accuracy: 65.6%\n",
      "Test Error: \n",
      "Accuracy: 60.0%, Avg loss: 0.666352 \n",
      "\n",
      "batch 5, loss: 0.650500, accuracy: 61.7%\n",
      "Test Error: \n",
      "Accuracy: 60.1%, Avg loss: 0.666108 \n",
      "\n",
      "batch 6, loss: 0.648804, accuracy: 62.9%\n",
      "Test Error: \n",
      "Accuracy: 60.4%, Avg loss: 0.665783 \n",
      "\n",
      "batch 7, loss: 0.666487, accuracy: 59.8%\n",
      "Test Error: \n",
      "Accuracy: 60.8%, Avg loss: 0.662749 \n",
      "\n",
      "batch 8, loss: 0.664757, accuracy: 60.2%\n",
      "Test Error: \n",
      "Accuracy: 60.9%, Avg loss: 0.661537 \n",
      "\n",
      "batch 9, loss: 0.663765, accuracy: 62.1%\n",
      "Test Error: \n",
      "Accuracy: 61.0%, Avg loss: 0.660854 \n",
      "\n",
      "batch 10, loss: 0.649371, accuracy: 66.4%\n",
      "Test Error: \n",
      "Accuracy: 60.8%, Avg loss: 0.661002 \n",
      "\n",
      "batch 11, loss: 0.651519, accuracy: 62.9%\n",
      "Test Error: \n",
      "Accuracy: 60.5%, Avg loss: 0.660654 \n",
      "\n",
      "batch 12, loss: 0.650413, accuracy: 60.9%\n",
      "Test Error: \n",
      "Accuracy: 60.5%, Avg loss: 0.661382 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "batch 0, loss: 0.633604, accuracy: 66.0%\n",
      "Test Error: \n",
      "Accuracy: 61.0%, Avg loss: 0.657712 \n",
      "\n",
      "batch 1, loss: 0.648896, accuracy: 66.0%\n",
      "Test Error: \n",
      "Accuracy: 60.8%, Avg loss: 0.657019 \n",
      "\n",
      "batch 2, loss: 0.648526, accuracy: 62.1%\n",
      "Test Error: \n",
      "Accuracy: 60.6%, Avg loss: 0.655580 \n",
      "\n",
      "batch 3, loss: 0.635214, accuracy: 63.3%\n",
      "Test Error: \n",
      "Accuracy: 61.3%, Avg loss: 0.655509 \n",
      "\n",
      "batch 4, loss: 0.654832, accuracy: 60.5%\n",
      "Test Error: \n",
      "Accuracy: 61.8%, Avg loss: 0.654710 \n",
      "\n",
      "batch 5, loss: 0.641333, accuracy: 67.2%\n",
      "Test Error: \n",
      "Accuracy: 62.4%, Avg loss: 0.651771 \n",
      "\n",
      "batch 6, loss: 0.642846, accuracy: 64.1%\n",
      "Test Error: \n",
      "Accuracy: 62.7%, Avg loss: 0.652712 \n",
      "\n",
      "batch 7, loss: 0.640042, accuracy: 68.4%\n",
      "Test Error: \n",
      "Accuracy: 63.5%, Avg loss: 0.650584 \n",
      "\n",
      "batch 8, loss: 0.642799, accuracy: 60.9%\n",
      "Test Error: \n",
      "Accuracy: 62.9%, Avg loss: 0.651612 \n",
      "\n",
      "batch 9, loss: 0.652135, accuracy: 60.9%\n",
      "Test Error: \n",
      "Accuracy: 62.1%, Avg loss: 0.652224 \n",
      "\n",
      "batch 10, loss: 0.649459, accuracy: 62.1%\n",
      "Test Error: \n",
      "Accuracy: 61.9%, Avg loss: 0.653340 \n",
      "\n",
      "batch 11, loss: 0.638772, accuracy: 63.7%\n",
      "Test Error: \n",
      "Accuracy: 61.9%, Avg loss: 0.654111 \n",
      "\n",
      "batch 12, loss: 0.628391, accuracy: 67.2%\n",
      "Test Error: \n",
      "Accuracy: 62.4%, Avg loss: 0.652151 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "batch 0, loss: 0.656871, accuracy: 60.9%\n",
      "Test Error: \n",
      "Accuracy: 61.8%, Avg loss: 0.653272 \n",
      "\n",
      "batch 1, loss: 0.644370, accuracy: 64.1%\n",
      "Test Error: \n",
      "Accuracy: 62.6%, Avg loss: 0.652301 \n",
      "\n",
      "batch 2, loss: 0.627209, accuracy: 68.8%\n",
      "Test Error: \n",
      "Accuracy: 62.5%, Avg loss: 0.650392 \n",
      "\n",
      "batch 3, loss: 0.649220, accuracy: 63.3%\n",
      "Test Error: \n",
      "Accuracy: 62.9%, Avg loss: 0.648250 \n",
      "\n",
      "batch 4, loss: 0.643525, accuracy: 64.8%\n",
      "Test Error: \n",
      "Accuracy: 63.4%, Avg loss: 0.647079 \n",
      "\n",
      "batch 5, loss: 0.628116, accuracy: 64.1%\n",
      "Test Error: \n",
      "Accuracy: 63.1%, Avg loss: 0.646570 \n",
      "\n",
      "batch 6, loss: 0.639033, accuracy: 64.1%\n",
      "Test Error: \n",
      "Accuracy: 63.6%, Avg loss: 0.645969 \n",
      "\n",
      "batch 7, loss: 0.632136, accuracy: 66.8%\n",
      "Test Error: \n",
      "Accuracy: 63.4%, Avg loss: 0.647332 \n",
      "\n",
      "batch 8, loss: 0.646330, accuracy: 64.5%\n",
      "Test Error: \n",
      "Accuracy: 64.0%, Avg loss: 0.644425 \n",
      "\n",
      "batch 9, loss: 0.640174, accuracy: 65.2%\n",
      "Test Error: \n",
      "Accuracy: 63.9%, Avg loss: 0.643987 \n",
      "\n",
      "batch 10, loss: 0.622031, accuracy: 69.1%\n",
      "Test Error: \n",
      "Accuracy: 63.7%, Avg loss: 0.642999 \n",
      "\n",
      "batch 11, loss: 0.646439, accuracy: 63.7%\n",
      "Test Error: \n",
      "Accuracy: 64.5%, Avg loss: 0.641561 \n",
      "\n",
      "batch 12, loss: 0.631316, accuracy: 65.6%\n",
      "Test Error: \n",
      "Accuracy: 64.0%, Avg loss: 0.642149 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "batch 0, loss: 0.631541, accuracy: 66.4%\n",
      "Test Error: \n",
      "Accuracy: 64.4%, Avg loss: 0.642201 \n",
      "\n",
      "batch 1, loss: 0.644173, accuracy: 64.5%\n",
      "Test Error: \n",
      "Accuracy: 64.2%, Avg loss: 0.641423 \n",
      "\n",
      "batch 2, loss: 0.621924, accuracy: 68.4%\n",
      "Test Error: \n",
      "Accuracy: 63.5%, Avg loss: 0.641634 \n",
      "\n",
      "batch 3, loss: 0.627236, accuracy: 70.3%\n",
      "Test Error: \n",
      "Accuracy: 64.2%, Avg loss: 0.641489 \n",
      "\n",
      "batch 4, loss: 0.644370, accuracy: 63.7%\n",
      "Test Error: \n",
      "Accuracy: 64.0%, Avg loss: 0.641112 \n",
      "\n",
      "batch 5, loss: 0.633489, accuracy: 70.3%\n",
      "Test Error: \n",
      "Accuracy: 64.9%, Avg loss: 0.643125 \n",
      "\n",
      "batch 6, loss: 0.629370, accuracy: 69.1%\n",
      "Test Error: \n",
      "Accuracy: 65.1%, Avg loss: 0.642941 \n",
      "\n",
      "batch 7, loss: 0.633649, accuracy: 68.4%\n",
      "Test Error: \n",
      "Accuracy: 64.5%, Avg loss: 0.643186 \n",
      "\n",
      "batch 8, loss: 0.637846, accuracy: 66.0%\n",
      "Test Error: \n",
      "Accuracy: 64.5%, Avg loss: 0.644847 \n",
      "\n",
      "batch 9, loss: 0.627057, accuracy: 69.1%\n",
      "Test Error: \n",
      "Accuracy: 63.5%, Avg loss: 0.646709 \n",
      "\n",
      "batch 10, loss: 0.655546, accuracy: 62.5%\n",
      "Test Error: \n",
      "Accuracy: 63.5%, Avg loss: 0.647430 \n",
      "\n",
      "batch 11, loss: 0.638773, accuracy: 68.0%\n",
      "Test Error: \n",
      "Accuracy: 63.1%, Avg loss: 0.649465 \n",
      "\n",
      "batch 12, loss: 0.644950, accuracy: 66.4%\n",
      "Test Error: \n",
      "Accuracy: 63.4%, Avg loss: 0.648081 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "batch 0, loss: 0.641174, accuracy: 64.1%\n",
      "Test Error: \n",
      "Accuracy: 64.0%, Avg loss: 0.647249 \n",
      "\n",
      "batch 1, loss: 0.637300, accuracy: 66.8%\n",
      "Test Error: \n",
      "Accuracy: 64.8%, Avg loss: 0.645675 \n",
      "\n",
      "batch 2, loss: 0.632160, accuracy: 66.8%\n",
      "Test Error: \n",
      "Accuracy: 65.0%, Avg loss: 0.644829 \n",
      "\n",
      "batch 3, loss: 0.630650, accuracy: 68.8%\n",
      "Test Error: \n",
      "Accuracy: 65.5%, Avg loss: 0.641785 \n",
      "\n",
      "batch 4, loss: 0.656659, accuracy: 60.2%\n",
      "Test Error: \n",
      "Accuracy: 66.0%, Avg loss: 0.638992 \n",
      "\n",
      "batch 5, loss: 0.636972, accuracy: 69.9%\n",
      "Test Error: \n",
      "Accuracy: 66.9%, Avg loss: 0.636517 \n",
      "\n",
      "batch 6, loss: 0.628105, accuracy: 68.8%\n",
      "Test Error: \n",
      "Accuracy: 66.5%, Avg loss: 0.637741 \n",
      "\n",
      "batch 7, loss: 0.631999, accuracy: 70.3%\n",
      "Test Error: \n",
      "Accuracy: 66.6%, Avg loss: 0.636569 \n",
      "\n",
      "batch 8, loss: 0.636224, accuracy: 66.4%\n",
      "Test Error: \n",
      "Accuracy: 66.0%, Avg loss: 0.637018 \n",
      "\n",
      "batch 9, loss: 0.627827, accuracy: 68.4%\n",
      "Test Error: \n",
      "Accuracy: 66.1%, Avg loss: 0.635866 \n",
      "\n",
      "batch 10, loss: 0.642795, accuracy: 65.6%\n",
      "Test Error: \n",
      "Accuracy: 66.6%, Avg loss: 0.634105 \n",
      "\n",
      "batch 11, loss: 0.641743, accuracy: 65.2%\n",
      "Test Error: \n",
      "Accuracy: 66.4%, Avg loss: 0.632449 \n",
      "\n",
      "batch 12, loss: 0.623192, accuracy: 68.8%\n",
      "Test Error: \n",
      "Accuracy: 66.4%, Avg loss: 0.630702 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "batch 0, loss: 0.637224, accuracy: 66.4%\n",
      "Test Error: \n",
      "Accuracy: 67.6%, Avg loss: 0.625726 \n",
      "\n",
      "batch 1, loss: 0.619235, accuracy: 70.7%\n",
      "Test Error: \n",
      "Accuracy: 67.6%, Avg loss: 0.624486 \n",
      "\n",
      "batch 2, loss: 0.637351, accuracy: 64.8%\n",
      "Test Error: \n",
      "Accuracy: 68.0%, Avg loss: 0.623559 \n",
      "\n",
      "batch 3, loss: 0.623707, accuracy: 66.0%\n",
      "Test Error: \n",
      "Accuracy: 68.1%, Avg loss: 0.622656 \n",
      "\n",
      "batch 4, loss: 0.602716, accuracy: 74.2%\n",
      "Test Error: \n",
      "Accuracy: 68.2%, Avg loss: 0.622016 \n",
      "\n",
      "batch 5, loss: 0.626206, accuracy: 68.4%\n",
      "Test Error: \n",
      "Accuracy: 68.8%, Avg loss: 0.619409 \n",
      "\n",
      "batch 6, loss: 0.629354, accuracy: 67.6%\n",
      "Test Error: \n",
      "Accuracy: 69.0%, Avg loss: 0.619730 \n",
      "\n",
      "batch 7, loss: 0.616146, accuracy: 69.9%\n",
      "Test Error: \n",
      "Accuracy: 69.4%, Avg loss: 0.618272 \n",
      "\n",
      "batch 8, loss: 0.620715, accuracy: 68.8%\n",
      "Test Error: \n",
      "Accuracy: 69.8%, Avg loss: 0.615462 \n",
      "\n",
      "batch 9, loss: 0.628337, accuracy: 65.6%\n",
      "Test Error: \n",
      "Accuracy: 69.6%, Avg loss: 0.615643 \n",
      "\n",
      "batch 10, loss: 0.603989, accuracy: 73.8%\n",
      "Test Error: \n",
      "Accuracy: 69.9%, Avg loss: 0.614386 \n",
      "\n",
      "batch 11, loss: 0.609385, accuracy: 72.7%\n",
      "Test Error: \n",
      "Accuracy: 70.2%, Avg loss: 0.611626 \n",
      "\n",
      "batch 12, loss: 0.608809, accuracy: 73.4%\n",
      "Test Error: \n",
      "Accuracy: 69.1%, Avg loss: 0.611690 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "batch 0, loss: 0.602952, accuracy: 73.0%\n",
      "Test Error: \n",
      "Accuracy: 69.2%, Avg loss: 0.611930 \n",
      "\n",
      "batch 1, loss: 0.622671, accuracy: 67.6%\n",
      "Test Error: \n",
      "Accuracy: 69.6%, Avg loss: 0.611417 \n",
      "\n",
      "batch 2, loss: 0.601758, accuracy: 75.0%\n",
      "Test Error: \n",
      "Accuracy: 69.6%, Avg loss: 0.610312 \n",
      "\n",
      "batch 3, loss: 0.615082, accuracy: 68.0%\n",
      "Test Error: \n",
      "Accuracy: 70.4%, Avg loss: 0.607698 \n",
      "\n",
      "batch 4, loss: 0.602282, accuracy: 71.1%\n",
      "Test Error: \n",
      "Accuracy: 69.5%, Avg loss: 0.610396 \n",
      "\n",
      "batch 5, loss: 0.594137, accuracy: 73.0%\n",
      "Test Error: \n",
      "Accuracy: 70.0%, Avg loss: 0.609176 \n",
      "\n",
      "batch 6, loss: 0.607561, accuracy: 68.8%\n",
      "Test Error: \n",
      "Accuracy: 69.5%, Avg loss: 0.609836 \n",
      "\n",
      "batch 7, loss: 0.613768, accuracy: 70.3%\n",
      "Test Error: \n",
      "Accuracy: 69.5%, Avg loss: 0.610979 \n",
      "\n",
      "batch 8, loss: 0.600421, accuracy: 73.4%\n",
      "Test Error: \n",
      "Accuracy: 68.6%, Avg loss: 0.610586 \n",
      "\n",
      "batch 9, loss: 0.586720, accuracy: 74.2%\n",
      "Test Error: \n",
      "Accuracy: 68.5%, Avg loss: 0.607518 \n",
      "\n",
      "batch 10, loss: 0.596227, accuracy: 72.7%\n",
      "Test Error: \n",
      "Accuracy: 68.4%, Avg loss: 0.607464 \n",
      "\n",
      "batch 11, loss: 0.599225, accuracy: 72.3%\n",
      "Test Error: \n",
      "Accuracy: 67.8%, Avg loss: 0.607248 \n",
      "\n",
      "batch 12, loss: 0.593821, accuracy: 74.2%\n",
      "Test Error: \n",
      "Accuracy: 67.6%, Avg loss: 0.606715 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "batch 0, loss: 0.586786, accuracy: 76.6%\n",
      "Test Error: \n",
      "Accuracy: 67.5%, Avg loss: 0.606745 \n",
      "\n",
      "batch 1, loss: 0.602056, accuracy: 71.9%\n",
      "Test Error: \n",
      "Accuracy: 67.8%, Avg loss: 0.605657 \n",
      "\n",
      "batch 2, loss: 0.599296, accuracy: 70.3%\n",
      "Test Error: \n",
      "Accuracy: 68.1%, Avg loss: 0.604051 \n",
      "\n",
      "batch 3, loss: 0.586507, accuracy: 72.3%\n",
      "Test Error: \n",
      "Accuracy: 68.9%, Avg loss: 0.603697 \n",
      "\n",
      "batch 4, loss: 0.595659, accuracy: 70.7%\n",
      "Test Error: \n",
      "Accuracy: 69.2%, Avg loss: 0.601599 \n",
      "\n",
      "batch 5, loss: 0.582290, accuracy: 75.8%\n",
      "Test Error: \n",
      "Accuracy: 69.8%, Avg loss: 0.598990 \n",
      "\n",
      "batch 6, loss: 0.591549, accuracy: 67.6%\n",
      "Test Error: \n",
      "Accuracy: 70.4%, Avg loss: 0.595497 \n",
      "\n",
      "batch 7, loss: 0.599432, accuracy: 71.1%\n",
      "Test Error: \n",
      "Accuracy: 70.2%, Avg loss: 0.592659 \n",
      "\n",
      "batch 8, loss: 0.590375, accuracy: 74.2%\n",
      "Test Error: \n",
      "Accuracy: 71.2%, Avg loss: 0.590579 \n",
      "\n",
      "batch 9, loss: 0.541635, accuracy: 82.4%\n",
      "Test Error: \n",
      "Accuracy: 71.9%, Avg loss: 0.589427 \n",
      "\n",
      "batch 10, loss: 0.597701, accuracy: 71.5%\n",
      "Test Error: \n",
      "Accuracy: 71.8%, Avg loss: 0.590460 \n",
      "\n",
      "batch 11, loss: 0.581921, accuracy: 74.2%\n",
      "Test Error: \n",
      "Accuracy: 71.4%, Avg loss: 0.589960 \n",
      "\n",
      "batch 12, loss: 0.611936, accuracy: 66.4%\n",
      "Test Error: \n",
      "Accuracy: 70.9%, Avg loss: 0.588720 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "batch 0, loss: 0.570393, accuracy: 74.6%\n",
      "Test Error: \n",
      "Accuracy: 71.0%, Avg loss: 0.588048 \n",
      "\n",
      "batch 1, loss: 0.586976, accuracy: 73.0%\n",
      "Test Error: \n",
      "Accuracy: 71.1%, Avg loss: 0.588494 \n",
      "\n",
      "batch 2, loss: 0.570902, accuracy: 74.6%\n",
      "Test Error: \n",
      "Accuracy: 72.5%, Avg loss: 0.583529 \n",
      "\n",
      "batch 3, loss: 0.572192, accuracy: 72.3%\n",
      "Test Error: \n",
      "Accuracy: 72.9%, Avg loss: 0.581891 \n",
      "\n",
      "batch 4, loss: 0.589196, accuracy: 72.3%\n",
      "Test Error: \n",
      "Accuracy: 73.8%, Avg loss: 0.578166 \n",
      "\n",
      "batch 5, loss: 0.601522, accuracy: 65.6%\n",
      "Test Error: \n",
      "Accuracy: 73.8%, Avg loss: 0.576644 \n",
      "\n",
      "batch 6, loss: 0.542505, accuracy: 77.7%\n",
      "Test Error: \n",
      "Accuracy: 73.2%, Avg loss: 0.575607 \n",
      "\n",
      "batch 7, loss: 0.529990, accuracy: 80.1%\n",
      "Test Error: \n",
      "Accuracy: 74.1%, Avg loss: 0.572062 \n",
      "\n",
      "batch 8, loss: 0.569488, accuracy: 73.0%\n",
      "Test Error: \n",
      "Accuracy: 74.4%, Avg loss: 0.569803 \n",
      "\n",
      "batch 9, loss: 0.570066, accuracy: 73.0%\n",
      "Test Error: \n",
      "Accuracy: 74.6%, Avg loss: 0.567100 \n",
      "\n",
      "batch 10, loss: 0.568001, accuracy: 73.4%\n",
      "Test Error: \n",
      "Accuracy: 74.6%, Avg loss: 0.565989 \n",
      "\n",
      "batch 11, loss: 0.534583, accuracy: 82.0%\n",
      "Test Error: \n",
      "Accuracy: 74.8%, Avg loss: 0.564824 \n",
      "\n",
      "batch 12, loss: 0.528911, accuracy: 77.3%\n",
      "Test Error: \n",
      "Accuracy: 74.2%, Avg loss: 0.564579 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "batch 0, loss: 0.581884, accuracy: 71.5%\n",
      "Test Error: \n",
      "Accuracy: 75.0%, Avg loss: 0.559709 \n",
      "\n",
      "batch 1, loss: 0.533298, accuracy: 79.3%\n",
      "Test Error: \n",
      "Accuracy: 75.8%, Avg loss: 0.557269 \n",
      "\n",
      "batch 2, loss: 0.519920, accuracy: 77.0%\n",
      "Test Error: \n",
      "Accuracy: 75.2%, Avg loss: 0.558147 \n",
      "\n",
      "batch 3, loss: 0.525897, accuracy: 77.0%\n",
      "Test Error: \n",
      "Accuracy: 75.4%, Avg loss: 0.557058 \n",
      "\n",
      "batch 4, loss: 0.540355, accuracy: 77.3%\n",
      "Test Error: \n",
      "Accuracy: 75.0%, Avg loss: 0.552896 \n",
      "\n",
      "batch 5, loss: 0.518458, accuracy: 78.9%\n",
      "Test Error: \n",
      "Accuracy: 74.9%, Avg loss: 0.550466 \n",
      "\n",
      "batch 6, loss: 0.555081, accuracy: 72.3%\n",
      "Test Error: \n",
      "Accuracy: 74.2%, Avg loss: 0.550520 \n",
      "\n",
      "batch 7, loss: 0.526783, accuracy: 75.4%\n",
      "Test Error: \n",
      "Accuracy: 74.5%, Avg loss: 0.545904 \n",
      "\n",
      "batch 8, loss: 0.515891, accuracy: 77.3%\n",
      "Test Error: \n",
      "Accuracy: 74.5%, Avg loss: 0.541453 \n",
      "\n",
      "batch 9, loss: 0.543536, accuracy: 74.2%\n",
      "Test Error: \n",
      "Accuracy: 75.1%, Avg loss: 0.537708 \n",
      "\n",
      "batch 10, loss: 0.531199, accuracy: 74.2%\n",
      "Test Error: \n",
      "Accuracy: 75.6%, Avg loss: 0.534315 \n",
      "\n",
      "batch 11, loss: 0.508430, accuracy: 80.9%\n",
      "Test Error: \n",
      "Accuracy: 75.6%, Avg loss: 0.533209 \n",
      "\n",
      "batch 12, loss: 0.484016, accuracy: 75.8%\n",
      "Test Error: \n",
      "Accuracy: 76.0%, Avg loss: 0.531098 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "batch 0, loss: 0.506845, accuracy: 77.3%\n",
      "Test Error: \n",
      "Accuracy: 75.9%, Avg loss: 0.528519 \n",
      "\n",
      "batch 1, loss: 0.495733, accuracy: 79.7%\n",
      "Test Error: \n",
      "Accuracy: 75.8%, Avg loss: 0.523703 \n",
      "\n",
      "batch 2, loss: 0.509964, accuracy: 77.7%\n",
      "Test Error: \n",
      "Accuracy: 75.6%, Avg loss: 0.520478 \n",
      "\n",
      "batch 3, loss: 0.505473, accuracy: 77.7%\n",
      "Test Error: \n",
      "Accuracy: 75.9%, Avg loss: 0.517604 \n",
      "\n",
      "batch 4, loss: 0.479484, accuracy: 82.4%\n",
      "Test Error: \n",
      "Accuracy: 76.4%, Avg loss: 0.512916 \n",
      "\n",
      "batch 5, loss: 0.487571, accuracy: 81.2%\n",
      "Test Error: \n",
      "Accuracy: 77.0%, Avg loss: 0.506973 \n",
      "\n",
      "batch 6, loss: 0.492005, accuracy: 75.4%\n",
      "Test Error: \n",
      "Accuracy: 77.1%, Avg loss: 0.504999 \n",
      "\n",
      "batch 7, loss: 0.468446, accuracy: 78.5%\n",
      "Test Error: \n",
      "Accuracy: 77.8%, Avg loss: 0.499665 \n",
      "\n",
      "batch 8, loss: 0.503001, accuracy: 77.3%\n",
      "Test Error: \n",
      "Accuracy: 77.9%, Avg loss: 0.494096 \n",
      "\n",
      "batch 9, loss: 0.486567, accuracy: 76.2%\n",
      "Test Error: \n",
      "Accuracy: 77.9%, Avg loss: 0.493404 \n",
      "\n",
      "batch 10, loss: 0.493285, accuracy: 79.3%\n",
      "Test Error: \n",
      "Accuracy: 77.9%, Avg loss: 0.488621 \n",
      "\n",
      "batch 11, loss: 0.444540, accuracy: 82.0%\n",
      "Test Error: \n",
      "Accuracy: 78.1%, Avg loss: 0.485334 \n",
      "\n",
      "batch 12, loss: 0.480845, accuracy: 77.3%\n",
      "Test Error: \n",
      "Accuracy: 78.0%, Avg loss: 0.483647 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "batch 0, loss: 0.455950, accuracy: 77.7%\n",
      "Test Error: \n",
      "Accuracy: 77.9%, Avg loss: 0.479155 \n",
      "\n",
      "batch 1, loss: 0.472852, accuracy: 78.1%\n",
      "Test Error: \n",
      "Accuracy: 78.9%, Avg loss: 0.473342 \n",
      "\n",
      "batch 2, loss: 0.462852, accuracy: 77.0%\n",
      "Test Error: \n",
      "Accuracy: 79.1%, Avg loss: 0.467370 \n",
      "\n",
      "batch 3, loss: 0.445941, accuracy: 79.3%\n",
      "Test Error: \n",
      "Accuracy: 78.9%, Avg loss: 0.458791 \n",
      "\n",
      "batch 4, loss: 0.441887, accuracy: 80.5%\n",
      "Test Error: \n",
      "Accuracy: 79.5%, Avg loss: 0.458278 \n",
      "\n",
      "batch 5, loss: 0.421594, accuracy: 84.0%\n",
      "Test Error: \n",
      "Accuracy: 80.1%, Avg loss: 0.450233 \n",
      "\n",
      "batch 6, loss: 0.481636, accuracy: 79.3%\n",
      "Test Error: \n",
      "Accuracy: 80.2%, Avg loss: 0.447261 \n",
      "\n",
      "batch 7, loss: 0.453447, accuracy: 80.5%\n",
      "Test Error: \n",
      "Accuracy: 80.8%, Avg loss: 0.440690 \n",
      "\n",
      "batch 8, loss: 0.403946, accuracy: 83.2%\n",
      "Test Error: \n",
      "Accuracy: 80.9%, Avg loss: 0.438761 \n",
      "\n",
      "batch 9, loss: 0.482761, accuracy: 78.1%\n",
      "Test Error: \n",
      "Accuracy: 80.8%, Avg loss: 0.436872 \n",
      "\n",
      "batch 10, loss: 0.431785, accuracy: 84.8%\n",
      "Test Error: \n",
      "Accuracy: 81.1%, Avg loss: 0.431781 \n",
      "\n",
      "batch 11, loss: 0.401259, accuracy: 84.4%\n",
      "Test Error: \n",
      "Accuracy: 81.6%, Avg loss: 0.426486 \n",
      "\n",
      "batch 12, loss: 0.410553, accuracy: 82.0%\n",
      "Test Error: \n",
      "Accuracy: 81.8%, Avg loss: 0.420440 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "batch 0, loss: 0.392043, accuracy: 84.0%\n",
      "Test Error: \n",
      "Accuracy: 82.0%, Avg loss: 0.414491 \n",
      "\n",
      "batch 1, loss: 0.445207, accuracy: 80.9%\n",
      "Test Error: \n",
      "Accuracy: 82.4%, Avg loss: 0.409642 \n",
      "\n",
      "batch 2, loss: 0.374670, accuracy: 87.1%\n",
      "Test Error: \n",
      "Accuracy: 81.6%, Avg loss: 0.406903 \n",
      "\n",
      "batch 3, loss: 0.412618, accuracy: 84.8%\n",
      "Test Error: \n",
      "Accuracy: 81.8%, Avg loss: 0.405047 \n",
      "\n",
      "batch 4, loss: 0.365739, accuracy: 84.8%\n",
      "Test Error: \n",
      "Accuracy: 81.8%, Avg loss: 0.399234 \n",
      "\n",
      "batch 5, loss: 0.428048, accuracy: 80.1%\n",
      "Test Error: \n",
      "Accuracy: 82.4%, Avg loss: 0.392771 \n",
      "\n",
      "batch 6, loss: 0.394740, accuracy: 82.8%\n",
      "Test Error: \n",
      "Accuracy: 83.0%, Avg loss: 0.387380 \n",
      "\n",
      "batch 7, loss: 0.412963, accuracy: 79.7%\n",
      "Test Error: \n",
      "Accuracy: 83.1%, Avg loss: 0.380364 \n",
      "\n",
      "batch 8, loss: 0.365020, accuracy: 87.1%\n",
      "Test Error: \n",
      "Accuracy: 83.5%, Avg loss: 0.374219 \n",
      "\n",
      "batch 9, loss: 0.398655, accuracy: 82.4%\n",
      "Test Error: \n",
      "Accuracy: 84.6%, Avg loss: 0.367294 \n",
      "\n",
      "batch 10, loss: 0.360275, accuracy: 86.7%\n",
      "Test Error: \n",
      "Accuracy: 84.4%, Avg loss: 0.363412 \n",
      "\n",
      "batch 11, loss: 0.373424, accuracy: 84.8%\n",
      "Test Error: \n",
      "Accuracy: 84.4%, Avg loss: 0.357964 \n",
      "\n",
      "batch 12, loss: 0.406941, accuracy: 82.8%\n",
      "Test Error: \n",
      "Accuracy: 84.4%, Avg loss: 0.353017 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "batch 0, loss: 0.357196, accuracy: 84.4%\n",
      "Test Error: \n",
      "Accuracy: 84.9%, Avg loss: 0.346155 \n",
      "\n",
      "batch 1, loss: 0.330765, accuracy: 87.5%\n",
      "Test Error: \n",
      "Accuracy: 85.1%, Avg loss: 0.340445 \n",
      "\n",
      "batch 2, loss: 0.344606, accuracy: 84.8%\n",
      "Test Error: \n",
      "Accuracy: 85.5%, Avg loss: 0.335544 \n",
      "\n",
      "batch 3, loss: 0.341243, accuracy: 85.9%\n",
      "Test Error: \n",
      "Accuracy: 86.0%, Avg loss: 0.331852 \n",
      "\n",
      "batch 4, loss: 0.345453, accuracy: 85.2%\n",
      "Test Error: \n",
      "Accuracy: 86.2%, Avg loss: 0.328973 \n",
      "\n",
      "batch 5, loss: 0.312722, accuracy: 88.3%\n",
      "Test Error: \n",
      "Accuracy: 86.6%, Avg loss: 0.326746 \n",
      "\n",
      "batch 6, loss: 0.317684, accuracy: 87.9%\n",
      "Test Error: \n",
      "Accuracy: 87.2%, Avg loss: 0.319477 \n",
      "\n",
      "batch 7, loss: 0.287989, accuracy: 89.8%\n",
      "Test Error: \n",
      "Accuracy: 86.9%, Avg loss: 0.316412 \n",
      "\n",
      "batch 8, loss: 0.306826, accuracy: 88.7%\n",
      "Test Error: \n",
      "Accuracy: 87.0%, Avg loss: 0.313393 \n",
      "\n",
      "batch 9, loss: 0.303832, accuracy: 87.1%\n",
      "Test Error: \n",
      "Accuracy: 87.1%, Avg loss: 0.311031 \n",
      "\n",
      "batch 10, loss: 0.300932, accuracy: 90.6%\n",
      "Test Error: \n",
      "Accuracy: 87.4%, Avg loss: 0.307660 \n",
      "\n",
      "batch 11, loss: 0.315227, accuracy: 87.1%\n",
      "Test Error: \n",
      "Accuracy: 87.6%, Avg loss: 0.305726 \n",
      "\n",
      "batch 12, loss: 0.283876, accuracy: 92.2%\n",
      "Test Error: \n",
      "Accuracy: 87.8%, Avg loss: 0.304300 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "batch 0, loss: 0.280018, accuracy: 90.6%\n",
      "Test Error: \n",
      "Accuracy: 87.5%, Avg loss: 0.300673 \n",
      "\n",
      "batch 1, loss: 0.261347, accuracy: 91.4%\n",
      "Test Error: \n",
      "Accuracy: 87.2%, Avg loss: 0.299340 \n",
      "\n",
      "batch 2, loss: 0.274318, accuracy: 90.2%\n",
      "Test Error: \n",
      "Accuracy: 88.0%, Avg loss: 0.298901 \n",
      "\n",
      "batch 3, loss: 0.317086, accuracy: 87.9%\n",
      "Test Error: \n",
      "Accuracy: 88.1%, Avg loss: 0.297600 \n",
      "\n",
      "batch 4, loss: 0.313116, accuracy: 89.1%\n",
      "Test Error: \n",
      "Accuracy: 88.4%, Avg loss: 0.295766 \n",
      "\n",
      "batch 5, loss: 0.251020, accuracy: 91.8%\n",
      "Test Error: \n",
      "Accuracy: 88.8%, Avg loss: 0.286580 \n",
      "\n",
      "batch 6, loss: 0.289524, accuracy: 89.5%\n",
      "Test Error: \n",
      "Accuracy: 88.8%, Avg loss: 0.283520 \n",
      "\n",
      "batch 7, loss: 0.272267, accuracy: 91.0%\n",
      "Test Error: \n",
      "Accuracy: 89.2%, Avg loss: 0.279887 \n",
      "\n",
      "batch 8, loss: 0.257937, accuracy: 91.8%\n",
      "Test Error: \n",
      "Accuracy: 89.5%, Avg loss: 0.277507 \n",
      "\n",
      "batch 9, loss: 0.273940, accuracy: 90.2%\n",
      "Test Error: \n",
      "Accuracy: 90.0%, Avg loss: 0.275799 \n",
      "\n",
      "batch 10, loss: 0.269337, accuracy: 91.0%\n",
      "Test Error: \n",
      "Accuracy: 90.0%, Avg loss: 0.273823 \n",
      "\n",
      "batch 11, loss: 0.235833, accuracy: 93.4%\n",
      "Test Error: \n",
      "Accuracy: 90.6%, Avg loss: 0.267145 \n",
      "\n",
      "batch 12, loss: 0.260211, accuracy: 94.5%\n",
      "Test Error: \n",
      "Accuracy: 91.0%, Avg loss: 0.264370 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "batch 0, loss: 0.236860, accuracy: 93.4%\n",
      "Test Error: \n",
      "Accuracy: 91.4%, Avg loss: 0.262238 \n",
      "\n",
      "batch 1, loss: 0.266816, accuracy: 90.6%\n",
      "Test Error: \n",
      "Accuracy: 91.9%, Avg loss: 0.256266 \n",
      "\n",
      "batch 2, loss: 0.274652, accuracy: 91.4%\n",
      "Test Error: \n",
      "Accuracy: 91.9%, Avg loss: 0.252256 \n",
      "\n",
      "batch 3, loss: 0.226650, accuracy: 92.6%\n",
      "Test Error: \n",
      "Accuracy: 92.2%, Avg loss: 0.250489 \n",
      "\n",
      "batch 4, loss: 0.225074, accuracy: 93.8%\n",
      "Test Error: \n",
      "Accuracy: 92.2%, Avg loss: 0.246872 \n",
      "\n",
      "batch 5, loss: 0.247444, accuracy: 91.0%\n",
      "Test Error: \n",
      "Accuracy: 91.2%, Avg loss: 0.263205 \n",
      "\n",
      "batch 6, loss: 0.264585, accuracy: 91.4%\n",
      "Test Error: \n",
      "Accuracy: 91.5%, Avg loss: 0.260138 \n",
      "\n",
      "batch 7, loss: 0.230947, accuracy: 93.8%\n",
      "Test Error: \n",
      "Accuracy: 91.5%, Avg loss: 0.256470 \n",
      "\n",
      "batch 8, loss: 0.266804, accuracy: 91.0%\n",
      "Test Error: \n",
      "Accuracy: 91.8%, Avg loss: 0.251501 \n",
      "\n",
      "batch 9, loss: 0.278573, accuracy: 90.2%\n",
      "Test Error: \n",
      "Accuracy: 91.9%, Avg loss: 0.251066 \n",
      "\n",
      "batch 10, loss: 0.217633, accuracy: 94.9%\n",
      "Test Error: \n",
      "Accuracy: 92.2%, Avg loss: 0.246949 \n",
      "\n",
      "batch 11, loss: 0.240907, accuracy: 92.6%\n",
      "Test Error: \n",
      "Accuracy: 92.6%, Avg loss: 0.241126 \n",
      "\n",
      "batch 12, loss: 0.190785, accuracy: 96.9%\n",
      "Test Error: \n",
      "Accuracy: 93.0%, Avg loss: 0.235877 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "batch 0, loss: 0.238957, accuracy: 93.4%\n",
      "Test Error: \n",
      "Accuracy: 92.9%, Avg loss: 0.233196 \n",
      "\n",
      "batch 1, loss: 0.236042, accuracy: 91.8%\n",
      "Test Error: \n",
      "Accuracy: 93.0%, Avg loss: 0.231674 \n",
      "\n",
      "batch 2, loss: 0.200568, accuracy: 94.5%\n",
      "Test Error: \n",
      "Accuracy: 93.1%, Avg loss: 0.230705 \n",
      "\n",
      "batch 3, loss: 0.231896, accuracy: 94.5%\n",
      "Test Error: \n",
      "Accuracy: 93.4%, Avg loss: 0.227001 \n",
      "\n",
      "batch 4, loss: 0.202780, accuracy: 95.7%\n",
      "Test Error: \n",
      "Accuracy: 93.5%, Avg loss: 0.224702 \n",
      "\n",
      "batch 5, loss: 0.247407, accuracy: 90.2%\n",
      "Test Error: \n",
      "Accuracy: 93.4%, Avg loss: 0.223573 \n",
      "\n",
      "batch 6, loss: 0.254934, accuracy: 91.0%\n",
      "Test Error: \n",
      "Accuracy: 93.6%, Avg loss: 0.220015 \n",
      "\n",
      "batch 7, loss: 0.206949, accuracy: 94.1%\n",
      "Test Error: \n",
      "Accuracy: 93.6%, Avg loss: 0.217801 \n",
      "\n",
      "batch 8, loss: 0.217602, accuracy: 95.7%\n",
      "Test Error: \n",
      "Accuracy: 93.9%, Avg loss: 0.215032 \n",
      "\n",
      "batch 9, loss: 0.206807, accuracy: 95.3%\n",
      "Test Error: \n",
      "Accuracy: 94.0%, Avg loss: 0.211385 \n",
      "\n",
      "batch 10, loss: 0.220656, accuracy: 92.6%\n",
      "Test Error: \n",
      "Accuracy: 94.1%, Avg loss: 0.208448 \n",
      "\n",
      "batch 11, loss: 0.203673, accuracy: 92.2%\n",
      "Test Error: \n",
      "Accuracy: 94.5%, Avg loss: 0.205185 \n",
      "\n",
      "batch 12, loss: 0.182250, accuracy: 96.1%\n",
      "Test Error: \n",
      "Accuracy: 94.8%, Avg loss: 0.202759 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "batch 0, loss: 0.212139, accuracy: 93.8%\n",
      "Test Error: \n",
      "Accuracy: 94.9%, Avg loss: 0.200099 \n",
      "\n",
      "batch 1, loss: 0.198007, accuracy: 95.7%\n",
      "Test Error: \n",
      "Accuracy: 95.0%, Avg loss: 0.199073 \n",
      "\n",
      "batch 2, loss: 0.209686, accuracy: 93.8%\n",
      "Test Error: \n",
      "Accuracy: 95.0%, Avg loss: 0.197176 \n",
      "\n",
      "batch 3, loss: 0.183945, accuracy: 95.7%\n",
      "Test Error: \n",
      "Accuracy: 95.1%, Avg loss: 0.195057 \n",
      "\n",
      "batch 4, loss: 0.198876, accuracy: 95.3%\n",
      "Test Error: \n",
      "Accuracy: 94.9%, Avg loss: 0.195255 \n",
      "\n",
      "batch 5, loss: 0.179309, accuracy: 96.1%\n",
      "Test Error: \n",
      "Accuracy: 95.0%, Avg loss: 0.191678 \n",
      "\n",
      "batch 6, loss: 0.201091, accuracy: 94.9%\n",
      "Test Error: \n",
      "Accuracy: 95.1%, Avg loss: 0.187392 \n",
      "\n",
      "batch 7, loss: 0.193284, accuracy: 94.5%\n",
      "Test Error: \n",
      "Accuracy: 95.1%, Avg loss: 0.184908 \n",
      "\n",
      "batch 8, loss: 0.200363, accuracy: 93.4%\n",
      "Test Error: \n",
      "Accuracy: 95.4%, Avg loss: 0.182978 \n",
      "\n",
      "batch 9, loss: 0.172379, accuracy: 95.3%\n",
      "Test Error: \n",
      "Accuracy: 95.2%, Avg loss: 0.180826 \n",
      "\n",
      "batch 10, loss: 0.164113, accuracy: 94.5%\n",
      "Test Error: \n",
      "Accuracy: 95.2%, Avg loss: 0.179006 \n",
      "\n",
      "batch 11, loss: 0.176420, accuracy: 95.7%\n",
      "Test Error: \n",
      "Accuracy: 95.5%, Avg loss: 0.176349 \n",
      "\n",
      "batch 12, loss: 0.141172, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 95.5%, Avg loss: 0.174973 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "batch 0, loss: 0.204571, accuracy: 94.5%\n",
      "Test Error: \n",
      "Accuracy: 95.6%, Avg loss: 0.173280 \n",
      "\n",
      "batch 1, loss: 0.153197, accuracy: 96.9%\n",
      "Test Error: \n",
      "Accuracy: 95.8%, Avg loss: 0.171143 \n",
      "\n",
      "batch 2, loss: 0.163640, accuracy: 95.7%\n",
      "Test Error: \n",
      "Accuracy: 95.8%, Avg loss: 0.171145 \n",
      "\n",
      "batch 3, loss: 0.140602, accuracy: 97.7%\n",
      "Test Error: \n",
      "Accuracy: 96.2%, Avg loss: 0.170610 \n",
      "\n",
      "batch 4, loss: 0.150612, accuracy: 96.5%\n",
      "Test Error: \n",
      "Accuracy: 96.2%, Avg loss: 0.168304 \n",
      "\n",
      "batch 5, loss: 0.153333, accuracy: 97.3%\n",
      "Test Error: \n",
      "Accuracy: 96.0%, Avg loss: 0.167160 \n",
      "\n",
      "batch 6, loss: 0.177148, accuracy: 94.9%\n",
      "Test Error: \n",
      "Accuracy: 96.1%, Avg loss: 0.166975 \n",
      "\n",
      "batch 7, loss: 0.167629, accuracy: 95.3%\n",
      "Test Error: \n",
      "Accuracy: 96.1%, Avg loss: 0.165471 \n",
      "\n",
      "batch 8, loss: 0.183798, accuracy: 94.5%\n",
      "Test Error: \n",
      "Accuracy: 96.1%, Avg loss: 0.164427 \n",
      "\n",
      "batch 9, loss: 0.159871, accuracy: 96.1%\n",
      "Test Error: \n",
      "Accuracy: 96.2%, Avg loss: 0.163110 \n",
      "\n",
      "batch 10, loss: 0.147045, accuracy: 96.9%\n",
      "Test Error: \n",
      "Accuracy: 96.1%, Avg loss: 0.162274 \n",
      "\n",
      "batch 11, loss: 0.163294, accuracy: 96.5%\n",
      "Test Error: \n",
      "Accuracy: 96.0%, Avg loss: 0.162415 \n",
      "\n",
      "batch 12, loss: 0.165179, accuracy: 96.9%\n",
      "Test Error: \n",
      "Accuracy: 96.1%, Avg loss: 0.161313 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "batch 0, loss: 0.147312, accuracy: 96.9%\n",
      "Test Error: \n",
      "Accuracy: 96.2%, Avg loss: 0.160107 \n",
      "\n",
      "batch 1, loss: 0.159483, accuracy: 97.3%\n",
      "Test Error: \n",
      "Accuracy: 96.8%, Avg loss: 0.158362 \n",
      "\n",
      "batch 2, loss: 0.189447, accuracy: 94.1%\n",
      "Test Error: \n",
      "Accuracy: 96.8%, Avg loss: 0.157715 \n",
      "\n",
      "batch 3, loss: 0.131132, accuracy: 97.7%\n",
      "Test Error: \n",
      "Accuracy: 96.6%, Avg loss: 0.157914 \n",
      "\n",
      "batch 4, loss: 0.146548, accuracy: 96.1%\n",
      "Test Error: \n",
      "Accuracy: 96.9%, Avg loss: 0.156051 \n",
      "\n",
      "batch 5, loss: 0.147673, accuracy: 96.1%\n",
      "Test Error: \n",
      "Accuracy: 97.1%, Avg loss: 0.154467 \n",
      "\n",
      "batch 6, loss: 0.157772, accuracy: 96.5%\n",
      "Test Error: \n",
      "Accuracy: 97.1%, Avg loss: 0.153877 \n",
      "\n",
      "batch 7, loss: 0.168139, accuracy: 95.3%\n",
      "Test Error: \n",
      "Accuracy: 97.2%, Avg loss: 0.151956 \n",
      "\n",
      "batch 8, loss: 0.124461, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 97.1%, Avg loss: 0.152453 \n",
      "\n",
      "batch 9, loss: 0.132996, accuracy: 98.0%\n",
      "Test Error: \n",
      "Accuracy: 97.2%, Avg loss: 0.150206 \n",
      "\n",
      "batch 10, loss: 0.150572, accuracy: 96.5%\n",
      "Test Error: \n",
      "Accuracy: 97.2%, Avg loss: 0.147349 \n",
      "\n",
      "batch 11, loss: 0.140417, accuracy: 96.9%\n",
      "Test Error: \n",
      "Accuracy: 97.5%, Avg loss: 0.145434 \n",
      "\n",
      "batch 12, loss: 0.164183, accuracy: 96.1%\n",
      "Test Error: \n",
      "Accuracy: 97.4%, Avg loss: 0.142158 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "batch 0, loss: 0.124225, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 97.5%, Avg loss: 0.140781 \n",
      "\n",
      "batch 1, loss: 0.114930, accuracy: 98.0%\n",
      "Test Error: \n",
      "Accuracy: 97.5%, Avg loss: 0.139321 \n",
      "\n",
      "batch 2, loss: 0.164118, accuracy: 94.9%\n",
      "Test Error: \n",
      "Accuracy: 97.8%, Avg loss: 0.137602 \n",
      "\n",
      "batch 3, loss: 0.139357, accuracy: 96.1%\n",
      "Test Error: \n",
      "Accuracy: 97.6%, Avg loss: 0.135911 \n",
      "\n",
      "batch 4, loss: 0.124474, accuracy: 98.0%\n",
      "Test Error: \n",
      "Accuracy: 97.5%, Avg loss: 0.134340 \n",
      "\n",
      "batch 5, loss: 0.129601, accuracy: 96.5%\n",
      "Test Error: \n",
      "Accuracy: 97.6%, Avg loss: 0.131740 \n",
      "\n",
      "batch 6, loss: 0.114818, accuracy: 97.3%\n",
      "Test Error: \n",
      "Accuracy: 97.6%, Avg loss: 0.130292 \n",
      "\n",
      "batch 7, loss: 0.123493, accuracy: 96.9%\n",
      "Test Error: \n",
      "Accuracy: 97.8%, Avg loss: 0.127910 \n",
      "\n",
      "batch 8, loss: 0.127456, accuracy: 97.3%\n",
      "Test Error: \n",
      "Accuracy: 97.9%, Avg loss: 0.125429 \n",
      "\n",
      "batch 9, loss: 0.154292, accuracy: 96.5%\n",
      "Test Error: \n",
      "Accuracy: 98.1%, Avg loss: 0.124788 \n",
      "\n",
      "batch 10, loss: 0.129988, accuracy: 96.9%\n",
      "Test Error: \n",
      "Accuracy: 98.1%, Avg loss: 0.124271 \n",
      "\n",
      "batch 11, loss: 0.134040, accuracy: 96.9%\n",
      "Test Error: \n",
      "Accuracy: 98.2%, Avg loss: 0.122435 \n",
      "\n",
      "batch 12, loss: 0.142880, accuracy: 96.1%\n",
      "Test Error: \n",
      "Accuracy: 98.2%, Avg loss: 0.122417 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "batch 0, loss: 0.127407, accuracy: 97.7%\n",
      "Test Error: \n",
      "Accuracy: 98.0%, Avg loss: 0.122488 \n",
      "\n",
      "batch 1, loss: 0.124428, accuracy: 97.7%\n",
      "Test Error: \n",
      "Accuracy: 97.8%, Avg loss: 0.122794 \n",
      "\n",
      "batch 2, loss: 0.113300, accuracy: 98.0%\n",
      "Test Error: \n",
      "Accuracy: 97.8%, Avg loss: 0.122438 \n",
      "\n",
      "batch 3, loss: 0.119771, accuracy: 98.0%\n",
      "Test Error: \n",
      "Accuracy: 97.6%, Avg loss: 0.121855 \n",
      "\n",
      "batch 4, loss: 0.100883, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 97.8%, Avg loss: 0.121289 \n",
      "\n",
      "batch 5, loss: 0.134464, accuracy: 96.9%\n",
      "Test Error: \n",
      "Accuracy: 97.6%, Avg loss: 0.120531 \n",
      "\n",
      "batch 6, loss: 0.090018, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 97.6%, Avg loss: 0.119298 \n",
      "\n",
      "batch 7, loss: 0.117574, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 97.5%, Avg loss: 0.118542 \n",
      "\n",
      "batch 8, loss: 0.127381, accuracy: 96.5%\n",
      "Test Error: \n",
      "Accuracy: 97.8%, Avg loss: 0.116463 \n",
      "\n",
      "batch 9, loss: 0.099889, accuracy: 98.0%\n",
      "Test Error: \n",
      "Accuracy: 97.9%, Avg loss: 0.115319 \n",
      "\n",
      "batch 10, loss: 0.118315, accuracy: 96.5%\n",
      "Test Error: \n",
      "Accuracy: 98.0%, Avg loss: 0.113963 \n",
      "\n",
      "batch 11, loss: 0.118398, accuracy: 97.7%\n",
      "Test Error: \n",
      "Accuracy: 98.1%, Avg loss: 0.112725 \n",
      "\n",
      "batch 12, loss: 0.133530, accuracy: 96.1%\n",
      "Test Error: \n",
      "Accuracy: 98.2%, Avg loss: 0.111242 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "batch 0, loss: 0.103734, accuracy: 98.0%\n",
      "Test Error: \n",
      "Accuracy: 98.1%, Avg loss: 0.110833 \n",
      "\n",
      "batch 1, loss: 0.108512, accuracy: 97.7%\n",
      "Test Error: \n",
      "Accuracy: 98.1%, Avg loss: 0.109919 \n",
      "\n",
      "batch 2, loss: 0.118382, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 98.2%, Avg loss: 0.108968 \n",
      "\n",
      "batch 3, loss: 0.107355, accuracy: 97.3%\n",
      "Test Error: \n",
      "Accuracy: 98.4%, Avg loss: 0.107544 \n",
      "\n",
      "batch 4, loss: 0.106902, accuracy: 97.7%\n",
      "Test Error: \n",
      "Accuracy: 98.5%, Avg loss: 0.105023 \n",
      "\n",
      "batch 5, loss: 0.097705, accuracy: 97.7%\n",
      "Test Error: \n",
      "Accuracy: 98.5%, Avg loss: 0.102473 \n",
      "\n",
      "batch 6, loss: 0.083912, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 98.5%, Avg loss: 0.100817 \n",
      "\n",
      "batch 7, loss: 0.102446, accuracy: 97.3%\n",
      "Test Error: \n",
      "Accuracy: 98.8%, Avg loss: 0.099243 \n",
      "\n",
      "batch 8, loss: 0.093745, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 98.8%, Avg loss: 0.098876 \n",
      "\n",
      "batch 9, loss: 0.105488, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 98.8%, Avg loss: 0.098696 \n",
      "\n",
      "batch 10, loss: 0.101154, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 98.8%, Avg loss: 0.098501 \n",
      "\n",
      "batch 11, loss: 0.084349, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 98.8%, Avg loss: 0.097921 \n",
      "\n",
      "batch 12, loss: 0.098460, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 98.9%, Avg loss: 0.096553 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "batch 0, loss: 0.105522, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 98.9%, Avg loss: 0.095830 \n",
      "\n",
      "batch 1, loss: 0.100109, accuracy: 98.0%\n",
      "Test Error: \n",
      "Accuracy: 98.9%, Avg loss: 0.094956 \n",
      "\n",
      "batch 2, loss: 0.113839, accuracy: 98.0%\n",
      "Test Error: \n",
      "Accuracy: 98.9%, Avg loss: 0.094883 \n",
      "\n",
      "batch 3, loss: 0.085180, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 98.9%, Avg loss: 0.093752 \n",
      "\n",
      "batch 4, loss: 0.096440, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 98.9%, Avg loss: 0.093992 \n",
      "\n",
      "batch 5, loss: 0.090337, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 98.9%, Avg loss: 0.093995 \n",
      "\n",
      "batch 6, loss: 0.098373, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 98.9%, Avg loss: 0.093413 \n",
      "\n",
      "batch 7, loss: 0.075128, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 98.9%, Avg loss: 0.092880 \n",
      "\n",
      "batch 8, loss: 0.091732, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 99.1%, Avg loss: 0.090568 \n",
      "\n",
      "batch 9, loss: 0.080178, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 99.1%, Avg loss: 0.090455 \n",
      "\n",
      "batch 10, loss: 0.063391, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.089110 \n",
      "\n",
      "batch 11, loss: 0.068203, accuracy: 100.0%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.088028 \n",
      "\n",
      "batch 12, loss: 0.082075, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.086188 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "batch 0, loss: 0.085658, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.085144 \n",
      "\n",
      "batch 1, loss: 0.094375, accuracy: 98.0%\n",
      "Test Error: \n",
      "Accuracy: 99.4%, Avg loss: 0.083805 \n",
      "\n",
      "batch 2, loss: 0.088748, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 99.4%, Avg loss: 0.082680 \n",
      "\n",
      "batch 3, loss: 0.080653, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 99.4%, Avg loss: 0.081942 \n",
      "\n",
      "batch 4, loss: 0.068401, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.082291 \n",
      "\n",
      "batch 5, loss: 0.067133, accuracy: 100.0%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.083139 \n",
      "\n",
      "batch 6, loss: 0.079086, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.082964 \n",
      "\n",
      "batch 7, loss: 0.062211, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.081451 \n",
      "\n",
      "batch 8, loss: 0.086830, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 99.1%, Avg loss: 0.082483 \n",
      "\n",
      "batch 9, loss: 0.066106, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.1%, Avg loss: 0.082203 \n",
      "\n",
      "batch 10, loss: 0.075303, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.080528 \n",
      "\n",
      "batch 11, loss: 0.087362, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.080082 \n",
      "\n",
      "batch 12, loss: 0.072793, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.078724 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "batch 0, loss: 0.088235, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.078406 \n",
      "\n",
      "batch 1, loss: 0.075799, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.078036 \n",
      "\n",
      "batch 2, loss: 0.071726, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.077479 \n",
      "\n",
      "batch 3, loss: 0.072874, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.1%, Avg loss: 0.077379 \n",
      "\n",
      "batch 4, loss: 0.087484, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 99.1%, Avg loss: 0.076152 \n",
      "\n",
      "batch 5, loss: 0.074359, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.1%, Avg loss: 0.075014 \n",
      "\n",
      "batch 6, loss: 0.061606, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.074683 \n",
      "\n",
      "batch 7, loss: 0.082271, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 98.4%, Avg loss: 0.085274 \n",
      "\n",
      "batch 8, loss: 0.080146, accuracy: 98.0%\n",
      "Test Error: \n",
      "Accuracy: 98.4%, Avg loss: 0.084913 \n",
      "\n",
      "batch 9, loss: 0.084908, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 98.4%, Avg loss: 0.084154 \n",
      "\n",
      "batch 10, loss: 0.090094, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 98.6%, Avg loss: 0.082515 \n",
      "\n",
      "batch 11, loss: 0.067193, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 98.6%, Avg loss: 0.082326 \n",
      "\n",
      "batch 12, loss: 0.080624, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 98.8%, Avg loss: 0.080462 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "batch 0, loss: 0.089986, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 98.8%, Avg loss: 0.079205 \n",
      "\n",
      "batch 1, loss: 0.078042, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 98.8%, Avg loss: 0.078132 \n",
      "\n",
      "batch 2, loss: 0.100236, accuracy: 97.3%\n",
      "Test Error: \n",
      "Accuracy: 98.8%, Avg loss: 0.077264 \n",
      "\n",
      "batch 3, loss: 0.086672, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 98.8%, Avg loss: 0.077220 \n",
      "\n",
      "batch 4, loss: 0.069571, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.5%, Avg loss: 0.068376 \n",
      "\n",
      "batch 5, loss: 0.062919, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 98.8%, Avg loss: 0.076041 \n",
      "\n",
      "batch 6, loss: 0.070342, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 98.9%, Avg loss: 0.074833 \n",
      "\n",
      "batch 7, loss: 0.073600, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 99.0%, Avg loss: 0.074190 \n",
      "\n",
      "batch 8, loss: 0.065506, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.0%, Avg loss: 0.073420 \n",
      "\n",
      "batch 9, loss: 0.073962, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 99.1%, Avg loss: 0.073150 \n",
      "\n",
      "batch 10, loss: 0.057975, accuracy: 100.0%\n",
      "Test Error: \n",
      "Accuracy: 99.1%, Avg loss: 0.072703 \n",
      "\n",
      "batch 11, loss: 0.062547, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 99.1%, Avg loss: 0.072310 \n",
      "\n",
      "batch 12, loss: 0.073919, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.071008 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "batch 0, loss: 0.068880, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.070576 \n",
      "\n",
      "batch 1, loss: 0.057314, accuracy: 100.0%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.069797 \n",
      "\n",
      "batch 2, loss: 0.080759, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.069527 \n",
      "\n",
      "batch 3, loss: 0.056098, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.069610 \n",
      "\n",
      "batch 4, loss: 0.071468, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.068820 \n",
      "\n",
      "batch 5, loss: 0.059269, accuracy: 100.0%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.068454 \n",
      "\n",
      "batch 6, loss: 0.063876, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.067324 \n",
      "\n",
      "batch 7, loss: 0.077585, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 99.1%, Avg loss: 0.067421 \n",
      "\n",
      "batch 8, loss: 0.059089, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.1%, Avg loss: 0.066898 \n",
      "\n",
      "batch 9, loss: 0.066833, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.0%, Avg loss: 0.067319 \n",
      "\n",
      "batch 10, loss: 0.068623, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.1%, Avg loss: 0.066879 \n",
      "\n",
      "batch 11, loss: 0.070652, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 99.2%, Avg loss: 0.067120 \n",
      "\n",
      "batch 12, loss: 0.080943, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.5%, Avg loss: 0.066085 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "batch 0, loss: 0.075980, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.5%, Avg loss: 0.065517 \n",
      "\n",
      "batch 1, loss: 0.051744, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.5%, Avg loss: 0.064904 \n",
      "\n",
      "batch 2, loss: 0.073159, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.5%, Avg loss: 0.064571 \n",
      "\n",
      "batch 3, loss: 0.056546, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.5%, Avg loss: 0.064297 \n",
      "\n",
      "batch 4, loss: 0.065939, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.6%, Avg loss: 0.063418 \n",
      "\n",
      "batch 5, loss: 0.062306, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 99.6%, Avg loss: 0.063102 \n",
      "\n",
      "batch 6, loss: 0.040751, accuracy: 100.0%\n",
      "Test Error: \n",
      "Accuracy: 99.8%, Avg loss: 0.062969 \n",
      "\n",
      "batch 7, loss: 0.057130, accuracy: 100.0%\n",
      "Test Error: \n",
      "Accuracy: 99.8%, Avg loss: 0.062551 \n",
      "\n",
      "batch 8, loss: 0.059013, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.8%, Avg loss: 0.062304 \n",
      "\n",
      "batch 9, loss: 0.062038, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.8%, Avg loss: 0.061807 \n",
      "\n",
      "batch 10, loss: 0.066480, accuracy: 98.8%\n",
      "Test Error: \n",
      "Accuracy: 99.8%, Avg loss: 0.061212 \n",
      "\n",
      "batch 11, loss: 0.057276, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.8%, Avg loss: 0.060729 \n",
      "\n",
      "batch 12, loss: 0.070037, accuracy: 98.4%\n",
      "Test Error: \n",
      "Accuracy: 99.8%, Avg loss: 0.060228 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "batch 0, loss: 0.056070, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.8%, Avg loss: 0.059827 \n",
      "\n",
      "batch 1, loss: 0.054222, accuracy: 99.6%\n",
      "Test Error: \n",
      "Accuracy: 99.8%, Avg loss: 0.059441 \n",
      "\n",
      "batch 2, loss: 0.060857, accuracy: 99.2%\n",
      "Test Error: \n",
      "Accuracy: 99.6%, Avg loss: 0.059149 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_snn():   \n",
    "    run_name = 'cross-entropy-no-weight'\n",
    "    config = { # Dataset:\n",
    "              'nb_input' : 100,\n",
    "              'nb_output' : 2,  \n",
    "              'nb_steps' : 50,\n",
    "              'nb_data_samples': 2000,\n",
    "              # SNN:\n",
    "              'nb_hidden' : 10,\n",
    "              'learn_beta' : False,             \n",
    "              # Evolution Strategy:\n",
    "              'nb_model_samples' : 1000, \n",
    "              # Training: \n",
    "              'std' : 0.05,\n",
    "              'epochs' : 50, \n",
    "              'batch_size' : 256,\n",
    "              # Optimization:\n",
    "              'loss': 'cross-entropy',\n",
    "              'optimizer' : 'Adam',\n",
    "              'lr' : 0.01,\n",
    "              'regularization':'none'}\n",
    "    with torch.no_grad(), wandb.init(entity = 'DarwinNeuron', project = 'DarwinNeuron', name=run_name, config=config) as run:  \n",
    "        train_dataset, val_dataset = train_test_split(get_randman_dataset(run.config.nb_output, run.config.nb_input, run.config.nb_steps, run.config.nb_data_samples), test_size=0.2, shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=512, shuffle=False)      \n",
    "        es_model = ESModel(SNN, run.config.nb_input, run.config.nb_hidden, run.config.nb_output, 0.95, param_std = run.config.std, Optimizer=optim.Adam, lr=run.config.lr)\n",
    "        for epoch in range(run.config.epochs):\n",
    "            print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "            # train the model\n",
    "            train_loop_snn(es_model,train_dataloader, val_dataloader, cross_entropy, run.config.nb_model_samples, run)\n",
    "    \n",
    "train_snn() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ~~print last layer voltage trace over time~~\n",
    "- ~~hidden layer spike train~~\n",
    "- ~~hidden layer voltage~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
