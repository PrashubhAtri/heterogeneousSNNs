{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trivial Evolution for Spiking Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Random Manifold Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import randman\n",
    "from randman import Randman\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for data generation\n",
    "NB_CLASSES = 10\n",
    "NB_UNITS = 3 # number of input neurons / embedding dimensions\n",
    "NB_STEPS = 200\n",
    "SEED = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_spiking_dataset() function from the paper\n",
    "\n",
    "def standardize(x,eps=1e-7):\n",
    "    # x's (which is actually y in the following code) shape will be [samples, units]\n",
    "    # Therefore, 0-axis shows that the author standardize across all samples for each units\n",
    "    mi,_ = x.min(0)\n",
    "    ma,_ = x.max(0)\n",
    "    return (x-mi)/(ma-mi+eps)\n",
    "\n",
    "def make_spiking_dataset(nb_classes=10, nb_units=100, nb_steps=100, step_frac=1.0, dim_manifold=2, nb_spikes=1, nb_samples=1000, alpha=2.0, shuffle=True, classification=True, seed=None):\n",
    "    \"\"\" Generates event-based generalized spiking randman classification/regression dataset. \n",
    "    In this dataset each unit fires a fixed number of spikes. So ratebased or spike count based decoding won't work. \n",
    "    All the information is stored in the relative timing between spikes.\n",
    "    For regression datasets the intrinsic manifold coordinates are returned for each target.\n",
    "    Args: \n",
    "        nb_classes: The number of classes to generate\n",
    "        nb_units: The number of units to assume\n",
    "        nb_steps: The number of time steps to assume\n",
    "        step_frac: Fraction of time steps from beginning of each to contain spikes (default 1.0)\n",
    "        nb_spikes: The number of spikes per unit\n",
    "        nb_samples: Number of samples from each manifold per class\n",
    "        alpha: Randman smoothness parameter\n",
    "        shuffe: Whether to shuffle the dataset\n",
    "        classification: Whether to generate a classification (default) or regression dataset\n",
    "        seed: The random seed (default: None)\n",
    "    Returns: \n",
    "        A tuple of data,labels. The data is structured as numpy array \n",
    "        (sample x event x 2 ) where the last dimension contains    \n",
    "        the relative [0,1] (time,unit) coordinates and labels.\n",
    "    \"\"\"\n",
    "  \n",
    "    data = []\n",
    "    labels = []\n",
    "    targets = []\n",
    "\n",
    "    if SEED is not None:\n",
    "        np.random.seed(SEED)\n",
    "    \n",
    "    max_value = np.iinfo(int).max\n",
    "    randman_seeds = np.random.randint(max_value, size=(nb_classes,nb_spikes) )\n",
    "\n",
    "    for k in range(nb_classes):\n",
    "        x = np.random.rand(nb_samples,dim_manifold)\n",
    "        \n",
    "        # The following code shows that if more than one spike, different spikes, even for the same unit, are generated by independent mappings \n",
    "        submans = [ randman.Randman(nb_units, dim_manifold, alpha=alpha, seed=randman_seeds[k,i]) for i in range(nb_spikes) ]\n",
    "        units = []\n",
    "        times = []\n",
    "        for i,rm in enumerate(submans):\n",
    "            y = rm.eval_manifold(x)\n",
    "            y = standardize(y)\n",
    "            units.append(np.repeat(np.arange(nb_units).reshape(1,-1),nb_samples,axis=0))\n",
    "            times.append(y.numpy())\n",
    "\n",
    "        units = np.concatenate(units,axis=1)\n",
    "        times = np.concatenate(times,axis=1)\n",
    "        events = np.stack([times,units],axis=2)\n",
    "        data.append(events)\n",
    "        labels.append(k*np.ones(len(units)))\n",
    "        targets.append(x)\n",
    "\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    labels = np.array(np.concatenate(labels, axis=0), dtype=int)\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "\n",
    "    if shuffle:\n",
    "        idx = np.arange(len(data))\n",
    "        np.random.shuffle(idx)\n",
    "        data = data[idx]\n",
    "        labels = labels[idx]\n",
    "        targets = targets[idx]\n",
    "\n",
    "    data[:,:,0] *= nb_steps*step_frac\n",
    "    # data = np.array(data, dtype=int)\n",
    "\n",
    "    if classification:\n",
    "        return data, labels\n",
    "    else:\n",
    "        return data, targets\n",
    "    \n",
    "def events_to_spike_train(data):\n",
    "    \"\"\"convert the data generated from manifold to spike train form\n",
    "\n",
    "    Args:\n",
    "        data (array): shape is [samples, nb_events, 2]\n",
    "\n",
    "    Returns:\n",
    "        spike_train: shape is [nb_samples, nb_time_steps, units]\n",
    "    \"\"\"\n",
    "    \n",
    "    # astyle() will discard the decimal to give integer timestep\n",
    "    spike_steps = data[:, :, 0].astype(int)\n",
    "    spike_units = data[:, :, 1].astype(int)\n",
    "    # These will be the indices to entrices in the spike train to be set to 1\n",
    "    \n",
    "    # Use the index on spike train matrix [samples, steps, units]\n",
    "    spike_train = np.zeros((data.shape[0], NB_STEPS, NB_UNITS))\n",
    "    sample_indicies = np.expand_dims(np.arange(data.shape[0]), -1)\n",
    "    spike_train[sample_indicies, spike_steps, spike_units] = 1\n",
    "    \n",
    "    return spike_train    \n",
    "\n",
    "def get_randman_dataset():\n",
    "    \"\"\"generate a TensorDataset encapsulated x and y, where x is spike trains\n",
    "\n",
    "    Returns:\n",
    "        TensorDataset: [nb_samples, time_steps, units] and [nb_samples]\n",
    "    \"\"\"\n",
    "    data, label = make_spiking_dataset(NB_CLASSES, NB_UNITS, NB_STEPS, nb_spikes=1)\n",
    "    spike_train = events_to_spike_train(data)\n",
    "    \n",
    "    spike_train = torch.Tensor(spike_train).to(device)\n",
    "    label = torch.Tensor(label).to(device)\n",
    "    \n",
    "    # encapulate using Torch.Dataset\n",
    "    dataset = TensorDataset(spike_train, label)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NB_HIDDEN_UNITS = int(NB_UNITS * 1.5)\n",
    "BETA = 0.85 # This can also be obtained using exp(-delta_t / tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: SnnTorch uses time-first dimensionality for the input $x$: [time, batch_size, feature_num]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandmanSNN(nn.Module):\n",
    "    '''\n",
    "    Spiking Neural Network with one hidden layer.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(NB_UNITS, NB_HIDDEN_UNITS)\n",
    "        self.lif1 = snn.Leaky(beta = BETA, reset_mechanism = 'subtract')\n",
    "        self.fc2 = nn.Linear(NB_HIDDEN_UNITS, NB_CLASSES)\n",
    "        self.lif2 = snn.Leaky(beta = BETA, reset_mechanism = 'subtract')\n",
    "        \n",
    "    def init_state(self):\n",
    "        # init recordings\n",
    "        self.mem1_rec = []\n",
    "        self.mem2_rec = []\n",
    "        self.spike1_rec = []\n",
    "        self.spike2_rec = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is given in [batch_size, time_steps, units], but SnnTorch uses [time, batch_size, nb_units] for x\n",
    "        # So reshape to whatSnn Torch wants by switching first two axis\n",
    "        x = x.transpose(0, 1)        \n",
    "        \n",
    "        # Initialize membrane potential to 0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        self.init_state()\n",
    "               \n",
    "        for step in range(NB_STEPS):\n",
    "            # Input spike trains are weighted by the synaptic weights to produce current to the first neuron\n",
    "            cur1 = self.fc1(x[step]) # can pull out from loop to do all at once? \n",
    "            \n",
    "            # lif1 accumulates the current and update its membrane potential, decide whether it spikes\n",
    "            spike1, mem1 = self.lif1(cur1, mem1)\n",
    "            \n",
    "            # Similar to cur1 and lif1 operations\n",
    "            cur2 = self.fc2(spike1)\n",
    "            spike2, mem2 = self.lif2(cur2)\n",
    "            \n",
    "            # Write to records\n",
    "            self.mem1_rec.append(mem1)\n",
    "            self.spike1_rec.append(spike1)\n",
    "            self.mem2_rec.append(mem2)\n",
    "            self.spike2_rec.append(spike2)\n",
    "            \n",
    "        # The output is of shape [time_steps, batch_size, classes]\n",
    "        return torch.stack(self.spike2_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spike_to_label(spike_train, scheme = 'most_spikes'):\n",
    "    \"\"\"Convert spike train to the label in one-hot encoded class\n",
    "\n",
    "    Args:\n",
    "        spike_train (tensor): spike train with shape [time_steps, batch_size, classes]\n",
    "        scheme(string): options: 'most_spikes' and 'first_spike'\n",
    "        \n",
    "    Return:\n",
    "        one label for each sample: [batch_size,]\n",
    "    \"\"\"\n",
    "    if scheme == 'most_spikes':\n",
    "        # count number of spikes along the time_steps dimension. Result is [batch_size, classes]\n",
    "        spike_counts = spike_train.count_nonzero(dim=0)\n",
    "        \n",
    "        # pick the index of along the clsses dimension\n",
    "        result = spike_counts.argmax(dim=-1)\n",
    "    else:\n",
    "        raise Exception('Undefined Scheme')\n",
    "    \n",
    "    return result\n",
    "\n",
    "def spike_count(spike_train):\n",
    "    \"\"\"count number of spikes for each spike train\n",
    "\n",
    "    Args:\n",
    "        spike_train (tensor): shape v[time_steps, batch_size, classes]\n",
    "\n",
    "    Returns:\n",
    "        tensor: [batch_size, classes]\n",
    "    \"\"\"\n",
    "    return spike_train.count_nonzero(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Evolution Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def log_grad_wrt_mean(mean, std, v):\n",
    "    ''' calculate the gradient of the log of the loss function with respect to the mean'''\n",
    "    return (v - mean) / std**2\n",
    "\n",
    "class ESParameter:    \n",
    "    def __init__(self, para_means, para_std=1, Optimizer = optim.Adam):\n",
    "        self.means = para_means\n",
    "        self.means.grad = torch.zeros(self.means.shape).to(device)\n",
    "        self.STD = para_std\n",
    "        self.samples = None\n",
    "        self.optimizer = Optimizer([self.means], lr=0.01)\n",
    "        \n",
    "    def sample(self, sample_size):\n",
    "        \"\"\"draw samples for each parameter from normal distribution with self.means and self.STD.\n",
    "\n",
    "        Args:\n",
    "            sample_size (int): number of samples\n",
    "\n",
    "        Returns:\n",
    "            tensor: shape [sample_size, ...shape of parameters...]\n",
    "        \"\"\"\n",
    "        # insert sample_size dimension\n",
    "        sample_means = self.means.unsqueeze(0)\n",
    "        \n",
    "        # the shape is for repeat(), it is [sample_size, 1, ... ,1]\n",
    "        shape = [1] * len(sample_means.shape)\n",
    "        shape[0] = sample_size\n",
    "        \n",
    "        # duplicate means along the sample size dimension\n",
    "        sample_means = sample_means.repeat(shape)\n",
    "        \n",
    "        # draw samples\n",
    "        self.samples = torch.normal(mean=sample_means, std=self.STD)\n",
    "        \n",
    "        return self.samples\n",
    "    \n",
    "    def gradient_descent(self, loss):\n",
    "        \"\"\" Move the means of the parameters against gradient. The gradient is calculated based on loss.\n",
    "            And self.optimizer will be used to step.\n",
    "\n",
    "        Args:\n",
    "            loss (Tensor): with shape [nb_samples,]\n",
    "        \"\"\"\n",
    "        # shape of self.samples = [nb_samples, ...weight shape...]\n",
    "        \n",
    "        # Result is the gradients for each prameter, so the shape should match\n",
    "        # with the parameters, which is [...weight shape...].\n",
    "        \n",
    "        # Calculate the gradient for each sample weight, so log_grad will have [nb_samples, ...weight shape...]\n",
    "        log_grad = log_grad_wrt_mean(self.means, self.STD, self.samples)\n",
    "        \n",
    "        ## Calculate the sum of log_grad = [nb_samples, ...weight shape...] and loss = [nb_samples,]\n",
    "        # Reshape loss for broadcasting to [nb_samples, 1....1]\n",
    "        new_shape = [loss.shape[0]] + [1] * (len(log_grad.shape) - 1)\n",
    "        \n",
    "        # grad is now [nb_samples, ...weight shape...]\n",
    "        grad = log_grad * loss.reshape(new_shape)\n",
    "        \n",
    "        # Take average across sample dimension to estimate the expectation\n",
    "        self.means.grad = grad.mean(dim=0)\n",
    "        \n",
    "        # step the optimizer\n",
    "        self.optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, I should run all the data samples for each of the model sample. Later I can do batch instead of whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO on train function:\n",
    "- ~~add vallidation loop after each train. How to validate though - use the mean to produce model maybe.~~\n",
    "- Training the fc1 layer first, then loop through all layers.\n",
    "- write function to encapsulate training across all batches, and possibly other loops.\n",
    "- If still not performing well\n",
    "    - try Spike Count-Based Hinge Loss\n",
    "    - look back at the paper and try the one parameter model it mentioned. Then scale back to this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, loss_fn, nb_model_samples=30, epochs = 100):       \n",
    "    with torch.no_grad(): # ES doesn't need gradient \n",
    "    \n",
    "        # Use a instance of model to initialize ESParameter\n",
    "        fc1_param = ESParameter(RandmanSNN().fc1.weight.clone().detach().to(device))\n",
    "        \n",
    "        for i in range(epochs):        \n",
    "            # Prepare loss for each model sample\n",
    "            models_losses = []\n",
    "            models_acc = []\n",
    "            \n",
    "            # Monte carlo: run each model sample\n",
    "            for fc1_param_sample in fc1_param.sample(nb_model_samples):           \n",
    "                batch_loss = 0\n",
    "                batch_correct = 0\n",
    "                \n",
    "                # Assign params to model\n",
    "                model = RandmanSNN().to(device)\n",
    "                model.fc1.weight.copy_(fc1_param_sample)\n",
    "                \n",
    "                # run the model for each batch \n",
    "                for x, y in dataloader:\n",
    "                    # shape: [time_steps, batch_size, classes]\n",
    "                    out_spikes = model(x)\n",
    "                    \n",
    "                    # shape: [batch_size, classes], the classes dimension can be treated as logits\n",
    "                    pred_spike_counts = spike_count(out_spikes)                \n",
    "                    \n",
    "                    # Crossentropy loss function requires pred.dtype=float32 and y.dtype=int64, maybe other loss have diff requirements? \n",
    "                    batch_loss += loss_fn(pred_spike_counts.float(), y.long())\n",
    "                    \n",
    "                    # add number of correct in this batch \n",
    "                    batch_correct += (spike_to_label(out_spikes) == y).sum()\n",
    "            \n",
    "                # The loss for a model is the average across batches\n",
    "                models_losses.append(batch_loss / len(dataloader))\n",
    "                models_acc.append(batch_correct / len(dataloader.dataset))\n",
    "            \n",
    "            # convert loss list to tensor\n",
    "            models_losses = torch.stack(models_losses, dim=0)\n",
    "            models_acc = torch.stack(models_acc, dim=0)\n",
    "            \n",
    "            # update the model with grdient\n",
    "            fc1_param.gradient_descent(models_losses)\n",
    "            \n",
    "            print(f'Epoch {i}: Average models loss: {models_losses.mean():.2f}; accuracy: {models_acc.mean():.2f}')         \n",
    "            \n",
    "    return fc1_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0Average models loss: 42.17; accuracy: 0.10\n",
      "Epoch  1Average models loss: 49.18; accuracy: 0.10\n",
      "Epoch  2Average models loss: 43.80; accuracy: 0.10\n",
      "Epoch  3Average models loss: 45.12; accuracy: 0.10\n",
      "Epoch  4Average models loss: 47.50; accuracy: 0.10\n",
      "Epoch  5Average models loss: 47.74; accuracy: 0.10\n",
      "Epoch  6Average models loss: 47.61; accuracy: 0.10\n",
      "Epoch  7Average models loss: 51.87; accuracy: 0.10\n",
      "Epoch  8Average models loss: 48.43; accuracy: 0.10\n",
      "Epoch  9Average models loss: 48.49; accuracy: 0.10\n",
      "Epoch  10Average models loss: 47.46; accuracy: 0.10\n",
      "Epoch  11Average models loss: 45.57; accuracy: 0.10\n",
      "Epoch  12Average models loss: 45.87; accuracy: 0.10\n",
      "Epoch  13Average models loss: 46.77; accuracy: 0.10\n",
      "Epoch  14Average models loss: 47.16; accuracy: 0.10\n",
      "Epoch  15Average models loss: 50.97; accuracy: 0.10\n",
      "Epoch  16Average models loss: 40.56; accuracy: 0.10\n",
      "Epoch  17Average models loss: 48.86; accuracy: 0.10\n",
      "Epoch  18Average models loss: 44.70; accuracy: 0.10\n",
      "Epoch  19Average models loss: 47.38; accuracy: 0.10\n",
      "Epoch  20Average models loss: 53.29; accuracy: 0.10\n",
      "Epoch  21Average models loss: 46.65; accuracy: 0.10\n",
      "Epoch  22Average models loss: 47.96; accuracy: 0.10\n",
      "Epoch  23Average models loss: 45.97; accuracy: 0.10\n",
      "Epoch  24Average models loss: 46.41; accuracy: 0.10\n",
      "Epoch  25Average models loss: 52.39; accuracy: 0.10\n",
      "Epoch  26Average models loss: 46.70; accuracy: 0.10\n",
      "Epoch  27Average models loss: 50.67; accuracy: 0.10\n",
      "Epoch  28Average models loss: 53.98; accuracy: 0.10\n",
      "Epoch  29Average models loss: 51.72; accuracy: 0.10\n",
      "Epoch  30Average models loss: 45.40; accuracy: 0.10\n",
      "Epoch  31Average models loss: 47.91; accuracy: 0.10\n",
      "Epoch  32Average models loss: 52.75; accuracy: 0.10\n",
      "Epoch  33Average models loss: 49.10; accuracy: 0.10\n",
      "Epoch  34Average models loss: 50.27; accuracy: 0.10\n",
      "Epoch  35Average models loss: 44.80; accuracy: 0.10\n",
      "Epoch  36Average models loss: 39.50; accuracy: 0.10\n",
      "Epoch  37Average models loss: 49.83; accuracy: 0.10\n",
      "Epoch  38Average models loss: 47.83; accuracy: 0.10\n",
      "Epoch  39Average models loss: 47.37; accuracy: 0.10\n",
      "Epoch  40Average models loss: 52.63; accuracy: 0.10\n",
      "Epoch  41Average models loss: 49.32; accuracy: 0.10\n",
      "Epoch  42Average models loss: 47.00; accuracy: 0.10\n",
      "Epoch  43Average models loss: 47.55; accuracy: 0.10\n",
      "Epoch  44Average models loss: 52.03; accuracy: 0.10\n",
      "Epoch  45Average models loss: 46.71; accuracy: 0.10\n",
      "Epoch  46Average models loss: 47.32; accuracy: 0.10\n",
      "Epoch  47Average models loss: 50.40; accuracy: 0.10\n",
      "Epoch  48Average models loss: 44.11; accuracy: 0.10\n",
      "Epoch  49Average models loss: 48.54; accuracy: 0.10\n",
      "Epoch  50Average models loss: 48.43; accuracy: 0.10\n",
      "Epoch  51Average models loss: 48.78; accuracy: 0.10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      9\u001b[0m     train(dataloader, loss_fn)\n\u001b[0;32m---> 12\u001b[0m test_train()\n",
      "Cell \u001b[0;32mIn[88], line 9\u001b[0m, in \u001b[0;36mtest_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mSubset(get_randman_dataset(), np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m600\u001b[39m)),batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 9\u001b[0m train(dataloader, loss_fn)\n",
      "Cell \u001b[0;32mIn[87], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, loss_fn, nb_model_samples, epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# run the model for each batch \u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# shape: [time_steps, batch_size, classes]\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     out_spikes \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# shape: [batch_size, classes], the classes dimension can be treated as logits\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     pred_spike_counts \u001b[38;5;241m=\u001b[39m spike_count(out_spikes)                \n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m, in \u001b[0;36mRandmanSNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m cur1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x[step]) \u001b[38;5;66;03m# can pull out from loop to do all at once? \u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# lif1 accumulates the current and update its membrane potential, decide whether it spikes\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m spike1, mem1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif1(cur1, mem1)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Similar to cur1 and lif1 operations\u001b[39;00m\n\u001b[1;32m     37\u001b[0m cur2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(spike1)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/snntorch/_neurons/leaky.py:199\u001b[0m, in \u001b[0;36mLeaky.forward\u001b[0;34m(self, input_, mem)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_, mem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mem \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m mem\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_hidden \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mem \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    203\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`mem` should not be passed as an argument while `init_hidden=True`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/torch/nn/modules/module.py:2016\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(name, value, persistent)\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2016\u001b[0m     sign \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer)\n\u001b[1;32m   2017\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersistent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sign\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[1;32m   2018\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(name, value, persistent)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/inspect.py:3345\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Signature\u001b[38;5;241m.\u001b[39mfrom_callable(obj, follow_wrapped\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[1;32m   3346\u001b[0m                                    \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/inspect.py:3085\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   3082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   3083\u001b[0m                   follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   3086\u001b[0m                                     follow_wrapper_chains\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[1;32m   3087\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/inspect.py:2527\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m is not a callable object\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj))\n\u001b[1;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mMethodType):\n\u001b[1;32m   2525\u001b[0m     \u001b[38;5;66;03m# In this case we skip the first parameter of the underlying\u001b[39;00m\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;66;03m# function (usually `self` or `cls`).\u001b[39;00m\n\u001b[0;32m-> 2527\u001b[0m     sig \u001b[38;5;241m=\u001b[39m _get_signature_of(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__func__\u001b[39m)\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_bound_arg:\n\u001b[1;32m   2530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _signature_bound_method(sig)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/inspect.py:2597\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mreplace(parameters\u001b[38;5;241m=\u001b[39mnew_params)\n\u001b[1;32m   2594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isfunction(obj) \u001b[38;5;129;01mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[1;32m   2595\u001b[0m     \u001b[38;5;66;03m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;66;03m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[0;32m-> 2597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_function(sigcls, obj,\n\u001b[1;32m   2598\u001b[0m                                     skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg,\n\u001b[1;32m   2599\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n\u001b[1;32m   2601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[1;32m   2602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[1;32m   2603\u001b[0m                                    skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/inspect.py:2487\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2482\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(Parameter(name, annotation\u001b[38;5;241m=\u001b[39mannotation,\n\u001b[1;32m   2483\u001b[0m                                 kind\u001b[38;5;241m=\u001b[39m_VAR_KEYWORD))\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;66;03m# Is 'func' is a pure Python function - don't validate the\u001b[39;00m\n\u001b[1;32m   2486\u001b[0m \u001b[38;5;66;03m# parameters list (for correct order and defaults), it should be OK.\u001b[39;00m\n\u001b[0;32m-> 2487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(parameters,\n\u001b[1;32m   2488\u001b[0m            return_annotation\u001b[38;5;241m=\u001b[39mannotations\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m'\u001b[39m, _empty),\n\u001b[1;32m   2489\u001b[0m            __validate_parameters__\u001b[38;5;241m=\u001b[39mis_duck_function)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/inspect.py:3076\u001b[0m, in \u001b[0;36mSignature.__init__\u001b[0;34m(self, parameters, return_annotation, __validate_parameters__)\u001b[0m\n\u001b[1;32m   3074\u001b[0m             params[name] \u001b[38;5;241m=\u001b[39m param\n\u001b[1;32m   3075\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3076\u001b[0m         params \u001b[38;5;241m=\u001b[39m OrderedDict((param\u001b[38;5;241m.\u001b[39mname, param) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m parameters)\n\u001b[1;32m   3078\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mMappingProxyType(params)\n\u001b[1;32m   3079\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_annotation \u001b[38;5;241m=\u001b[39m return_annotation\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def _trivial_loss(pred, label):\n",
    "    print(f'type of pred: {pred.dtype}, type of label: {label[0].dtype}')\n",
    "    return torch.Tensor([1]).to(device)\n",
    "\n",
    "def test_train():\n",
    "    dataloader = DataLoader(torch.utils.data.Subset(get_randman_dataset(), np.arange(600)),batch_size=256,shuffle=True)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    train(dataloader, loss_fn)\n",
    "\n",
    "    \n",
    "test_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
