{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trivial Evolution for Spiking Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Random Manifold Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import randman\n",
    "from randman import Randman\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for data generation\n",
    "NB_CLASSES = 10\n",
    "NB_UNITS = 3 # number of input neurons / embedding dimensions\n",
    "NB_STEPS = 200\n",
    "SEED = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_spiking_dataset() function from the paper\n",
    "\n",
    "def standardize(x,eps=1e-7):\n",
    "    # x's (which is actually y in the following code) shape will be [samples, units]\n",
    "    # Therefore, 0-axis shows that the author standardize across all samples for each units\n",
    "    mi,_ = x.min(0)\n",
    "    ma,_ = x.max(0)\n",
    "    return (x-mi)/(ma-mi+eps)\n",
    "\n",
    "def make_spiking_dataset(nb_classes=10, nb_units=100, nb_steps=100, step_frac=1.0, dim_manifold=2, nb_spikes=1, nb_samples=1000, alpha=2.0, shuffle=True, classification=True, seed=None):\n",
    "    \"\"\" Generates event-based generalized spiking randman classification/regression dataset. \n",
    "    In this dataset each unit fires a fixed number of spikes. So ratebased or spike count based decoding won't work. \n",
    "    All the information is stored in the relative timing between spikes.\n",
    "    For regression datasets the intrinsic manifold coordinates are returned for each target.\n",
    "    Args: \n",
    "        nb_classes: The number of classes to generate\n",
    "        nb_units: The number of units to assume\n",
    "        nb_steps: The number of time steps to assume\n",
    "        step_frac: Fraction of time steps from beginning of each to contain spikes (default 1.0)\n",
    "        nb_spikes: The number of spikes per unit\n",
    "        nb_samples: Number of samples from each manifold per class\n",
    "        alpha: Randman smoothness parameter\n",
    "        shuffe: Whether to shuffle the dataset\n",
    "        classification: Whether to generate a classification (default) or regression dataset\n",
    "        seed: The random seed (default: None)\n",
    "    Returns: \n",
    "        A tuple of data,labels. The data is structured as numpy array \n",
    "        (sample x event x 2 ) where the last dimension contains    \n",
    "        the relative [0,1] (time,unit) coordinates and labels.\n",
    "    \"\"\"\n",
    "  \n",
    "    data = []\n",
    "    labels = []\n",
    "    targets = []\n",
    "\n",
    "    if SEED is not None:\n",
    "        np.random.seed(SEED)\n",
    "    \n",
    "    max_value = np.iinfo(int).max\n",
    "    randman_seeds = np.random.randint(max_value, size=(nb_classes,nb_spikes) )\n",
    "\n",
    "    for k in range(nb_classes):\n",
    "        x = np.random.rand(nb_samples,dim_manifold)\n",
    "        \n",
    "        # The following code shows that if more than one spike, different spikes, even for the same unit, are generated by independent mappings \n",
    "        submans = [ randman.Randman(nb_units, dim_manifold, alpha=alpha, seed=randman_seeds[k,i]) for i in range(nb_spikes) ]\n",
    "        units = []\n",
    "        times = []\n",
    "        for i,rm in enumerate(submans):\n",
    "            y = rm.eval_manifold(x)\n",
    "            y = standardize(y)\n",
    "            units.append(np.repeat(np.arange(nb_units).reshape(1,-1),nb_samples,axis=0))\n",
    "            times.append(y.numpy())\n",
    "\n",
    "        units = np.concatenate(units,axis=1)\n",
    "        times = np.concatenate(times,axis=1)\n",
    "        events = np.stack([times,units],axis=2)\n",
    "        data.append(events)\n",
    "        labels.append(k*np.ones(len(units)))\n",
    "        targets.append(x)\n",
    "\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    labels = np.array(np.concatenate(labels, axis=0), dtype=int)\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "\n",
    "    if shuffle:\n",
    "        idx = np.arange(len(data))\n",
    "        np.random.shuffle(idx)\n",
    "        data = data[idx]\n",
    "        labels = labels[idx]\n",
    "        targets = targets[idx]\n",
    "\n",
    "    data[:,:,0] *= nb_steps*step_frac\n",
    "    # data = np.array(data, dtype=int)\n",
    "\n",
    "    if classification:\n",
    "        return data, labels\n",
    "    else:\n",
    "        return data, targets\n",
    "    \n",
    "def events_to_spike_train(data):\n",
    "    \"\"\"convert the data generated from manifold to spike train form\n",
    "\n",
    "    Args:\n",
    "        data (array): shape is [samples, nb_events, 2]\n",
    "\n",
    "    Returns:\n",
    "        spike_train: shape is [nb_samples, nb_time_steps, units]\n",
    "    \"\"\"\n",
    "    \n",
    "    # astyle() will discard the decimal to give integer timestep\n",
    "    spike_steps = data[:, :, 0].astype(int)\n",
    "    spike_units = data[:, :, 1].astype(int)\n",
    "    # These will be the indices to entrices in the spike train to be set to 1\n",
    "    \n",
    "    # Use the index on spike train matrix [samples, steps, units]\n",
    "    spike_train = np.zeros((data.shape[0], NB_STEPS, NB_UNITS))\n",
    "    sample_indicies = np.expand_dims(np.arange(data.shape[0]), -1)\n",
    "    spike_train[sample_indicies, spike_steps, spike_units] = 1\n",
    "    \n",
    "    return spike_train    \n",
    "\n",
    "def get_randman_dataset():\n",
    "    \"\"\"generate a TensorDataset encapsulated x and y, where x is spike trains\n",
    "\n",
    "    Returns:\n",
    "        TensorDataset: [nb_samples, time_steps, units] and [nb_samples]\n",
    "    \"\"\"\n",
    "    data, label = make_spiking_dataset(NB_CLASSES, NB_UNITS, NB_STEPS, nb_spikes=1)\n",
    "    spike_train = events_to_spike_train(data)\n",
    "    \n",
    "    spike_train = torch.Tensor(spike_train).to(device)\n",
    "    label = torch.Tensor(label).to(device)\n",
    "    \n",
    "    # encapulate using Torch.Dataset\n",
    "    dataset = TensorDataset(spike_train, label)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NB_HIDDEN_UNITS = int(NB_UNITS * 1.5)\n",
    "BETA = 0.85 # This can also be obtained using exp(-delta_t / tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: SnnTorch uses time-first dimensionality for the input $x$: [time, batch_size, feature_num]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandmanSNN(nn.Module):\n",
    "    '''\n",
    "    Spiking Neural Network with one hidden layer.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(NB_UNITS, NB_HIDDEN_UNITS)\n",
    "        self.lif1 = snn.Leaky(beta = BETA, reset_mechanism = 'subtract')\n",
    "        self.fc2 = nn.Linear(NB_HIDDEN_UNITS, NB_CLASSES)\n",
    "        self.lif2 = snn.Leaky(beta = BETA, reset_mechanism = 'subtract')\n",
    "        \n",
    "    def init_state(self):\n",
    "        # init recordings\n",
    "        self.mem1_rec = []\n",
    "        self.mem2_rec = []\n",
    "        self.spike1_rec = []\n",
    "        self.spike2_rec = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is given in [batch_size, time_steps, units], but SnnTorch uses [time, batch_size, nb_units] for x\n",
    "        # So reshape to whatSnn Torch wants by switching first two axis\n",
    "        x = x.transpose(0, 1)        \n",
    "        \n",
    "        # Initialize membrane potential to 0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        self.init_state()\n",
    "               \n",
    "        for step in range(NB_STEPS):\n",
    "            # Input spike trains are weighted by the synaptic weights to produce current to the first neuron\n",
    "            cur1 = self.fc1(x[step]) # can pull out from loop to do all at once? \n",
    "            \n",
    "            # lif1 accumulates the current and update its membrane potential, decide whether it spikes\n",
    "            spike1, mem1 = self.lif1(cur1, mem1)\n",
    "            \n",
    "            # Similar to cur1 and lif1 operations\n",
    "            cur2 = self.fc2(spike1)\n",
    "            spike2, mem2 = self.lif2(cur2)\n",
    "            \n",
    "            # Write to records\n",
    "            self.mem1_rec.append(mem1)\n",
    "            self.spike1_rec.append(spike1)\n",
    "            self.mem2_rec.append(mem2)\n",
    "            self.spike2_rec.append(spike2)\n",
    "            \n",
    "        # The output is of shape [time_steps, data_sample_size, classes]\n",
    "        return torch.stack(self.spike2_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spike_to_label(spike_train, scheme = 'most_spikes'):\n",
    "    \"\"\"Convert spike train to the label in one-hot encoded class\n",
    "\n",
    "    Args:\n",
    "        spike_train (tensor): spike train with shape [time_steps, batch_size, classes]\n",
    "        scheme(string): options: 'most_spikes' and 'first_spike'\n",
    "        \n",
    "    Return:\n",
    "        one label for each sample: [batch_size,]\n",
    "    \"\"\"\n",
    "    if scheme == 'most_spikes':\n",
    "        # count number of spikes along the time_steps dimension. Result is [batch_size, classes]\n",
    "        spike_counts = spike_train.count_nonzero(dim=0)\n",
    "        \n",
    "        # pick the index of along the clsses dimension\n",
    "        result = spike_counts.argmax(dim=-1)\n",
    "    else:\n",
    "        raise Exception('Undefined Scheme')\n",
    "    \n",
    "    return result\n",
    "\n",
    "def spike_count(spike_train):\n",
    "    # shape of return is [batch_size, classes]\n",
    "    return spike_train.count_nonzero(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Evolution Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def log_grad_wrt_mean(mean, std, v):\n",
    "    ''' calculate the gradient of the log of the loss function with respect to the mean'''\n",
    "    return (v - mean) / std**2\n",
    "\n",
    "class ESParameter:    \n",
    "    def __init__(self, para_means, para_std=1, Optimizer = optim.Adam):\n",
    "        self.means = para_means\n",
    "        self.means.grad = torch.zeros(self.means.shape).to(device)\n",
    "        self.STD = para_std\n",
    "        self.samples = None\n",
    "        self.optimizer = Optimizer([self.means], lr=0.01)\n",
    "        \n",
    "    def sample(self, sample_size):\n",
    "        \"\"\"draw samples for each parameter from normal distribution with self.means and self.STD.\n",
    "\n",
    "        Args:\n",
    "            sample_size (int): number of samples\n",
    "\n",
    "        Returns:\n",
    "            tensor: shape [sample_size, ...shape of parameters...]\n",
    "        \"\"\"\n",
    "        # insert sample_size dimension\n",
    "        sample_means = self.means.unsqueeze(0)\n",
    "        \n",
    "        # the shape is for repeat(), it is [sample_size, 1, ... ,1]\n",
    "        shape = [1] * len(sample_means.shape)\n",
    "        shape[0] = sample_size\n",
    "        \n",
    "        # duplicate means along the sample size dimension\n",
    "        sample_means = sample_means.repeat(shape)\n",
    "        \n",
    "        # draw samples\n",
    "        self.samples = torch.normal(mean=sample_means, std=self.STD)\n",
    "        \n",
    "        return self.samples\n",
    "    \n",
    "    def gradient_descent(self, loss):\n",
    "        \"\"\" Move the means of the parameters against gradient. The gradient is calculated based on loss.\n",
    "            And self.optimizer will be used to step.\n",
    "\n",
    "        Args:\n",
    "            loss (Tensor): with shape [nb_samples,]\n",
    "        \"\"\"\n",
    "        # shape of self.samples = [nb_samples, ...weight shape...]\n",
    "        \n",
    "        # Result is the gradients for each prameter, so the shape should match\n",
    "        # with the parameters, which is [...weight shape...].\n",
    "        \n",
    "        # Calculate the gradient for each sample weight, so log_grad will have [nb_samples, ...weight shape...]\n",
    "        log_grad = log_grad_wrt_mean(self.means, self.STD, self.samples)\n",
    "        \n",
    "        ## Calculate the sum of log_grad = [nb_samples, ...weight shape...] and loss = [nb_samples,]\n",
    "        # Reshape loss for broadcasting to [nb_samples, 1....1]\n",
    "        new_shape = [loss.shape[0]] + [1] * (len(log_grad.shape) - 1)\n",
    "        \n",
    "        # grad is now [nb_samples, ...weight shape...]\n",
    "        grad = log_grad * loss.reshape(new_shape)\n",
    "        \n",
    "        # Take average across sample dimension to estimate the expectation\n",
    "        self.means.grad = grad.mean(dim=0)\n",
    "        \n",
    "        # step the optimizer\n",
    "        self.optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, I should run all the data samples for each of the model sample. Later I can do batch instead of whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO on train function:\n",
    "- add vallidation loop after each train. How to validate though - use the mean to produce model maybe.\n",
    "- Training the fc1 layer first, then loop through all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, loss_fn, nb_model_samples=30):   \n",
    "    # Use a instance of model to initialize ESParameter\n",
    "    fc1_param = ESParameter(RandmanSNN().fc1.weight.clone().detach().to(device))\n",
    "    \n",
    "    # Prepare loss for each model sample\n",
    "    models_losses = []\n",
    "    \n",
    "    # Monte carlo: run each model sample\n",
    "    for fc1_param_sample in fc1_param.sample(nb_model_samples):           \n",
    "        batch_loss = 0\n",
    "        with torch.no_grad(): # ES doesn't need gradient \n",
    "            # Assign params to model\n",
    "            model = RandmanSNN().to(device)\n",
    "            model.fc1.weight.copy_(fc1_param_sample)\n",
    "            \n",
    "            # run the model for each batch \n",
    "            for x, y in dataloader:\n",
    "                out_spikes = model(x)\n",
    "                pred_spike_counts = spike_count(out_spikes)\n",
    "                batch_loss += loss_fn(pred_spike_counts, y)\n",
    "        \n",
    "        # The loss for a model is the average across batches\n",
    "        models_losses.append(batch_loss / len(dataloader))\n",
    "    \n",
    "    print(f'Average models loss: {models_losses.mean()}; ')                \n",
    "    \n",
    "    # update the model with grdient\n",
    "    fc1_param.gradient_descent(torch.stack(models_losses, dim = 0))\n",
    "    \n",
    "    \n",
    "    return fc1_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n",
      "Batch done\n"
     ]
    }
   ],
   "source": [
    "def _trivial_loss(pred, label):\n",
    "    return 1\n",
    "\n",
    "def test_train():\n",
    "    dataloader = DataLoader(get_randman_dataset(),batch_size=256,shuffle=True)\n",
    "    train(dataloader, _trivial_loss)\n",
    "\n",
    "    \n",
    "test_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
