{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLTL4ajt1ls24eo6Sj24A+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrashubhAtri/heterogeneousSNNs/blob/main/Baselines/SNNs/SurrogateGradients/SNNwithVoltage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxR1B4hnWgAt",
        "outputId": "7391c534-bc77-46be-cb08-16223baf3a08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snntorch\n",
            "  Downloading snntorch-0.9.4-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading snntorch-0.9.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snntorch\n",
            "Successfully installed snntorch-0.9.4\n"
          ]
        }
      ],
      "source": [
        "! pip install snntorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fzenke/randman"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COs_AU2bbVFP",
        "outputId": "41700be1-7e2f-4ba7-a4b1-1b97c457429e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'randman'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 104 (delta 16), reused 34 (delta 8), pack-reused 59 (from 1)\u001b[K\n",
            "Receiving objects: 100% (104/104), 683.31 KiB | 8.65 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/randman')"
      ],
      "metadata": {
        "id": "cQ7AjswvdNAV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import pandas as pd\n",
        "import randman\n",
        "from randman import Randman\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "NTn9_PTdWkUt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "UNLAszk4dYWW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants for data generation\n",
        "NB_CLASSES = 2\n",
        "NB_UNITS = 10 # number of input neurons / embedding dimensions\n",
        "NB_STEPS = 50\n",
        "NB_SAMPLES = 2000\n",
        "SEED = 12"
      ],
      "metadata": {
        "id": "hr_J65_vaDZl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(x,eps=1e-7):\n",
        "    # x's (which is actually y in the following code) shape will be [samples, units]\n",
        "    # Therefore, 0-axis shows that the author standardize across all samples for each units\n",
        "    mi,_ = x.min(0)\n",
        "    ma,_ = x.max(0)\n",
        "    return (x-mi)/(ma-mi+eps)"
      ],
      "metadata": {
        "id": "KG0Rc_djW3__"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_spiking_dataset(nb_classes=10, nb_units=100, nb_steps=100, step_frac=1.0, dim_manifold=2, nb_spikes=1, nb_samples=1000, alpha=2.0, shuffle=True, classification=True, seed=None):\n",
        "    \"\"\" Generates event-based generalized spiking randman classification/regression dataset.\n",
        "    In this dataset each unit fires a fixed number of spikes. So ratebased or spike count based decoding won't work.\n",
        "    All the information is stored in the relative timing between spikes.\n",
        "    For regression datasets the intrinsic manifold coordinates are returned for each target.\n",
        "    Args:\n",
        "        nb_classes: The number of classes to generate\n",
        "        nb_units: The number of units to assume\n",
        "        nb_steps: The number of time steps to assume\n",
        "        step_frac: Fraction of time steps from beginning of each to contain spikes (default 1.0)\n",
        "        nb_spikes: The number of spikes per unit\n",
        "        nb_samples: Number of samples from each manifold per class\n",
        "        alpha: Randman smoothness parameter\n",
        "        shuffe: Whether to shuffle the dataset\n",
        "        classification: Whether to generate a classification (default) or regression dataset\n",
        "        seed: The random seed (default: None)\n",
        "    Returns:\n",
        "        A tuple of data,labels. The data is structured as numpy array\n",
        "        (sample x event x 2 ) where the last dimension contains\n",
        "        the relative [0,1] (time,unit) coordinates and labels.\n",
        "    \"\"\"\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    targets = []\n",
        "\n",
        "    if SEED is not None:\n",
        "        np.random.seed(SEED)\n",
        "\n",
        "    max_value = np.iinfo(int).max\n",
        "    randman_seeds = np.random.randint(max_value, size=(nb_classes,nb_spikes) )\n",
        "\n",
        "    for k in range(nb_classes):\n",
        "        x = np.random.rand(nb_samples,dim_manifold)\n",
        "\n",
        "        # The following code shows that if more than one spike, different spikes, even for the same unit, are generated by independent mappings\n",
        "        submans = [ randman.Randman(nb_units, dim_manifold, alpha=alpha, seed=randman_seeds[k,i]) for i in range(nb_spikes) ]\n",
        "        units = []\n",
        "        times = []\n",
        "        for i,rm in enumerate(submans):\n",
        "            y = rm.eval_manifold(x)\n",
        "            y = standardize(y)\n",
        "            units.append(np.repeat(np.arange(nb_units).reshape(1,-1),nb_samples,axis=0))\n",
        "            times.append(y.numpy())\n",
        "\n",
        "        units = np.concatenate(units,axis=1)\n",
        "        times = np.concatenate(times,axis=1)\n",
        "        events = np.stack([times,units],axis=2)\n",
        "        data.append(events)\n",
        "        labels.append(k*np.ones(len(units)))\n",
        "        targets.append(x)\n",
        "\n",
        "    data = np.concatenate(data, axis=0)\n",
        "    labels = np.array(np.concatenate(labels, axis=0), dtype=int)\n",
        "    targets = np.concatenate(targets, axis=0)\n",
        "\n",
        "    if shuffle:\n",
        "        idx = np.arange(len(data))\n",
        "        np.random.shuffle(idx)\n",
        "        data = data[idx]\n",
        "        labels = labels[idx]\n",
        "        targets = targets[idx]\n",
        "\n",
        "    data[:,:,0] *= nb_steps*step_frac\n",
        "    # data = np.array(data, dtype=int)\n",
        "\n",
        "    if classification:\n",
        "        return data, labels\n",
        "    else:\n",
        "        return data, targets"
      ],
      "metadata": {
        "id": "fgM4VZv6W_dI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def events_to_spike_train(data):\n",
        "    \"\"\"convert the data generated from manifold to spike train form\n",
        "\n",
        "    Args:\n",
        "        data (array): shape is [samples, nb_events, 2]\n",
        "\n",
        "    Returns:\n",
        "        spike_train: shape is [nb_samples, nb_time_steps, units]\n",
        "    \"\"\"\n",
        "\n",
        "    # astyle() will discard the decimal to give integer timestep\n",
        "    spike_steps = data[:, :, 0].astype(int)\n",
        "    spike_units = data[:, :, 1].astype(int)\n",
        "    # These will be the indices to entrices in the spike train to be set to 1\n",
        "\n",
        "    # Use the index on spike train matrix [samples, steps, units]\n",
        "    spike_train = np.zeros((data.shape[0], NB_STEPS, NB_UNITS))\n",
        "    sample_indicies = np.expand_dims(np.arange(data.shape[0]), -1)\n",
        "    spike_train[sample_indicies, spike_steps, spike_units] = 1\n",
        "\n",
        "    return spike_train"
      ],
      "metadata": {
        "id": "1YXDm2hyXCpw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_randman_dataset():\n",
        "    \"\"\"generate a TensorDataset encapsulated x and y, where x is spike trains\n",
        "\n",
        "    Returns:\n",
        "        TensorDataset: [nb_samples, time_steps, units] and [nb_samples]\n",
        "    \"\"\"\n",
        "    data, label = make_spiking_dataset(NB_CLASSES, NB_UNITS, NB_STEPS, nb_spikes=1, nb_samples=NB_SAMPLES)\n",
        "    spike_train = events_to_spike_train(data)\n",
        "\n",
        "    spike_train = torch.Tensor(spike_train).to(device)\n",
        "    label = torch.Tensor(label).to(device)\n",
        "\n",
        "    # encapulate using Torch.Dataset\n",
        "    dataset = TensorDataset(spike_train, label)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "glHckWSHXG0X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "NB_HIDDEN_UNITS = int(NB_UNITS * 1.5)\n",
        "BETA = 0.85 # This can also be obtained using exp(-delta_t / tau)"
      ],
      "metadata": {
        "id": "LH96VsByXSpx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spike_trains = get_randman_dataset()"
      ],
      "metadata": {
        "id": "HE_jYj9-Z_rW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample, label = spike_trains[0]  # Access the first sample\n",
        "print(f\"Sample shape: {sample.shape}\")\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csaf0dxCbKbE",
        "outputId": "87dc1aaf-2de0-4bb0-dd62-a82119147691"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample shape: torch.Size([50, 10])\n",
            "Label: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into ttv sets\n",
        "train, test, validation = 0.8, 0.1, 0.1\n",
        "\n",
        "all_labels = [spike_trains[i][1] for i in range(len(spike_trains))]\n",
        "\n",
        "# First split: train (80%) and temp (20%)\n",
        "train_idx, temp_idx = train_test_split(\n",
        "    np.arange(len(spike_trains)),\n",
        "    test_size=test,\n",
        "    stratify=all_labels,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# Second split: val (10%) and test (10%) from temp\n",
        "val_idx, test_idx = train_test_split(\n",
        "    temp_idx,\n",
        "    test_size=0.5,\n",
        "    stratify=[all_labels[i] for i in temp_idx],\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "train_dataset = Subset(spike_trains, train_idx)\n",
        "val_dataset = Subset(spike_trains, val_idx)\n",
        "test_dataset = Subset(spike_trains, test_idx)\n",
        "\n",
        "#Batch_size = 32\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "train_labels = [spike_trains[i][1] for i in train_dataset.indices]\n",
        "test_labels = [spike_trains[i][1] for i in train_dataset.indices]\n",
        "train_labels = [spike_trains[i][1] for i in train_dataset.indices]"
      ],
      "metadata": {
        "id": "yCDWVxp0eHdI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Iterate through the training DataLoader\n",
        "for batch_data, batch_labels in train_loader:\n",
        "    print(f\"Batch data shape: {batch_data.shape}\")\n",
        "    print(f\"Batch labels: {batch_labels}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPxAh_lahBYS",
        "outputId": "a3e5a92d-33f3-4d30-9809-4113e3618a57"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch data shape: torch.Size([32, 50, 10])\n",
            "Batch labels: tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SNN(nn.Module):\n",
        "    def __init__(self, num_inputs=NB_STEPS, num_hidden=100, num_outputs=10, beta=0.85):\n",
        "        super(SNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
        "        self.lif1 = snn.Leaky(beta=beta, learn_beta=True, threshold=0.5,\n",
        "                              reset_mechanism=\"subtract\",\n",
        "                              spike_grad=surrogate.fast_sigmoid(slope=5))\n",
        "\n",
        "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
        "        self.lif2 = snn.Leaky(beta=beta, learn_beta=True, threshold=0.5,\n",
        "                              reset_mechanism=\"none\",\n",
        "                              spike_grad=surrogate.fast_sigmoid(slope=5))\n",
        "\n",
        "        # Xavier Uniform Initialization for Stability\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_neurons, time_steps = x.shape\n",
        "        x = x.permute(2, 0, 1)  # (time, batch, neurons)\n",
        "\n",
        "        mem1, mem2 = [torch.zeros(batch_size, layer.out_features, device=x.device)\n",
        "                             for layer in [self.fc1, self.fc2]]\n",
        "        # Store output membrane potentials over time\n",
        "        mem2_rec = []\n",
        "\n",
        "        for t in range(time_steps):\n",
        "            spk1, mem1 = self.lif1(self.fc1(x[t]), mem1)\n",
        "            spk2, mem2 = self.lif2(self.fc2(spk1), mem2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        # Aggregate membrane potentials over time (sum)\n",
        "        # mem2_rec = torch.stack(mem2_rec, dim=0).sum(dim=0)\n",
        "        return mem2_rec[-1]"
      ],
      "metadata": {
        "id": "Dk9YQ8dddiON"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate Model with voltage aggregation\n",
        "model = SNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
        "\n",
        "# Training Loop with Validation\n",
        "num_epochs = 10\n",
        "best_val_accuracy = 0.0\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        labels = labels.long()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            labels = labels.long()\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = correct / total\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save best model\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy*100:.2f}%\")\n",
        "\n",
        "# Load best model for testing\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Testing Loop\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        labels = labels.long()\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "test_accuracy = correct / total\n",
        "\n",
        "print(f\"\\nFinal Test Results:\")\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vhwI18vhX4V",
        "outputId": "e9a04974-39db-4671-dbd7-c67ae7358c61"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10]\n",
            "  Train Loss: 0.9647 | Val Loss: 0.5637 | Val Acc: 66.00%\n",
            "Epoch [2/10]\n",
            "  Train Loss: 0.5463 | Val Loss: 0.5487 | Val Acc: 66.50%\n",
            "Epoch [3/10]\n",
            "  Train Loss: 0.5218 | Val Loss: 0.5322 | Val Acc: 69.50%\n",
            "Epoch [4/10]\n",
            "  Train Loss: 0.5120 | Val Loss: 0.5584 | Val Acc: 69.00%\n",
            "Epoch [5/10]\n",
            "  Train Loss: 0.4970 | Val Loss: 0.5810 | Val Acc: 65.00%\n",
            "Epoch [6/10]\n",
            "  Train Loss: 0.4866 | Val Loss: 0.5241 | Val Acc: 71.00%\n",
            "Epoch [7/10]\n",
            "  Train Loss: 0.4710 | Val Loss: 0.5327 | Val Acc: 71.00%\n",
            "Epoch [8/10]\n",
            "  Train Loss: 0.4556 | Val Loss: 0.4939 | Val Acc: 71.50%\n",
            "Epoch [9/10]\n",
            "  Train Loss: 0.4343 | Val Loss: 0.4956 | Val Acc: 73.00%\n",
            "Epoch [10/10]\n",
            "  Train Loss: 0.4172 | Val Loss: 0.4760 | Val Acc: 72.50%\n",
            "\n",
            "Final Test Results:\n",
            "Test Loss: 0.4245 | Test Accuracy: 77.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Z2VEuEboxLa"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}