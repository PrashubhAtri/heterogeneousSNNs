{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttn7nN3Fnhhp",
        "outputId": "01139e51-2846-45cf-ca2c-5140d6ec3b9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tonic in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: snntorch in /usr/local/lib/python3.11/dist-packages (0.9.4)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from tonic) (1.26.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from tonic) (3.13.0)\n",
            "Requirement already satisfied: importRosbag>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from tonic) (1.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from tonic) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tonic) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from tonic) (4.13.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from tonic) (0.11.0)\n",
            "Requirement already satisfied: pbr in /usr/local/lib/python3.11/dist-packages (from tonic) (6.1.1)\n",
            "Requirement already satisfied: expelliarmus in /usr/local/lib/python3.11/dist-packages (from tonic) (1.1.12)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.26.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->tonic) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->tonic) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->tonic) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->tonic) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->tonic) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->tonic) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->tonic) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->tonic) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->tonic) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->tonic) (1.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa->tonic) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->tonic) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->tonic) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->tonic) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->tonic) (2.22)\n",
            "Requirement already satisfied: weave in /usr/local/lib/python3.11/dist-packages (0.51.44)\n",
            "Requirement already satisfied: diskcache==5.6.3 in /usr/local/lib/python3.11/dist-packages (from weave) (5.6.3)\n",
            "Requirement already satisfied: emoji>=2.12.1 in /usr/local/lib/python3.11/dist-packages (from weave) (2.14.1)\n",
            "Requirement already satisfied: gql[aiohttp,requests] in /usr/local/lib/python3.11/dist-packages (from weave) (3.5.2)\n",
            "Requirement already satisfied: jsonschema>=4.23.0 in /usr/local/lib/python3.11/dist-packages (from weave) (4.23.0)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.11/dist-packages (from weave) (1.6.0)\n",
            "Requirement already satisfied: numpy>1.21.0 in /usr/local/lib/python3.11/dist-packages (from weave) (1.26.4)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.11/dist-packages (from weave) (24.2)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from weave) (2.11.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from weave) (13.9.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,>=8.3.0 in /usr/local/lib/python3.11/dist-packages (from weave) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from weave) (0.10.0)\n",
            "Requirement already satisfied: wandb>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from weave) (0.19.9)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.23.0->weave) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.23.0->weave) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.23.0->weave) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.23.0->weave) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->weave) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->weave) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->weave) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->weave) (0.4.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (2.26.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.1->weave) (75.2.0)\n",
            "Requirement already satisfied: graphql-core<3.2.5,>=3.2 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (3.2.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (1.20.0)\n",
            "Requirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (2.2.1)\n",
            "Requirement already satisfied: anyio<5,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (4.9.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (3.11.15)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[aiohttp,requests]->weave) (1.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->weave) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->weave) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.0->gql[aiohttp,requests]->weave) (0.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.0->gql[aiohttp,requests]->weave) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.0->gql[aiohttp,requests]->weave) (1.3.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.17.1->weave) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave) (4.0.12)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->weave) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave) (5.0.2)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!pip install tonic wandb snntorch\n",
        "!pip install weave\n",
        "!wandb login '624747d405596915090d3160f109335907281de4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KXNqY-Z6z7kr"
      },
      "outputs": [],
      "source": [
        "import randman\n",
        "from randman import Randman\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "def standardize(x,eps=1e-7):\n",
        "    # x's (which is actually y in the following code) shape will be [samples, units]\n",
        "    # Therefore, 0-axis shows that the author standardize across all samples for each units\n",
        "    mi,_ = x.min(0)\n",
        "    ma,_ = x.max(0)\n",
        "    return (x-mi)/(ma-mi+eps)\n",
        "\n",
        "def make_spiking_dataset(nb_classes=10, nb_units=100, nb_steps=100, step_frac=1.0, dim_manifold=2, nb_spikes=1, nb_samples=1000, alpha=2.0, shuffle=True, classification=True, seed=None):\n",
        "    \"\"\" Generates event-based generalized spiking randman classification/regression dataset.\n",
        "    In this dataset each unit fires a fixed number of spikes. So ratebased or spike count based decoding won't work.\n",
        "    All the information is stored in the relative timing between spikes.\n",
        "    For regression datasets the intrinsic manifold coordinates are returned for each target.\n",
        "    Args:\n",
        "        nb_classes: The number of classes to generate\n",
        "        nb_units: The number of units to assume\n",
        "        nb_steps: The number of time steps to assume\n",
        "        step_frac: Fraction of time steps from beginning of each to contain spikes (default 1.0)\n",
        "        nb_spikes: The number of spikes per unit\n",
        "        nb_samples: Number of samples from each manifold per class\n",
        "        alpha: Randman smoothness parameter\n",
        "        shuffe: Whether to shuffle the dataset\n",
        "        classification: Whether to generate a classification (default) or regression dataset\n",
        "        seed: The random seed (default: None)\n",
        "    Returns:\n",
        "        A tuple of data,labels. The data is structured as numpy array\n",
        "        (sample x event x 2 ) where the last dimension contains\n",
        "        the relative [0,1] (time,unit) coordinates and labels.\n",
        "    \"\"\"\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    targets = []\n",
        "\n",
        "    if SEED is not None:\n",
        "        np.random.seed(SEED)\n",
        "\n",
        "    max_value = np.iinfo(int).max\n",
        "    randman_seeds = np.random.randint(max_value, size=(nb_classes,nb_spikes) )\n",
        "\n",
        "    for k in range(nb_classes):\n",
        "        x = np.random.rand(nb_samples,dim_manifold)\n",
        "\n",
        "        # The following code shows that if more than one spike, different spikes, even for the same unit, are generated by independent mappings\n",
        "        submans = [ randman.Randman(nb_units, dim_manifold, alpha=alpha, seed=randman_seeds[k,i]) for i in range(nb_spikes) ]\n",
        "        units = []\n",
        "        times = []\n",
        "        for i,rm in enumerate(submans):\n",
        "            y = rm.eval_manifold(x)\n",
        "            y = standardize(y)\n",
        "            units.append(np.repeat(np.arange(nb_units).reshape(1,-1),nb_samples,axis=0))\n",
        "            times.append(y.numpy())\n",
        "\n",
        "        units = np.concatenate(units,axis=1)\n",
        "        times = np.concatenate(times,axis=1)\n",
        "        events = np.stack([times,units],axis=2)\n",
        "        data.append(events)\n",
        "        labels.append(k*np.ones(len(units)))\n",
        "        targets.append(x)\n",
        "\n",
        "    data = np.concatenate(data, axis=0)\n",
        "    labels = np.array(np.concatenate(labels, axis=0), dtype=int)\n",
        "    targets = np.concatenate(targets, axis=0)\n",
        "\n",
        "    if shuffle:\n",
        "        idx = np.arange(len(data))\n",
        "        np.random.shuffle(idx)\n",
        "        data = data[idx]\n",
        "        labels = labels[idx]\n",
        "        targets = targets[idx]\n",
        "\n",
        "    data[:,:,0] *= nb_steps*step_frac\n",
        "    # data = np.array(data, dtype=int)\n",
        "\n",
        "    if classification:\n",
        "        return data, labels\n",
        "    else:\n",
        "        return data, targets\n",
        "\n",
        "def events_to_spike_train(data, nb_steps, nb_units):\n",
        "    \"\"\"convert the data generated from manifold to spike train form\n",
        "\n",
        "    Args:\n",
        "        data (array): shape is [samples, nb_events, 2]\n",
        "\n",
        "    Returns:\n",
        "        spike_train: shape is [nb_samples, nb_time_steps, units]\n",
        "    \"\"\"\n",
        "\n",
        "    # astyle() will discard the decimal to give integer timestep\n",
        "    spike_steps = data[:, :, 0].astype(int)\n",
        "    spike_units = data[:, :, 1].astype(int)\n",
        "    # These will be the indices to entrices in the spike train to be set to 1\n",
        "\n",
        "    # Use the index on spike train matrix [samples, steps, units]\n",
        "    spike_train = np.zeros((data.shape[0], nb_steps, nb_units))\n",
        "    sample_indicies = np.expand_dims(np.arange(data.shape[0]), -1)\n",
        "    spike_train[sample_indicies, spike_steps, spike_units] = 1\n",
        "\n",
        "    return spike_train\n",
        "\n",
        "def get_randman_dataset(nb_classes = 2, nb_units = 10, nb_steps = 50, nb_samples = 1000):\n",
        "    \"\"\"generate a TensorDataset encapsulated x and y, where x is spike trains\n",
        "\n",
        "    Returns:\n",
        "        TensorDataset: [nb_samples, time_steps, units] and [nb_samples]\n",
        "    \"\"\"\n",
        "    data, label = make_spiking_dataset(nb_classes, nb_units, nb_steps, nb_spikes=1, nb_samples = nb_samples)\n",
        "    spike_train = events_to_spike_train(data, nb_steps, nb_units)\n",
        "\n",
        "    spike_train = torch.Tensor(spike_train)\n",
        "    label = torch.Tensor(label)\n",
        "\n",
        "    # encapulate using Torch.Dataset\n",
        "    dataset = TensorDataset(spike_train, label)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AsZEDSF4ivOM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import snntorch as snn\n",
        "import wandb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---- SNN Architecture with Sparse Connectivity ----\n",
        "class SNN(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs, learn_beta=False, beta=0.95, sparsity=0.8):\n",
        "        super(SNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden, bias=False)\n",
        "        self.fc2 = nn.Linear(num_hidden, num_outputs, bias=False)\n",
        "        self.lif1 = snn.Leaky(beta=beta, learn_beta=learn_beta)\n",
        "        self.lif2 = snn.Leaky(beta=beta, learn_beta=learn_beta, reset_mechanism='none')\n",
        "\n",
        "        # Apply sparsity masks to fc1 and fc2\n",
        "        with torch.no_grad():\n",
        "            mask1 = torch.rand_like(self.fc1.weight) > sparsity\n",
        "            mask2 = torch.rand_like(self.fc2.weight) > sparsity\n",
        "            self.fc1.weight.data *= mask1\n",
        "            self.fc2.weight.data *= mask2\n",
        "            self.register_buffer(\"mask1\", mask1)\n",
        "            self.register_buffer(\"mask2\", mask2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem2_rec = []\n",
        "        for t in range(x.size(1)):\n",
        "            cur1 = self.fc1(x[:, t])\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            _, mem2 = self.lif2(cur2, mem2)\n",
        "            mem2_rec.append(mem2)\n",
        "        return torch.stack(mem2_rec, dim=1)  # (batch, time, outputs)\n",
        "\n",
        "# ---- Hybrid Parameter Update (PSO-inspired + adaptive Pool with Offspring) ----\n",
        "def hybrid_update(mean, velocity, personal_best, global_best, loss_fn, std, samples, x, y, hidden_dim, num_classes, lr=0.1, acc_threshold=0.95):\n",
        "    device = mean.device\n",
        "    sample_batch = mean + std * torch.randn(samples, *mean.shape).to(device)\n",
        "    losses = []\n",
        "    accs = []\n",
        "\n",
        "    # Evaluate each sampled particle\n",
        "    for i in range(samples):\n",
        "        model = SNN(x.shape[2], hidden_dim, num_classes).to(device)\n",
        "        with torch.no_grad():\n",
        "            flat_params = sample_batch[i]\n",
        "            offset = 0\n",
        "            for p in model.parameters():\n",
        "                numel = p.numel()\n",
        "                p.data.copy_(flat_params[offset:offset+numel].view_as(p))\n",
        "                offset += numel\n",
        "            output = model(x)\n",
        "            loss = loss_fn(output.mean(1), y)\n",
        "            pred = output.mean(1).argmax(1)\n",
        "            acc = (pred == y).float().mean().item()\n",
        "            losses.append(loss.item())\n",
        "            accs.append(acc)\n",
        "\n",
        "    losses = torch.tensor(losses, device=device)\n",
        "    accs = torch.tensor(accs, device=device)\n",
        "\n",
        "    # Update global (group) best\n",
        "    best_idx = torch.argmin(losses)\n",
        "    model = SNN(x.shape[2], hidden_dim, num_classes).to(device)\n",
        "    with torch.no_grad():\n",
        "        offset = 0\n",
        "        for p in model.parameters():\n",
        "            numel = p.numel()\n",
        "            p.data.copy_(mean[offset:offset+numel].view_as(p))\n",
        "            offset += numel\n",
        "        output = model(x)\n",
        "        curr_loss = loss_fn(output.mean(1), y)\n",
        "    if losses[best_idx] < curr_loss:\n",
        "        global_best = sample_batch[best_idx].clone()\n",
        "\n",
        "    # PSO update\n",
        "    r1, r2 = torch.rand(2)\n",
        "    velocity = 0.5 * velocity + 1.5 * r1 * (personal_best - mean) + 1.5 * r2 * (global_best - mean)\n",
        "    mean = mean + lr * velocity\n",
        "\n",
        "    # Evaluate new mean\n",
        "    model = SNN(x.shape[2], hidden_dim, num_classes).to(device)\n",
        "    with torch.no_grad():\n",
        "        offset = 0\n",
        "        for p in model.parameters():\n",
        "            numel = p.numel()\n",
        "            p.data.copy_(mean[offset:offset+numel].view_as(p))\n",
        "            offset += numel\n",
        "        output_now = model(x)\n",
        "        loss_now = loss_fn(output_now.mean(1), y)\n",
        "        acc_now = (output_now.mean(1).argmax(1) == y).float().mean().item()\n",
        "\n",
        "    # Maintain personal best\n",
        "    model = SNN(x.shape[2], hidden_dim, num_classes).to(device)\n",
        "    with torch.no_grad():\n",
        "        offset = 0\n",
        "        for p in model.parameters():\n",
        "            numel = p.numel()\n",
        "            p.data.copy_(personal_best[offset:offset+numel].view_as(p))\n",
        "            offset += numel\n",
        "        output_pbest = model(x)\n",
        "        loss_pbest = loss_fn(output_pbest.mean(1), y)\n",
        "    if loss_now < loss_pbest:\n",
        "        personal_best = mean.clone()\n",
        "\n",
        "    # Adaptive Pooling with Offspring\n",
        "    if accs[best_idx] < acc_threshold:\n",
        "        print(f\"Adaptive Pooling with Offspring | \")\n",
        "        topk = sample_batch[torch.argsort(losses)[:samples//4]]\n",
        "        offspring = topk + std * torch.randn_like(topk)\n",
        "\n",
        "        offspring_losses = []\n",
        "        for i in range(offspring.size(0)):\n",
        "            model = SNN(x.shape[2], hidden_dim, num_classes).to(device)\n",
        "            with torch.no_grad():\n",
        "                flat_params = offspring[i]\n",
        "                offset = 0\n",
        "                for p in model.parameters():\n",
        "                    numel = p.numel()\n",
        "                    p.data.copy_(flat_params[offset:offset+numel].view_as(p))\n",
        "                    offset += numel\n",
        "                output = model(x)\n",
        "                loss = loss_fn(output.mean(1), y)\n",
        "                offspring_losses.append(loss.item())\n",
        "\n",
        "        offspring_losses = torch.tensor(offspring_losses, device=device)\n",
        "        best_offspring_idx = offspring_losses.argsort()[:max(1, topk.size(0)//2)]\n",
        "        mean = offspring[best_offspring_idx].mean(0).detach()\n",
        "\n",
        "        # After pooling, re-evaluate mean and personal best\n",
        "        model = SNN(x.shape[2], hidden_dim, num_classes).to(device)\n",
        "        with torch.no_grad():\n",
        "            offset = 0\n",
        "            for p in model.parameters():\n",
        "                numel = p.numel()\n",
        "                p.data.copy_(mean[offset:offset+numel].view_as(p))\n",
        "                offset += numel\n",
        "            output_now = model(x)\n",
        "            loss_now = loss_fn(output_now.mean(1), y)\n",
        "        if loss_now < loss_pbest:\n",
        "            personal_best = mean.clone()\n",
        "\n",
        "    # Log batch best performance\n",
        "    print(f\"    Batch Accuracy: {acc_now * 100:.2f}%\")\n",
        "    wandb.log({\"train_acc\": acc_now})\n",
        "\n",
        "    return mean, velocity, personal_best, global_best\n",
        "\n",
        "# ---- Annealing Schedulers ----\n",
        "def get_annealed_param(init, final, current_epoch, total_epochs, mode='exp'):\n",
        "    if mode == 'linear':\n",
        "        return final + (init - final) * (1 - current_epoch / total_epochs)\n",
        "    elif mode == 'exp':\n",
        "        return final + (init - final) * (0.995 ** current_epoch)\n",
        "    else:\n",
        "        return init\n",
        "\n",
        "# # ---- Train Function ----\n",
        "# def train_snn():\n",
        "#     run_name = 'EA_10_classes_randman_offspring'\n",
        "#     config = {\n",
        "#         'nb_input': 100, 'nb_output': 10, 'nb_steps': 50, 'nb_data_samples': 1000,\n",
        "#         'nb_hidden': 20, 'learn_beta': False, 'nb_model_samples': 100,\n",
        "#         'std': 0.05, 'epochs': 20, 'batch_size': 256,\n",
        "#         'loss': 'cross-entropy', 'optimizer': 'Adam', 'lr': 0.01, 'regularization': 'none'\n",
        "#     }\n",
        "\n",
        "#     wandb.init(entity='DarwinNeuron', project='EA-Randman', name=run_name, config=config)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         dataset = get_randman_dataset(config['nb_output'], config['nb_input'], config['nb_steps'], config['nb_data_samples'])\n",
        "#         train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, shuffle=False)\n",
        "#         train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "#         val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "#         sample_model = SNN(config['nb_input'], config['nb_hidden'], config['nb_output'])\n",
        "#         param_vector = torch.cat([p.flatten() for p in sample_model.parameters()]).detach()\n",
        "#         mean = param_vector.clone()\n",
        "#         velocity = torch.zeros_like(mean)\n",
        "#         personal_best = mean.clone()\n",
        "#         global_best = mean.clone()\n",
        "\n",
        "#         for epoch in range(config['epochs']):\n",
        "#             print(f\"Epoch {epoch}\")\n",
        "#             current_std = get_annealed_param(init=0.1, final=0.01, current_epoch=epoch, total_epochs=config['epochs'])\n",
        "#             current_samples = int(get_annealed_param(init=1000, final=100, current_epoch=epoch, total_epochs=config['epochs']))\n",
        "#             acc_thresh = get_annealed_param(init=0.90, final=0.98, current_epoch=epoch, total_epochs=config['epochs'])\n",
        "\n",
        "#             for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "#                 x_batch, y_batch = x_batch.float(), y_batch.long()\n",
        "#                 mean, velocity, personal_best, global_best = hybrid_update(\n",
        "#                     mean, velocity, personal_best, global_best,\n",
        "#                     nn.CrossEntropyLoss(), current_std, current_samples,\n",
        "#                     x_batch, y_batch, 20, 10, lr=config['lr'], acc_threshold=acc_thresh\n",
        "#                 )\n",
        "\n",
        "#             # ---- Evaluate validation set ----\n",
        "#             val_accs = []\n",
        "#             with torch.no_grad():\n",
        "#                 model = SNN(config['nb_input'], config['nb_hidden'], config['nb_output']).to(mean.device)\n",
        "#                 offset = 0\n",
        "#                 for p in model.parameters():\n",
        "#                     numel = p.numel()\n",
        "#                     p.data.copy_(mean[offset:offset+numel].view_as(p))\n",
        "#                     offset += numel\n",
        "\n",
        "#                 for x_val, y_val in val_loader:\n",
        "#                     x_val, y_val = x_val.float().to(mean.device), y_val.long().to(mean.device)\n",
        "#                     output = model(x_val)\n",
        "#                     pred = output.mean(1).argmax(1)\n",
        "#                     acc = (pred == y_val).float().mean().item()\n",
        "#                     val_accs.append(acc)\n",
        "\n",
        "#                 val_accuracy = sum(val_accs) / len(val_accs)\n",
        "#                 print(f\"Validation Accuracy: {val_accuracy:.4f} | std: {current_std:.4f} | samples: {current_samples} | acc_thresh: {acc_thresh:.4f}\")\n",
        "#                 wandb.log({\n",
        "#                     \"val_accuracy\": val_accuracy,\n",
        "#                     \"epoch\": epoch,\n",
        "#                     \"std\": current_std,\n",
        "#                     \"samples\": current_samples,\n",
        "#                     \"acc_threshold\": acc_thresh\n",
        "#                 })\n",
        "\n",
        "#             torch.cuda.empty_cache()\n",
        "\n",
        "# train_snn()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "auodhytGkwCx",
        "outputId": "a457a1bb-5e3c-4416-db53-e62a95ab50e8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.08594</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">EA_10_classes_randman_offspring_save</strong> at: <a href='https://wandb.ai/DarwinNeuron/EA-Randman/runs/62mhil8p' target=\"_blank\">https://wandb.ai/DarwinNeuron/EA-Randman/runs/62mhil8p</a><br> View project at: <a href='https://wandb.ai/DarwinNeuron/EA-Randman' target=\"_blank\">https://wandb.ai/DarwinNeuron/EA-Randman</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250426_010321-62mhil8p/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250426_010748-zjggw83t</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/DarwinNeuron/EA-Randman/runs/zjggw83t' target=\"_blank\">EA_10_classes_randman_offspring_save</a></strong> to <a href='https://wandb.ai/DarwinNeuron/EA-Randman' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/DarwinNeuron/EA-Randman' target=\"_blank\">https://wandb.ai/DarwinNeuron/EA-Randman</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/DarwinNeuron/EA-Randman/runs/zjggw83t' target=\"_blank\">https://wandb.ai/DarwinNeuron/EA-Randman/runs/zjggw83t</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Adaptive Pooling with Offspring | \n",
            "    Batch Accuracy: 8.59%\n",
            "Adaptive Pooling with Offspring | \n",
            "    Batch Accuracy: 11.72%\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Train Function ----\n",
        "def train_snn():\n",
        "    run_name = 'EA_2_classes_randman_offspring_save'\n",
        "    config = {\n",
        "        'nb_input': 100, 'nb_output': 2, 'nb_steps': 50, 'nb_data_samples': 1000,\n",
        "        'nb_hidden': 10, 'learn_beta': False, 'nb_model_samples': 100,\n",
        "        'std': 0.05, 'epochs': 20, 'batch_size': 256,\n",
        "        'loss': 'cross-entropy', 'optimizer': 'Adam', 'lr': 0.01, 'regularization': 'none'\n",
        "    }\n",
        "\n",
        "    wandb.init(entity='DarwinNeuron', project='EA-Randman', name=run_name, config=config)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        dataset = get_randman_dataset(config['nb_output'], config['nb_input'], config['nb_steps'], config['nb_data_samples'])\n",
        "        train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, shuffle=False)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        sample_model = SNN(config['nb_input'], config['nb_hidden'], config['nb_output'])\n",
        "        param_vector = torch.cat([p.flatten() for p in sample_model.parameters()]).detach()\n",
        "        mean = param_vector.clone().to(device)\n",
        "        velocity = torch.zeros_like(mean)\n",
        "        personal_best = mean.clone()\n",
        "        global_best = mean.clone()\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(config['epochs']):\n",
        "            print(f\"Epoch {epoch}\")\n",
        "            current_std = get_annealed_param(init=0.1, final=0.01, current_epoch=epoch, total_epochs=config['epochs'])\n",
        "            current_samples = int(get_annealed_param(init=1000, final=100, current_epoch=epoch, total_epochs=config['epochs']))\n",
        "            acc_thresh = get_annealed_param(init=0.90, final=0.98, current_epoch=epoch, total_epochs=config['epochs'])\n",
        "\n",
        "            neuron_firing = torch.zeros(config['nb_hidden'], device=device)\n",
        "            running_loss = 0.0\n",
        "            total_batches = 0\n",
        "\n",
        "            for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "                x_batch, y_batch = x_batch.float().to(device), y_batch.long().to(device)\n",
        "                mean, velocity, personal_best, global_best = hybrid_update(\n",
        "                    mean, velocity, personal_best, global_best,\n",
        "                    nn.CrossEntropyLoss(), current_std, current_samples,\n",
        "                    x_batch, y_batch, config['nb_hidden'], config['nb_output'],\n",
        "                    lr=config['lr'], acc_threshold=acc_thresh\n",
        "                )\n",
        "\n",
        "                # After update, load mean into model and accumulate training loss\n",
        "                model = SNN(config['nb_input'], config['nb_hidden'], config['nb_output']).to(device)\n",
        "                offset = 0\n",
        "                for p in model.parameters():\n",
        "                    numel = p.numel()\n",
        "                    p.data.copy_(mean[offset:offset+numel].view_as(p))\n",
        "                    offset += numel\n",
        "\n",
        "                out = model(x_batch)\n",
        "                loss = nn.CrossEntropyLoss()(out.mean(1), y_batch)\n",
        "                running_loss += loss.item()\n",
        "                total_batches += 1\n",
        "\n",
        "                # Count spikes during training\n",
        "                mem1 = model.lif1.init_leaky().to(device)\n",
        "                for t in range(x_batch.size(1)):\n",
        "                    cur1 = model.fc1(x_batch[:, t])\n",
        "                    spk1, mem1 = model.lif1(cur1, mem1)\n",
        "                    neuron_firing += spk1.sum(0)\n",
        "\n",
        "            avg_train_loss = running_loss / total_batches\n",
        "            train_losses.append(avg_train_loss)\n",
        "\n",
        "            # Save model checkpoint\n",
        "            checkpoint_path = f\"checkpoint_epoch_{epoch}.pth\"\n",
        "            torch.save(mean.cpu(), checkpoint_path)\n",
        "            artifact = wandb.Artifact(f'checkpoint-epoch-{epoch}', type='model')\n",
        "            artifact.add_file(checkpoint_path)\n",
        "            wandb.log_artifact(artifact)\n",
        "\n",
        "            # ---- Evaluate validation set and firing counts ----\n",
        "            val_accs = []\n",
        "            val_losses = []\n",
        "            hidden_firing_counts = torch.zeros(config['nb_hidden'], device=mean.device)\n",
        "            with torch.no_grad():\n",
        "                model = SNN(config['nb_input'], config['nb_hidden'], config['nb_output']).to(mean.device)\n",
        "                offset = 0\n",
        "                for p in model.parameters():\n",
        "                    numel = p.numel()\n",
        "                    p.data.copy_(mean[offset:offset+numel].view_as(p))\n",
        "                    offset += numel\n",
        "\n",
        "                for x_val, y_val in val_loader:\n",
        "                    x_val, y_val = x_val.float().to(mean.device), y_val.long().to(mean.device)\n",
        "                    mem1 = model.lif1.init_leaky().to(mean.device)\n",
        "                    mem2 = model.lif2.init_leaky().to(mean.device)\n",
        "                    for t in range(x_val.size(1)):\n",
        "                        cur1 = model.fc1(x_val[:, t])\n",
        "                        spk1, mem1 = model.lif1(cur1, mem1)\n",
        "                        cur2 = model.fc2(spk1)\n",
        "                        _, mem2 = model.lif2(cur2, mem2)\n",
        "                        hidden_firing_counts += spk1.sum(0)\n",
        "\n",
        "                    output = model(x_val)\n",
        "                    pred = output.mean(1).argmax(1)\n",
        "                    acc = (pred == y_val).float().mean().item()\n",
        "                    loss = nn.CrossEntropyLoss()(output.mean(1), y_val)\n",
        "                    val_losses.append(loss.item())\n",
        "                    val_accs.append(acc)\n",
        "\n",
        "                val_accuracy = sum(val_accs) / len(val_accs)\n",
        "                val_loss = sum(val_losses) / len(val_losses)\n",
        "                hidden_firing_counts = hidden_firing_counts.cpu().numpy()\n",
        "\n",
        "                # Log heatmap\n",
        "                import matplotlib.pyplot as plt\n",
        "                import seaborn as sns\n",
        "                fig, ax = plt.subplots(figsize=(10, 3))\n",
        "                sns.heatmap(hidden_firing_counts[np.newaxis, :], cmap='viridis', cbar=True, xticklabels=False, yticklabels=False)\n",
        "                ax.set_title(f\"Hidden Neuron Firing Counts (Epoch {epoch})\")\n",
        "                wandb.log({\"hidden_firing_heatmap\": wandb.Image(fig)})\n",
        "                plt.close(fig)\n",
        "\n",
        "                # Log scalar values\n",
        "                wandb.log({\n",
        "                    \"val_accuracy\": val_accuracy,\n",
        "                    \"val_loss\": val_loss,\n",
        "                    \"epoch\": epoch,\n",
        "                    \"std\": current_std,\n",
        "                    \"samples\": current_samples,\n",
        "                    \"acc_threshold\": acc_thresh,\n",
        "                    \"hidden_firing_mean\": hidden_firing_counts.mean()\n",
        "                })\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "train_snn()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
