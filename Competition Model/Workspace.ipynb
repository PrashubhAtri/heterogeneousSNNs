{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from EvolutionStrategy import ESModel\n",
    "from RandmanFunctions import get_randman_dataset\n",
    "from Utilities import spike_to_label, voltage_to_logits\n",
    "\n",
    "\n",
    "import snntorch as snn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExcitationPopulation(nn.Module):\n",
    "    def __init__(self, nb_neurons, nb_input, nb_inh, beta, nb_decision_steps):\n",
    "        super().__init__()        \n",
    "        self.input_fc = nn.Linear(nb_input, nb_neurons, bias = False)\n",
    "        self.recurrent_fc = nn.Linear(nb_neurons, nb_neurons, bias = False)\n",
    "        self.inhibition_fc = nn.Linear(nb_inh, nb_neurons, bias = False)\n",
    "        self.lif = snn.Leaky(beta, learn_beta = False, threshold=1, reset_mechanism='subtract')\n",
    "        self.readout_fc = nn.Linear(nb_neurons, 1, bias=False)\n",
    "        self.readout_lif = snn.Leaky(beta=0.95, reset_mechanism = 'none')\n",
    "        self.nb_decision_steps = nb_decision_steps\n",
    "        self.init_states(nb_decision_steps)       \n",
    "\n",
    "    def get_nb_neurons(self):\n",
    "        return self.input_fc.out_features\n",
    "    \n",
    "    def init_states(self, nb_decision_steps=None):\n",
    "        # excitatory neurons\n",
    "        self.mem = self.lif.init_leaky().to(device)\n",
    "        self.last_spks_queue = [torch.zeros(self.get_nb_neurons(), device=device) for _ in range(nb_decision_steps if nb_decision_steps != None else len(self.last_spks_queue))]  \n",
    "        \n",
    "        # readout neuron\n",
    "        self.readout_mem = self.readout_lif.init_leaky().to(device)\n",
    "        self.readout_mem_rec = []\n",
    "\n",
    "    def forward(self, input, inhibition):\n",
    "        # excitatory neurons\n",
    "        # TODO: \n",
    "        # PROBLEM: THE ABS() IS APPLIED IN THE WRONG PLACE. SHOULD APPLY ON WEIGHTS BEFORE LINEAR COMBINATION\n",
    "        curr = torch.abs(self.input_fc(input)) + torch.abs(self.recurrent_fc(self.last_spks_queue[-1])) - torch.abs(self.inhibition_fc(inhibition))\n",
    "        spk, self.mem = self.lif(curr, self.mem)\n",
    "        \n",
    "        # readout neuron\n",
    "        readout_curr = torch.abs(self.readout_fc(spk))\n",
    "        _, self.readout_mem = self.readout_lif(readout_curr, self.readout_mem)    \n",
    "        \n",
    "        # update spk record\n",
    "        self.last_spks_queue.pop(0)\n",
    "        self.last_spks_queue.append(spk.clone().detach())\n",
    "        \n",
    "        # update readout record\n",
    "        self.readout_mem_rec.append(self.readout_mem.squeeze(dim=-1).clone())\n",
    "        \n",
    "        return spk\n",
    "        \n",
    "    def get_last_spikes_means(self):\n",
    "        # stacked shape: [nb_decision_steps, batch_size, nb_neurons]\n",
    "        # For each batch, the mean should include all the final steps and all the neurons (first and last dimension)\n",
    "        # return shape: [batch_size,]\n",
    "        return torch.stack(self.last_spks_queue).mean(dim = [0, 2])\n",
    "    \n",
    "    def get_readout(self):    \n",
    "        # stacked shape [nb_decision_steps, batch_size]\n",
    "        return torch.stack(self.readout_mem_rec[-self.nb_decision_steps: ]).mean(dim=0)\n",
    "    \n",
    "# def test_ep():\n",
    "#     ep = ExcitationPopulation(nb_neurons=3, nb_input=10, nb_inh=1, beta=0.95, nb_decision_steps=5).to(device)\n",
    "#     for _ in range(100):\n",
    "#         fake_spk = torch.rand([64, 10], device=device)\n",
    "#         fake_inh = torch.rand([64, 1], device=device)\n",
    "#         out = ep(fake_spk, fake_inh)\n",
    "#     print(ep.get_readout().shape)\n",
    "        \n",
    "# test_ep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitionModel(nn.Module):\n",
    "    def __init__(self, nb_input, nb_ext, nb_inh, beta_ext, beta_inh, nb_decision_steps):\n",
    "        super().__init__()\n",
    "        \n",
    "        # excitatory\n",
    "        self.excitatory_1 = ExcitationPopulation(nb_ext, nb_input, nb_inh, beta_ext, nb_decision_steps)\n",
    "        self.excitatory_2 = ExcitationPopulation(nb_ext, nb_input, nb_inh, beta_ext, nb_decision_steps)\n",
    "        \n",
    "        # inhibitory.\n",
    "        self.inh_fc = nn.Linear(nb_ext, 1, bias = False) # Note: two ext share same inh weights\n",
    "        self.inh_lif = snn.Leaky(beta_inh, learn_beta = False)\n",
    "        \n",
    "        # records\n",
    "        self.nb_decision_steps = nb_decision_steps\n",
    "        \n",
    "    def get_nb_ext(self):\n",
    "        return self.excitatory_1.get_nb_neurons()\n",
    "    \n",
    "    def get_nb_inh(self):\n",
    "        return self.inh_fc.out_features\n",
    "    \n",
    "    def init_states(self):\n",
    "        self.excitatory_1.init_states()\n",
    "        self.excitatory_2.init_states()\n",
    "        self.mem_inh = self.inh_lif.init_leaky()\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        # change x shape from [batch, time steps, nb_input] to [time steps, batch, nb_input]\n",
    "        x = x.permute([1, 0, 2])\n",
    "        \n",
    "        # pad time steps for model to go to steady states\n",
    "        x = torch.cat([x, torch.zeros(5 + self.nb_decision_steps, x.shape[1], x.shape[2], device=device)])\n",
    "        \n",
    "        # initalize membrane potentials\n",
    "        self.init_states()\n",
    "        \n",
    "        # init spikes with shape [nb_neurons]. The batch size will be broadcasted\n",
    "        inh_spk = torch.zeros([self.get_nb_inh()], device=device)\n",
    "        \n",
    "        for t in range(len(x)):          \n",
    "            # excitation\n",
    "            ext_1_spk = self.excitatory_1(x[t], inh_spk)\n",
    "            ext_2_spk = self.excitatory_2(x[t], inh_spk)\n",
    "            \n",
    "            # inhibition. Inhibitory neurons are excited, so curr should be positive\n",
    "            curr_inh = torch.abs(self.inh_fc(ext_1_spk)) + torch.abs(self.inh_fc(ext_2_spk))\n",
    "            \n",
    "            inh_spk, self.mem_inh = self.inh_lif(curr_inh, self.mem_inh)\n",
    "        \n",
    "        # return shape: [batch_size, 2], where column 0 is ext1, column 1 is ext2\n",
    "        return torch.stack([self.excitatory_1.get_readout(), self.excitatory_2.get_readout()], dim = 1)\n",
    "    \n",
    "    def get_mem_rec(self):\n",
    "        # stacked shape: [batch, time_steps]\n",
    "        mem_rec_1 = torch.stack(self.excitatory_1.readout_mem_rec, dim = 1)\n",
    "        mem_rec_2 = torch.stack(self.excitatory_2.readout_mem_rec, dim = 1)\n",
    "        \n",
    "        # return shape: [batch, time_steps, 2]\n",
    "        return torch.stack([mem_rec_1, mem_rec_2], dim=2)\n",
    "\n",
    "# def test_cm():\n",
    "#     cm = CompetitionModel(nb_input=10, nb_ext=3, nb_inh=1, beta_ext=0.75, beta_inh=0.95, nb_decision_steps=10)\n",
    "#     x = torch.rand([64, 100, 10])\n",
    "#     print(cm(x).shape)\n",
    "# test_cm()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Training SNN for Randman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "def _run_snn_on_batch(model, x, y, loss_fn): \n",
    "    # shape: [time_steps, batch_size, classes]\n",
    "    logits = model(x)\n",
    "    pred_y = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    loss = loss_fn(logits, y.long())\n",
    "    correct = (pred_y == y).sum().item()\n",
    "    \n",
    "    return loss, correct\n",
    "\n",
    "def log_model(es_model,run):\n",
    "    filename = 'best-model.pth'\n",
    "    model = es_model.get_best_model()\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    run.log_model(path=filename)\n",
    "    os.remove(filename)  \n",
    "\n",
    "def val_loop_snn(es_model, dataloader, loss_fn):\n",
    "    model = es_model.get_best_model()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        batch_loss, batch_correct = _run_snn_on_batch(model, x, y, loss_fn) \n",
    "        test_loss += batch_loss\n",
    "        correct += batch_correct\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_acc = correct / size\n",
    "    print(f\"Test Error: \\nAccuracy: {(100*test_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "        \n",
    "    return test_loss.item(), test_acc\n",
    "\n",
    "def train_loop_snn(es_model, train_dataloader, val_dataloader, loss_fn, nb_model_samples, run):\n",
    "    \"\"\" one epoch of training, going through all the batches once\n",
    "    \"\"\"    \n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # train model with samples\n",
    "        samples_loss = []\n",
    "        for model in es_model.samples(nb_model_samples):\n",
    "            loss, _ = _run_snn_on_batch(model, x, y, loss_fn)\n",
    "            samples_loss.append(loss)            \n",
    "            \n",
    "        samples_loss = torch.stack(samples_loss) \n",
    "        es_model.gradient_descent(samples_loss)\n",
    "    \n",
    "        # best model loss and accuracy\n",
    "        best_model = es_model.get_best_model()\n",
    "        best_loss, best_correct = _run_snn_on_batch(best_model, x, y, loss_fn)   \n",
    "        best_acc = best_correct / len(y)\n",
    "        print(f\"batch {batch}, loss: {best_loss:>7f}, accuracy: {100 * best_acc:>0.1f}%\")\n",
    "        \n",
    "        # validation loss and accuracy\n",
    "        val_loss, val_acc = val_loop_snn(es_model, val_dataloader, loss_fn)\n",
    "        \n",
    "        # record keeping\n",
    "        run.log({'train_loss': best_loss.item(), 'train_acc' : best_acc, 'val_loss': val_loss, 'val_acc': val_acc}) \n",
    "        log_model(es_model, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myixing\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/darwin_neuron/wandb/run-20250428_224906-yiedzecb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/DarwinNeuron/ES-Randman-Competition/runs/yiedzecb' target=\"_blank\">ABSOLUUUUUTE</a></strong> to <a href='https://wandb.ai/DarwinNeuron/ES-Randman-Competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/DarwinNeuron/ES-Randman-Competition' target=\"_blank\">https://wandb.ai/DarwinNeuron/ES-Randman-Competition</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/DarwinNeuron/ES-Randman-Competition/runs/yiedzecb' target=\"_blank\">https://wandb.ai/DarwinNeuron/ES-Randman-Competition/runs/yiedzecb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "batch 0, loss: 0.712599, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 49.4%, Avg loss: 0.711836 \n",
      "\n",
      "batch 1, loss: 0.706591, accuracy: 50.4%\n",
      "Test Error: \n",
      "Accuracy: 47.8%, Avg loss: 0.717977 \n",
      "\n",
      "batch 2, loss: 0.697645, accuracy: 53.5%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.717653 \n",
      "\n",
      "batch 3, loss: 0.701385, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 47.9%, Avg loss: 0.729832 \n",
      "\n",
      "batch 4, loss: 0.693850, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 47.5%, Avg loss: 0.724438 \n",
      "\n",
      "batch 5, loss: 0.713635, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 48.8%, Avg loss: 0.727030 \n",
      "\n",
      "batch 6, loss: 0.727859, accuracy: 52.0%\n",
      "Test Error: \n",
      "Accuracy: 47.9%, Avg loss: 0.719463 \n",
      "\n",
      "batch 7, loss: 0.709250, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 48.5%, Avg loss: 0.731352 \n",
      "\n",
      "batch 8, loss: 0.688512, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 47.4%, Avg loss: 0.721888 \n",
      "\n",
      "batch 9, loss: 0.704369, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 49.9%, Avg loss: 0.716532 \n",
      "\n",
      "batch 10, loss: 0.694753, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 48.5%, Avg loss: 0.716569 \n",
      "\n",
      "batch 11, loss: 0.722742, accuracy: 47.7%\n",
      "Test Error: \n",
      "Accuracy: 49.0%, Avg loss: 0.715199 \n",
      "\n",
      "batch 12, loss: 0.691443, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 49.1%, Avg loss: 0.719944 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "batch 0, loss: 0.696485, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 47.6%, Avg loss: 0.714117 \n",
      "\n",
      "batch 1, loss: 0.683482, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 49.0%, Avg loss: 0.711331 \n",
      "\n",
      "batch 2, loss: 0.699707, accuracy: 52.0%\n",
      "Test Error: \n",
      "Accuracy: 50.9%, Avg loss: 0.699787 \n",
      "\n",
      "batch 3, loss: 0.703691, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 52.9%, Avg loss: 0.692946 \n",
      "\n",
      "batch 4, loss: 0.704611, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 51.5%, Avg loss: 0.698898 \n",
      "\n",
      "batch 5, loss: 0.701035, accuracy: 52.0%\n",
      "Test Error: \n",
      "Accuracy: 51.5%, Avg loss: 0.699651 \n",
      "\n",
      "batch 6, loss: 0.711217, accuracy: 48.0%\n",
      "Test Error: \n",
      "Accuracy: 50.9%, Avg loss: 0.702551 \n",
      "\n",
      "batch 7, loss: 0.714191, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 51.4%, Avg loss: 0.704077 \n",
      "\n",
      "batch 8, loss: 0.698065, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 50.4%, Avg loss: 0.703774 \n",
      "\n",
      "batch 9, loss: 0.679969, accuracy: 59.4%\n",
      "Test Error: \n",
      "Accuracy: 50.1%, Avg loss: 0.700332 \n",
      "\n",
      "batch 10, loss: 0.689876, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 50.1%, Avg loss: 0.698026 \n",
      "\n",
      "batch 11, loss: 0.700392, accuracy: 48.0%\n",
      "Test Error: \n",
      "Accuracy: 52.0%, Avg loss: 0.695971 \n",
      "\n",
      "batch 12, loss: 0.669079, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 51.9%, Avg loss: 0.699115 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "batch 0, loss: 0.686331, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 51.9%, Avg loss: 0.699739 \n",
      "\n",
      "batch 1, loss: 0.677723, accuracy: 57.4%\n",
      "Test Error: \n",
      "Accuracy: 51.5%, Avg loss: 0.698171 \n",
      "\n",
      "batch 2, loss: 0.686873, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 51.9%, Avg loss: 0.696558 \n",
      "\n",
      "batch 3, loss: 0.677340, accuracy: 57.8%\n",
      "Test Error: \n",
      "Accuracy: 51.6%, Avg loss: 0.699202 \n",
      "\n",
      "batch 4, loss: 0.694377, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.703170 \n",
      "\n",
      "batch 5, loss: 0.692514, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.701892 \n",
      "\n",
      "batch 6, loss: 0.693446, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 50.6%, Avg loss: 0.701488 \n",
      "\n",
      "batch 7, loss: 0.689623, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 53.0%, Avg loss: 0.693459 \n",
      "\n",
      "batch 8, loss: 0.685334, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.697368 \n",
      "\n",
      "batch 9, loss: 0.689271, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 51.5%, Avg loss: 0.697156 \n",
      "\n",
      "batch 10, loss: 0.697291, accuracy: 52.0%\n",
      "Test Error: \n",
      "Accuracy: 51.5%, Avg loss: 0.697258 \n",
      "\n",
      "batch 11, loss: 0.679715, accuracy: 56.6%\n",
      "Test Error: \n",
      "Accuracy: 52.1%, Avg loss: 0.693897 \n",
      "\n",
      "batch 12, loss: 0.674219, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 54.5%, Avg loss: 0.688887 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "batch 0, loss: 0.687634, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 54.1%, Avg loss: 0.695547 \n",
      "\n",
      "batch 1, loss: 0.686728, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 50.1%, Avg loss: 0.702115 \n",
      "\n",
      "batch 2, loss: 0.682614, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.700659 \n",
      "\n",
      "batch 3, loss: 0.701807, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.697202 \n",
      "\n",
      "batch 4, loss: 0.682200, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 52.0%, Avg loss: 0.694127 \n",
      "\n",
      "batch 5, loss: 0.703689, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 52.8%, Avg loss: 0.690890 \n",
      "\n",
      "batch 6, loss: 0.687052, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 52.9%, Avg loss: 0.694301 \n",
      "\n",
      "batch 7, loss: 0.699641, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 53.9%, Avg loss: 0.693043 \n",
      "\n",
      "batch 8, loss: 0.694252, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 51.1%, Avg loss: 0.695352 \n",
      "\n",
      "batch 9, loss: 0.693815, accuracy: 53.5%\n",
      "Test Error: \n",
      "Accuracy: 50.6%, Avg loss: 0.695924 \n",
      "\n",
      "batch 10, loss: 0.683603, accuracy: 58.2%\n",
      "Test Error: \n",
      "Accuracy: 51.6%, Avg loss: 0.697741 \n",
      "\n",
      "batch 11, loss: 0.692019, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 48.9%, Avg loss: 0.704350 \n",
      "\n",
      "batch 12, loss: 0.679201, accuracy: 58.6%\n",
      "Test Error: \n",
      "Accuracy: 50.0%, Avg loss: 0.699262 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "batch 0, loss: 0.694270, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.695753 \n",
      "\n",
      "batch 1, loss: 0.689496, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 49.9%, Avg loss: 0.696418 \n",
      "\n",
      "batch 2, loss: 0.687302, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 51.6%, Avg loss: 0.696207 \n",
      "\n",
      "batch 3, loss: 0.685398, accuracy: 55.9%\n",
      "Test Error: \n",
      "Accuracy: 53.1%, Avg loss: 0.693059 \n",
      "\n",
      "batch 4, loss: 0.688080, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 53.2%, Avg loss: 0.693277 \n",
      "\n",
      "batch 5, loss: 0.694042, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 52.0%, Avg loss: 0.694387 \n",
      "\n",
      "batch 6, loss: 0.692073, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 54.0%, Avg loss: 0.693913 \n",
      "\n",
      "batch 7, loss: 0.678077, accuracy: 60.2%\n",
      "Test Error: \n",
      "Accuracy: 52.9%, Avg loss: 0.691019 \n",
      "\n",
      "batch 8, loss: 0.694134, accuracy: 52.0%\n",
      "Test Error: \n",
      "Accuracy: 49.9%, Avg loss: 0.693909 \n",
      "\n",
      "batch 9, loss: 0.692883, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 52.1%, Avg loss: 0.695451 \n",
      "\n",
      "batch 10, loss: 0.691034, accuracy: 50.4%\n",
      "Test Error: \n",
      "Accuracy: 50.1%, Avg loss: 0.698498 \n",
      "\n",
      "batch 11, loss: 0.685497, accuracy: 56.6%\n",
      "Test Error: \n",
      "Accuracy: 50.6%, Avg loss: 0.696358 \n",
      "\n",
      "batch 12, loss: 0.698222, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.697342 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "batch 0, loss: 0.691820, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 51.1%, Avg loss: 0.696501 \n",
      "\n",
      "batch 1, loss: 0.689027, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 52.6%, Avg loss: 0.695196 \n",
      "\n",
      "batch 2, loss: 0.696492, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 54.4%, Avg loss: 0.696459 \n",
      "\n",
      "batch 3, loss: 0.687353, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 53.2%, Avg loss: 0.694680 \n",
      "\n",
      "batch 4, loss: 0.683122, accuracy: 60.9%\n",
      "Test Error: \n",
      "Accuracy: 51.6%, Avg loss: 0.693392 \n",
      "\n",
      "batch 5, loss: 0.688247, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 51.9%, Avg loss: 0.696123 \n",
      "\n",
      "batch 6, loss: 0.687194, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 52.6%, Avg loss: 0.695715 \n",
      "\n",
      "batch 7, loss: 0.692632, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 52.1%, Avg loss: 0.694556 \n",
      "\n",
      "batch 8, loss: 0.690559, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 51.6%, Avg loss: 0.697196 \n",
      "\n",
      "batch 9, loss: 0.691850, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.697170 \n",
      "\n",
      "batch 10, loss: 0.681589, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 52.1%, Avg loss: 0.699320 \n",
      "\n",
      "batch 11, loss: 0.697502, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 51.4%, Avg loss: 0.697625 \n",
      "\n",
      "batch 12, loss: 0.694026, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 52.1%, Avg loss: 0.698577 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "batch 0, loss: 0.686658, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 52.0%, Avg loss: 0.695478 \n",
      "\n",
      "batch 1, loss: 0.692398, accuracy: 51.6%\n",
      "Test Error: \n",
      "Accuracy: 51.5%, Avg loss: 0.697345 \n",
      "\n",
      "batch 2, loss: 0.689230, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 52.8%, Avg loss: 0.696437 \n",
      "\n",
      "batch 3, loss: 0.690855, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 54.4%, Avg loss: 0.695682 \n",
      "\n",
      "batch 4, loss: 0.679081, accuracy: 59.0%\n",
      "Test Error: \n",
      "Accuracy: 52.1%, Avg loss: 0.696258 \n",
      "\n",
      "batch 5, loss: 0.690085, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.698631 \n",
      "\n",
      "batch 6, loss: 0.682920, accuracy: 57.8%\n",
      "Test Error: \n",
      "Accuracy: 52.0%, Avg loss: 0.696400 \n",
      "\n",
      "batch 7, loss: 0.689350, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 52.2%, Avg loss: 0.698842 \n",
      "\n",
      "batch 8, loss: 0.688114, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 51.6%, Avg loss: 0.696512 \n",
      "\n",
      "batch 9, loss: 0.684731, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 51.1%, Avg loss: 0.696710 \n",
      "\n",
      "batch 10, loss: 0.694496, accuracy: 47.7%\n",
      "Test Error: \n",
      "Accuracy: 51.4%, Avg loss: 0.697145 \n",
      "\n",
      "batch 11, loss: 0.689467, accuracy: 58.6%\n",
      "Test Error: \n",
      "Accuracy: 51.1%, Avg loss: 0.696501 \n",
      "\n",
      "batch 12, loss: 0.699067, accuracy: 46.1%\n",
      "Test Error: \n",
      "Accuracy: 51.7%, Avg loss: 0.692724 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "batch 0, loss: 0.694939, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 49.4%, Avg loss: 0.695943 \n",
      "\n",
      "batch 1, loss: 0.697609, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 50.2%, Avg loss: 0.693886 \n",
      "\n",
      "batch 2, loss: 0.697147, accuracy: 43.4%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.695404 \n",
      "\n",
      "batch 3, loss: 0.691967, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 50.6%, Avg loss: 0.693884 \n",
      "\n",
      "batch 4, loss: 0.684956, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.695249 \n",
      "\n",
      "batch 5, loss: 0.685085, accuracy: 53.5%\n",
      "Test Error: \n",
      "Accuracy: 53.0%, Avg loss: 0.693341 \n",
      "\n",
      "batch 6, loss: 0.689506, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 51.9%, Avg loss: 0.694058 \n",
      "\n",
      "batch 7, loss: 0.690646, accuracy: 53.5%\n",
      "Test Error: \n",
      "Accuracy: 51.2%, Avg loss: 0.695652 \n",
      "\n",
      "batch 8, loss: 0.687270, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 50.7%, Avg loss: 0.697053 \n",
      "\n",
      "batch 9, loss: 0.698374, accuracy: 47.7%\n",
      "Test Error: \n",
      "Accuracy: 48.9%, Avg loss: 0.696981 \n",
      "\n",
      "batch 10, loss: 0.684009, accuracy: 56.2%\n",
      "Test Error: \n",
      "Accuracy: 48.1%, Avg loss: 0.696608 \n",
      "\n",
      "batch 11, loss: 0.691040, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 50.6%, Avg loss: 0.694737 \n",
      "\n",
      "batch 12, loss: 0.698791, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.694990 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "batch 0, loss: 0.686558, accuracy: 51.2%\n",
      "Test Error: \n",
      "Accuracy: 50.5%, Avg loss: 0.695823 \n",
      "\n",
      "batch 1, loss: 0.698282, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 52.0%, Avg loss: 0.695966 \n",
      "\n",
      "batch 2, loss: 0.696041, accuracy: 53.5%\n",
      "Test Error: \n",
      "Accuracy: 50.9%, Avg loss: 0.695283 \n",
      "\n",
      "batch 3, loss: 0.696271, accuracy: 50.4%\n",
      "Test Error: \n",
      "Accuracy: 48.6%, Avg loss: 0.698149 \n",
      "\n",
      "batch 4, loss: 0.685796, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 50.0%, Avg loss: 0.697383 \n",
      "\n",
      "batch 5, loss: 0.695894, accuracy: 52.0%\n",
      "Test Error: \n",
      "Accuracy: 49.0%, Avg loss: 0.698884 \n",
      "\n",
      "batch 6, loss: 0.700388, accuracy: 47.3%\n",
      "Test Error: \n",
      "Accuracy: 49.5%, Avg loss: 0.698162 \n",
      "\n",
      "batch 7, loss: 0.692143, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 51.0%, Avg loss: 0.695752 \n",
      "\n",
      "batch 8, loss: 0.685492, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 50.4%, Avg loss: 0.696616 \n",
      "\n",
      "batch 9, loss: 0.688884, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 49.4%, Avg loss: 0.697694 \n",
      "\n",
      "batch 10, loss: 0.695944, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 48.4%, Avg loss: 0.699473 \n",
      "\n",
      "batch 11, loss: 0.693099, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 47.6%, Avg loss: 0.699954 \n",
      "\n",
      "batch 12, loss: 0.693493, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 46.5%, Avg loss: 0.701779 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "batch 0, loss: 0.683547, accuracy: 57.0%\n",
      "Test Error: \n",
      "Accuracy: 47.8%, Avg loss: 0.700250 \n",
      "\n",
      "batch 1, loss: 0.691727, accuracy: 53.5%\n",
      "Test Error: \n",
      "Accuracy: 49.2%, Avg loss: 0.701934 \n",
      "\n",
      "batch 2, loss: 0.686687, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 48.9%, Avg loss: 0.703696 \n",
      "\n",
      "batch 3, loss: 0.700900, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 49.9%, Avg loss: 0.701221 \n",
      "\n",
      "batch 4, loss: 0.686426, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 49.4%, Avg loss: 0.700090 \n",
      "\n",
      "batch 5, loss: 0.706126, accuracy: 47.3%\n",
      "Test Error: \n",
      "Accuracy: 50.6%, Avg loss: 0.698875 \n",
      "\n",
      "batch 6, loss: 0.700606, accuracy: 46.9%\n",
      "Test Error: \n",
      "Accuracy: 51.6%, Avg loss: 0.698184 \n",
      "\n",
      "batch 7, loss: 0.695114, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 49.4%, Avg loss: 0.701026 \n",
      "\n",
      "batch 8, loss: 0.686980, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 50.4%, Avg loss: 0.700160 \n",
      "\n",
      "batch 9, loss: 0.687107, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 48.9%, Avg loss: 0.700094 \n",
      "\n",
      "batch 10, loss: 0.696584, accuracy: 50.8%\n",
      "Test Error: \n",
      "Accuracy: 49.0%, Avg loss: 0.700334 \n",
      "\n",
      "batch 11, loss: 0.697960, accuracy: 50.4%\n",
      "Test Error: \n",
      "Accuracy: 48.0%, Avg loss: 0.702405 \n",
      "\n",
      "batch 12, loss: 0.691318, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 47.4%, Avg loss: 0.703787 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "batch 0, loss: 0.697716, accuracy: 52.0%\n",
      "Test Error: \n",
      "Accuracy: 47.2%, Avg loss: 0.705827 \n",
      "\n",
      "batch 1, loss: 0.688164, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 49.1%, Avg loss: 0.702543 \n",
      "\n",
      "batch 2, loss: 0.687603, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 48.4%, Avg loss: 0.703193 \n",
      "\n",
      "batch 3, loss: 0.696123, accuracy: 50.4%\n",
      "Test Error: \n",
      "Accuracy: 48.5%, Avg loss: 0.703455 \n",
      "\n",
      "batch 4, loss: 0.681880, accuracy: 55.1%\n",
      "Test Error: \n",
      "Accuracy: 47.8%, Avg loss: 0.704217 \n",
      "\n",
      "batch 5, loss: 0.686339, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 47.8%, Avg loss: 0.702543 \n",
      "\n",
      "batch 6, loss: 0.694850, accuracy: 52.0%\n",
      "Test Error: \n",
      "Accuracy: 48.1%, Avg loss: 0.703371 \n",
      "\n",
      "batch 7, loss: 0.703284, accuracy: 47.7%\n",
      "Test Error: \n",
      "Accuracy: 48.2%, Avg loss: 0.705840 \n",
      "\n",
      "batch 8, loss: 0.688784, accuracy: 53.1%\n",
      "Test Error: \n",
      "Accuracy: 47.9%, Avg loss: 0.706488 \n",
      "\n",
      "batch 9, loss: 0.702134, accuracy: 50.4%\n",
      "Test Error: \n",
      "Accuracy: 48.6%, Avg loss: 0.705326 \n",
      "\n",
      "batch 10, loss: 0.704157, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 49.4%, Avg loss: 0.704437 \n",
      "\n",
      "batch 11, loss: 0.705115, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 48.1%, Avg loss: 0.706469 \n",
      "\n",
      "batch 12, loss: 0.717084, accuracy: 42.2%\n",
      "Test Error: \n",
      "Accuracy: 47.1%, Avg loss: 0.708583 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "batch 0, loss: 0.693604, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 46.9%, Avg loss: 0.708368 \n",
      "\n",
      "batch 1, loss: 0.697783, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 47.1%, Avg loss: 0.707879 \n",
      "\n",
      "batch 2, loss: 0.696396, accuracy: 50.4%\n",
      "Test Error: \n",
      "Accuracy: 47.6%, Avg loss: 0.709969 \n",
      "\n",
      "batch 3, loss: 0.693676, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 47.9%, Avg loss: 0.707742 \n",
      "\n",
      "batch 4, loss: 0.703191, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 46.6%, Avg loss: 0.706768 \n",
      "\n",
      "batch 5, loss: 0.683360, accuracy: 54.3%\n",
      "Test Error: \n",
      "Accuracy: 47.4%, Avg loss: 0.705397 \n",
      "\n",
      "batch 6, loss: 0.699897, accuracy: 49.2%\n",
      "Test Error: \n",
      "Accuracy: 47.9%, Avg loss: 0.706990 \n",
      "\n",
      "batch 7, loss: 0.707515, accuracy: 45.7%\n",
      "Test Error: \n",
      "Accuracy: 47.5%, Avg loss: 0.707843 \n",
      "\n",
      "batch 8, loss: 0.705074, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 47.9%, Avg loss: 0.705281 \n",
      "\n",
      "batch 9, loss: 0.694827, accuracy: 50.4%\n",
      "Test Error: \n",
      "Accuracy: 47.4%, Avg loss: 0.706825 \n",
      "\n",
      "batch 10, loss: 0.688314, accuracy: 50.0%\n",
      "Test Error: \n",
      "Accuracy: 48.4%, Avg loss: 0.703192 \n",
      "\n",
      "batch 11, loss: 0.690708, accuracy: 54.7%\n",
      "Test Error: \n",
      "Accuracy: 47.5%, Avg loss: 0.703086 \n",
      "\n",
      "batch 12, loss: 0.689497, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 47.9%, Avg loss: 0.703162 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "batch 0, loss: 0.683849, accuracy: 58.6%\n",
      "Test Error: \n",
      "Accuracy: 47.6%, Avg loss: 0.703458 \n",
      "\n",
      "batch 1, loss: 0.706255, accuracy: 47.7%\n",
      "Test Error: \n",
      "Accuracy: 48.0%, Avg loss: 0.704111 \n",
      "\n",
      "batch 2, loss: 0.695242, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 48.1%, Avg loss: 0.704251 \n",
      "\n",
      "batch 3, loss: 0.700952, accuracy: 48.4%\n",
      "Test Error: \n",
      "Accuracy: 47.4%, Avg loss: 0.704062 \n",
      "\n",
      "batch 4, loss: 0.704834, accuracy: 47.7%\n",
      "Test Error: \n",
      "Accuracy: 48.0%, Avg loss: 0.702800 \n",
      "\n",
      "batch 5, loss: 0.686577, accuracy: 55.5%\n",
      "Test Error: \n",
      "Accuracy: 47.9%, Avg loss: 0.703649 \n",
      "\n",
      "batch 6, loss: 0.700747, accuracy: 48.8%\n",
      "Test Error: \n",
      "Accuracy: 47.4%, Avg loss: 0.704290 \n",
      "\n",
      "batch 7, loss: 0.688407, accuracy: 52.3%\n",
      "Test Error: \n",
      "Accuracy: 47.6%, Avg loss: 0.702570 \n",
      "\n",
      "batch 8, loss: 0.711804, accuracy: 43.0%\n",
      "Test Error: \n",
      "Accuracy: 47.1%, Avg loss: 0.701060 \n",
      "\n",
      "batch 9, loss: 0.697650, accuracy: 49.6%\n",
      "Test Error: \n",
      "Accuracy: 47.5%, Avg loss: 0.701839 \n",
      "\n",
      "batch 10, loss: 0.681593, accuracy: 58.6%\n",
      "Test Error: \n",
      "Accuracy: 47.5%, Avg loss: 0.701047 \n",
      "\n",
      "batch 11, loss: 0.699391, accuracy: 47.7%\n",
      "Test Error: \n",
      "Accuracy: 47.5%, Avg loss: 0.701576 \n",
      "\n",
      "batch 12, loss: 0.683655, accuracy: 53.9%\n",
      "Test Error: \n",
      "Accuracy: 48.0%, Avg loss: 0.702125 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "batch 0, loss: 0.695541, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 48.2%, Avg loss: 0.699058 \n",
      "\n",
      "batch 1, loss: 0.684207, accuracy: 52.7%\n",
      "Test Error: \n",
      "Accuracy: 48.8%, Avg loss: 0.698684 \n",
      "\n",
      "batch 2, loss: 0.700795, accuracy: 45.3%\n",
      "Test Error: \n",
      "Accuracy: 48.2%, Avg loss: 0.696808 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1074863/1566236211.py\", line 33, in train_snn\n",
      "    train_loop_snn(es_model,train_dataloader, val_dataloader, cross_entropy, run.config.nb_model_samples, run)\n",
      "  File \"/tmp/ipykernel_1074863/2498007126.py\", line 65, in train_loop_snn\n",
      "    log_model(es_model, run)\n",
      "  File \"/tmp/ipykernel_1074863/2498007126.py\", line 18, in log_model\n",
      "    torch.save(model.state_dict(), filename)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/serialization.py\", line 849, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/serialization.py\", line 716, in _open_zipfile_writer\n",
      "    return container(name_or_buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/serialization.py\", line 687, in __init__\n",
      "    super().__init__(torch._C.PyTorchFileWriter(self.name))\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: File best-model.pth cannot be opened.\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1074863/1566236211.py\", line 33, in train_snn\n",
      "    train_loop_snn(es_model,train_dataloader, val_dataloader, cross_entropy, run.config.nb_model_samples, run)\n",
      "  File \"/tmp/ipykernel_1074863/2498007126.py\", line 65, in train_loop_snn\n",
      "    log_model(es_model, run)\n",
      "  File \"/tmp/ipykernel_1074863/2498007126.py\", line 18, in log_model\n",
      "    torch.save(model.state_dict(), filename)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/serialization.py\", line 849, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/serialization.py\", line 716, in _open_zipfile_writer\n",
      "    return container(name_or_buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/torch/serialization.py\", line 687, in __init__\n",
      "    super().__init__(torch._C.PyTorchFileWriter(self.name))\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: File best-model.pth cannot be opened.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 5] Input/output error\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1074863/1566236211.py\", line 35, in <module>\n",
      "    train_snn()\n",
      "  File \"/tmp/ipykernel_1074863/1566236211.py\", line 25, in train_snn\n",
      "    with torch.no_grad(), wandb.init(entity = 'DarwinNeuron', project = 'ES-Randman-Competition', name=run_name, config=config) as run:\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_run.py\", line 3719, in __exit__\n",
      "    self._finish(exit_code=exit_code)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_run.py\", line 387, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_run.py\", line 2189, in _finish\n",
      "    logger.info(f\"finishing run {self._get_path()}\")\n",
      "Message: 'finishing run DarwinNeuron/ES-Randman-Competition/yiedzecb'\n",
      "Arguments: ()\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 19] No such device: '/mnt/d/darwin_neuron'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mtrain_snn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m train_loop_snn(es_model,train_dataloader, val_dataloader, cross_entropy, run\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnb_model_samples, run)\n",
      "Cell \u001b[0;32mIn[5], line 65\u001b[0m, in \u001b[0;36mtrain_loop_snn\u001b[0;34m(es_model, train_dataloader, val_dataloader, loss_fn, nb_model_samples, run)\u001b[0m\n\u001b[1;32m     64\u001b[0m run\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: best_loss\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m : best_acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: val_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: val_acc}) \n\u001b[0;32m---> 65\u001b[0m log_model(es_model, run)\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mlog_model\u001b[0;34m(es_model, run)\u001b[0m\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m es_model\u001b[38;5;241m.\u001b[39mget_best_model()\n\u001b[0;32m---> 18\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), filename)\n\u001b[1;32m     19\u001b[0m run\u001b[38;5;241m.\u001b[39mlog_model(path\u001b[38;5;241m=\u001b[39mfilename)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/torch/serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    850\u001b[0m         _save(\n\u001b[1;32m    851\u001b[0m             obj,\n\u001b[1;32m    852\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/torch/serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/torch/serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: File best-model.pth cannot be opened.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m     33\u001b[0m             train_loop_snn(es_model,train_dataloader, val_dataloader, cross_entropy, run\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnb_model_samples, run)\n\u001b[0;32m---> 35\u001b[0m train_snn()\n",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m, in \u001b[0;36mtrain_snn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mABSOLUUUUUTE\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m { \u001b[38;5;66;03m# Dataset:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_input\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      5\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_output\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m2\u001b[39m,  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     24\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregularization\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), wandb\u001b[38;5;241m.\u001b[39minit(entity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDarwinNeuron\u001b[39m\u001b[38;5;124m'\u001b[39m, project \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mES-Randman-Competition\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39mrun_name, config\u001b[38;5;241m=\u001b[39mconfig) \u001b[38;5;28;01mas\u001b[39;00m run:  \n\u001b[1;32m     26\u001b[0m     train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m train_test_split(get_randman_dataset(run\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnb_output, run\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnb_input, run\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnb_steps, run\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnb_data_samples), test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mrun\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:3719\u001b[0m, in \u001b[0;36mRun.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m   3717\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exception(exc_type, exc_val, exc_tb)\n\u001b[1;32m   3718\u001b[0m exit_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 3719\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finish(exit_code\u001b[38;5;241m=\u001b[39mexit_code)\n\u001b[1;32m   3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exception_raised\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:387\u001b[0m, in \u001b[0;36m_log_to_run.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m     run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_id\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging\u001b[38;5;241m.\u001b[39mlog_to_run(run_id):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2197\u001b[0m, in \u001b[0;36mRun._finish\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown_hooks:\n\u001b[1;32m   2196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook\u001b[38;5;241m.\u001b[39mstage \u001b[38;5;241m==\u001b[39m TeardownStage\u001b[38;5;241m.\u001b[39mEARLY:\n\u001b[0;32m-> 2197\u001b[0m         hook\u001b[38;5;241m.\u001b[39mcall()\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;66;03m# Early-stage hooks may use methods that require _is_finished\u001b[39;00m\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;66;03m# to be False, so we set this after running those hooks.\u001b[39;00m\n\u001b[1;32m   2201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:558\u001b[0m, in \u001b[0;36m_WandbInit._jupyter_teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotebook\n\u001b[1;32m    557\u001b[0m ipython \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotebook\u001b[38;5;241m.\u001b[39mshell\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotebook\u001b[38;5;241m.\u001b[39msave_history()\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotebook\u001b[38;5;241m.\u001b[39msave_ipynb():\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/jupyter.py:447\u001b[0m, in \u001b[0;36mNotebook.save_history\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This saves all cell executions in the current session as a new notebook.\"\"\"\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnbformat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v4, validator, write\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    449\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mtermerror(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe nbformat package was not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m It is required to save notebook history.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m     )\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1532\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1506\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1609\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1652\u001b[0m, in \u001b[0;36m_fill_cache\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 19] No such device: '/mnt/d/darwin_neuron'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 5] Input/output error\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3081, in run_cell\n",
      "    self.events.trigger('post_run_cell', result)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/IPython/core/events.py\", line 82, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_init.py\", line 541, in _pause_backend\n",
      "    if self.notebook.save_ipynb():  # type: ignore\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/jupyter.py\", line 384, in save_ipynb\n",
      "    logger.info(\"not saving jupyter notebook\")\n",
      "Message: 'not saving jupyter notebook'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/logging/__init__.py\", line 1164, in emit\n",
      "    self.flush()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/logging/__init__.py\", line 1144, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 5] Input/output error\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3081, in run_cell\n",
      "    self.events.trigger('post_run_cell', result)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/IPython/core/events.py\", line 82, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/home/wyx/miniconda3/envs/snn/lib/python3.12/site-packages/wandb/sdk/wandb_init.py\", line 546, in _pause_backend\n",
      "    self._logger.info(\"pausing backend\")  # type: ignore\n",
      "Message: 'pausing backend'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "def train_snn():   \n",
    "    run_name = 'ABSOLUUUUUTE'\n",
    "    config = { # Dataset:\n",
    "              'nb_input' : 100,\n",
    "              'nb_output' : 2,  \n",
    "              'nb_steps' : 50,\n",
    "              'nb_data_samples': 2000,\n",
    "              # SNN:\n",
    "              'nb_ext' : 3,\n",
    "              'nb_inh' : 1,\n",
    "              'beta_ext': 0.95,\n",
    "              'beta_inh' : 0.75,\n",
    "              'nb_decision_steps' : 10,           \n",
    "              # Evolution Strategy:\n",
    "              'nb_model_samples' : 1000, \n",
    "              # Training: \n",
    "              'std' : 0.15,\n",
    "              'epochs' : 50, \n",
    "              'batch_size' : 256,\n",
    "              # Optimization:\n",
    "              'loss': 'cross-entropy',\n",
    "              'optimizer' : 'Adam',\n",
    "              'lr' : 0.01,\n",
    "              'regularization':'none'}\n",
    "    with torch.no_grad(), wandb.init(entity = 'DarwinNeuron', project = 'ES-Randman-Competition', name=run_name, config=config) as run:  \n",
    "        train_dataset, val_dataset = train_test_split(get_randman_dataset(run.config.nb_output, run.config.nb_input, run.config.nb_steps, run.config.nb_data_samples), test_size=0.2, shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=512, shuffle=False)      \n",
    "        es_model = ESModel(CompetitionModel, run.config.nb_input, run.config.nb_ext, run.config.nb_inh, run.config.beta_ext, run.config.beta_inh, run.config.nb_decision_steps, param_std = run.config.std, Optimizer=optim.Adam, lr=run.config.lr)\n",
    "        for epoch in range(run.config.epochs):\n",
    "            print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "            # train the model\n",
    "            train_loop_snn(es_model,train_dataloader, val_dataloader, cross_entropy, run.config.nb_model_samples, run)\n",
    "    \n",
    "train_snn() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
